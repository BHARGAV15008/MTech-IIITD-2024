{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fe52f377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install lucifer-ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "eaa1b06c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Row#</th>\n",
       "      <th>clonesize</th>\n",
       "      <th>honeybee</th>\n",
       "      <th>bumbles</th>\n",
       "      <th>andrena</th>\n",
       "      <th>osmia</th>\n",
       "      <th>MaxOfUpperTRange</th>\n",
       "      <th>MinOfUpperTRange</th>\n",
       "      <th>AverageOfUpperTRange</th>\n",
       "      <th>MaxOfLowerTRange</th>\n",
       "      <th>MinOfLowerTRange</th>\n",
       "      <th>AverageOfLowerTRange</th>\n",
       "      <th>RainingDays</th>\n",
       "      <th>AverageRainingDays</th>\n",
       "      <th>fruitset</th>\n",
       "      <th>fruitmass</th>\n",
       "      <th>seeds</th>\n",
       "      <th>yield</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>680.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.50</td>\n",
       "      <td>94.6</td>\n",
       "      <td>57.2</td>\n",
       "      <td>79.0</td>\n",
       "      <td>68.2</td>\n",
       "      <td>33.0</td>\n",
       "      <td>55.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.523846</td>\n",
       "      <td>0.460180</td>\n",
       "      <td>37.966864</td>\n",
       "      <td>6328.89332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>514.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.63</td>\n",
       "      <td>77.4</td>\n",
       "      <td>46.8</td>\n",
       "      <td>64.7</td>\n",
       "      <td>55.8</td>\n",
       "      <td>27.0</td>\n",
       "      <td>45.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.583380</td>\n",
       "      <td>0.485219</td>\n",
       "      <td>40.813181</td>\n",
       "      <td>7502.24389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>431.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.63</td>\n",
       "      <td>77.4</td>\n",
       "      <td>46.8</td>\n",
       "      <td>64.7</td>\n",
       "      <td>55.8</td>\n",
       "      <td>27.0</td>\n",
       "      <td>45.8</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.429001</td>\n",
       "      <td>0.408851</td>\n",
       "      <td>31.881847</td>\n",
       "      <td>4587.38557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>656.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.50</td>\n",
       "      <td>94.6</td>\n",
       "      <td>57.2</td>\n",
       "      <td>79.0</td>\n",
       "      <td>68.2</td>\n",
       "      <td>33.0</td>\n",
       "      <td>55.9</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.404225</td>\n",
       "      <td>0.401708</td>\n",
       "      <td>30.798351</td>\n",
       "      <td>4234.86859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>594.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>94.6</td>\n",
       "      <td>57.2</td>\n",
       "      <td>79.0</td>\n",
       "      <td>68.2</td>\n",
       "      <td>33.0</td>\n",
       "      <td>55.9</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.509001</td>\n",
       "      <td>0.452027</td>\n",
       "      <td>35.884011</td>\n",
       "      <td>6026.06365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>14995</td>\n",
       "      <td>153.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.50</td>\n",
       "      <td>86.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>71.9</td>\n",
       "      <td>62.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>50.8</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.462102</td>\n",
       "      <td>0.428904</td>\n",
       "      <td>33.331194</td>\n",
       "      <td>5322.84043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>14996</td>\n",
       "      <td>388.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.63</td>\n",
       "      <td>69.7</td>\n",
       "      <td>42.1</td>\n",
       "      <td>58.2</td>\n",
       "      <td>50.2</td>\n",
       "      <td>24.3</td>\n",
       "      <td>41.2</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.542170</td>\n",
       "      <td>0.459384</td>\n",
       "      <td>43.755034</td>\n",
       "      <td>8357.06722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>14997</td>\n",
       "      <td>194.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.75</td>\n",
       "      <td>94.6</td>\n",
       "      <td>57.2</td>\n",
       "      <td>79.0</td>\n",
       "      <td>68.2</td>\n",
       "      <td>33.0</td>\n",
       "      <td>55.9</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.545095</td>\n",
       "      <td>0.467854</td>\n",
       "      <td>38.317710</td>\n",
       "      <td>6578.23522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>14998</td>\n",
       "      <td>703.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>69.7</td>\n",
       "      <td>42.1</td>\n",
       "      <td>58.2</td>\n",
       "      <td>50.2</td>\n",
       "      <td>24.3</td>\n",
       "      <td>41.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.551264</td>\n",
       "      <td>0.476149</td>\n",
       "      <td>40.402916</td>\n",
       "      <td>7041.38018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14999</th>\n",
       "      <td>14999</td>\n",
       "      <td>571.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.50</td>\n",
       "      <td>94.6</td>\n",
       "      <td>57.2</td>\n",
       "      <td>79.0</td>\n",
       "      <td>68.2</td>\n",
       "      <td>33.0</td>\n",
       "      <td>55.9</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.572726</td>\n",
       "      <td>0.483047</td>\n",
       "      <td>39.675668</td>\n",
       "      <td>7230.13250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows Ã— 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id   Row#  clonesize  honeybee  bumbles  andrena  osmia  \\\n",
       "0          0  680.0       25.0      0.50     0.38     0.38   0.50   \n",
       "1          1  514.0       25.0      0.50     0.25     0.50   0.63   \n",
       "2          2  431.0       12.5      0.25     0.38     0.50   0.63   \n",
       "3          3  656.0       25.0      0.50     0.38     0.38   0.50   \n",
       "4          4  594.0       25.0      0.50     0.25     0.50   0.75   \n",
       "...      ...    ...        ...       ...      ...      ...    ...   \n",
       "14995  14995  153.0       12.5      0.25     0.25     0.38   0.50   \n",
       "14996  14996  388.0       12.5      0.25     0.38     0.38   0.63   \n",
       "14997  14997  194.0       12.5      0.25     0.25     0.38   0.75   \n",
       "14998  14998  703.0       12.5      0.25     0.38     0.50   0.50   \n",
       "14999  14999  571.0       25.0      0.50     0.25     0.38   0.50   \n",
       "\n",
       "       MaxOfUpperTRange  MinOfUpperTRange  AverageOfUpperTRange  \\\n",
       "0                  94.6              57.2                  79.0   \n",
       "1                  77.4              46.8                  64.7   \n",
       "2                  77.4              46.8                  64.7   \n",
       "3                  94.6              57.2                  79.0   \n",
       "4                  94.6              57.2                  79.0   \n",
       "...                 ...               ...                   ...   \n",
       "14995              86.0              52.0                  71.9   \n",
       "14996              69.7              42.1                  58.2   \n",
       "14997              94.6              57.2                  79.0   \n",
       "14998              69.7              42.1                  58.2   \n",
       "14999              94.6              57.2                  79.0   \n",
       "\n",
       "       MaxOfLowerTRange  MinOfLowerTRange  AverageOfLowerTRange  RainingDays  \\\n",
       "0                  68.2              33.0                  55.9          1.0   \n",
       "1                  55.8              27.0                  45.8          1.0   \n",
       "2                  55.8              27.0                  45.8         34.0   \n",
       "3                  68.2              33.0                  55.9         16.0   \n",
       "4                  68.2              33.0                  55.9         34.0   \n",
       "...                 ...               ...                   ...          ...   \n",
       "14995              62.0              30.0                  50.8         24.0   \n",
       "14996              50.2              24.3                  41.2         16.0   \n",
       "14997              68.2              33.0                  55.9         16.0   \n",
       "14998              50.2              24.3                  41.2          1.0   \n",
       "14999              68.2              33.0                  55.9         34.0   \n",
       "\n",
       "       AverageRainingDays  fruitset  fruitmass      seeds       yield  \n",
       "0                    0.10  0.523846   0.460180  37.966864  6328.89332  \n",
       "1                    0.10  0.583380   0.485219  40.813181  7502.24389  \n",
       "2                    0.56  0.429001   0.408851  31.881847  4587.38557  \n",
       "3                    0.26  0.404225   0.401708  30.798351  4234.86859  \n",
       "4                    0.56  0.509001   0.452027  35.884011  6026.06365  \n",
       "...                   ...       ...        ...        ...         ...  \n",
       "14995                0.39  0.462102   0.428904  33.331194  5322.84043  \n",
       "14996                0.26  0.542170   0.459384  43.755034  8357.06722  \n",
       "14997                0.26  0.545095   0.467854  38.317710  6578.23522  \n",
       "14998                0.10  0.551264   0.476149  40.402916  7041.38018  \n",
       "14999                0.56  0.572726   0.483047  39.675668  7230.13250  \n",
       "\n",
       "[15000 rows x 19 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from luciferml.preprocessing import Preprocess\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv('train.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "92369fa6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mluciferml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msupervised\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Regression\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#  dataset = pd.read_excel('examples\\Folds5x2_pp.xlsx')\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(dataset)\n",
      "File \u001b[1;32mf:\\MTech_IIITD\\pyEnv\\Lib\\site-packages\\luciferml\\supervised\\regression.py:14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mluciferml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msupervised\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbest\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Best\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mluciferml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msupervised\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfigs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mluciferml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msupervised\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpredictors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m regression_predictor\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mluciferml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msupervised\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocesser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreProcesser\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mluciferml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msupervised\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtuner\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mluciferml_tuner\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m luciferml_tuner\n",
      "File \u001b[1;32mf:\\MTech_IIITD\\pyEnv\\Lib\\site-packages\\luciferml\\supervised\\utils\\predictors.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtkinter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m N\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcatboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CatBoostClassifier, CatBoostRegressor\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcolorama\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Fore\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LGBMClassifier, LGBMRegressor\n",
      "File \u001b[1;32mf:\\MTech_IIITD\\pyEnv\\Lib\\site-packages\\catboost\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     FeaturesData, EFstrType, EShapCalcType, EFeaturesSelectionAlgorithm, EFeaturesSelectionGrouping,\n\u001b[0;32m      3\u001b[0m     Pool, CatBoost, CatBoostClassifier, CatBoostRegressor, CatBoostRanker, CatBoostError, cv, sample_gaussian_process, train,\n\u001b[0;32m      4\u001b[0m     sum_models, _have_equal_features, to_regressor, to_classifier, to_ranker, MultiRegressionCustomMetric,\n\u001b[0;32m      5\u001b[0m     MultiRegressionCustomObjective, MultiTargetCustomMetric, MultiTargetCustomObjective\n\u001b[0;32m      6\u001b[0m )  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VERSION \u001b[38;5;28;01mas\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeaturesData\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEFstrType\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEShapCalcType\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEFeaturesSelectionAlgorithm\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEFeaturesSelectionGrouping\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPool\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCatBoost\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCatBoostClassifier\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCatBoostRegressor\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCatBoostRanker\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCatboostError\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMultiTargetCustomMetric\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMultiTargetCustomObjective\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     14\u001b[0m ]\n",
      "File \u001b[1;32mf:\\MTech_IIITD\\pyEnv\\Lib\\site-packages\\catboost\\core.py:45\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplot_helpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save_plot_file, try_plot_offline, OfflineMetricVisualizer\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _catboost\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BuiltinMetric\n",
      "File \u001b[1;32mf:\\MTech_IIITD\\pyEnv\\Lib\\site-packages\\catboost\\plot_helpers.py:5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _catboost\n\u001b[0;32m      6\u001b[0m fspath \u001b[38;5;241m=\u001b[39m _catboost\u001b[38;5;241m.\u001b[39mfspath\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtry_plot_offline\u001b[39m(figs):\n",
      "File \u001b[1;32m_catboost.pyx:1\u001b[0m, in \u001b[0;36minit _catboost\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "from luciferml.supervised.regression import Regression\n",
    "#  dataset = pd.read_excel('examples\\Folds5x2_pp.xlsx')\n",
    "# print(dataset)\n",
    "X = dataset.iloc[:, :-1]\n",
    "y = dataset.iloc[:, -1]\n",
    "regressor = Regression(predictor = ['lin'])\n",
    "regressor.fit(X, y)\n",
    "result = regressor.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0e566a",
   "metadata": {},
   "source": [
    "### EXTRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f463027c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependency\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import dabl\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c5bd35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96187241",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_use = train.columns.tolist()\n",
    "for i in ['id', 'Row#', 'yield']: features_to_use.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057bd9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_Loader(train, test, no_samples):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aaee81e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eb2530",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2d19bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Select best features using Random Forest Feature Importance (this part remains the same)\n",
    "def select_best_features_using_importance(X, y, n_estimators=200, max_depth=5, random_state=42):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Fit RandomForest to find feature importance\n",
    "    rf_model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, random_state=random_state)\n",
    "    rf_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Get feature importance\n",
    "    feature_importances = rf_model.feature_importances_\n",
    "    \n",
    "    # Sort features by importance\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': feature_importances\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    print(\"Features sorted by importance:\")\n",
    "    print(feature_importance_df)\n",
    "    \n",
    "    # Select top features (you can decide how many to select)\n",
    "    selected_features = feature_importance_df['Feature'].head(5)  # Selecting top 5 features\n",
    "    return selected_features\n",
    "\n",
    "# Step 2: Train a model using selected features with Cross-Validation\n",
    "def train_model_with_selected_features_cv(X, y, selected_features, testData=None, idTest=None, model_type='gradient_boosting'):\n",
    "    X_selected = X[selected_features]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_selected)\n",
    "\n",
    "    # Select model type\n",
    "    if model_type == 'gradient_boosting':\n",
    "        model = GradientBoostingRegressor(n_estimators=120, learning_rate=0.09, max_depth=5, random_state=42)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model type\")\n",
    "\n",
    "    # Perform 5-Fold Cross-Validation\n",
    "    cv_scores = cross_val_score(model, X_scaled, y, cv=5, scoring='neg_mean_absolute_error')\n",
    "\n",
    "    # Since the scoring returns negative MAE, we'll negate it to show positive values\n",
    "    print(f\"\\nCross-validated MAE (5-Fold): {np.mean(-cv_scores):.4f}\")\n",
    "    \n",
    "    # Fit the model on the entire dataset\n",
    "    model.fit(X_scaled, y)\n",
    "    \n",
    "    # Predict on testData (optional)\n",
    "    if testData is not None:\n",
    "        X_test_scaled = scaler.transform(testData[selected_features])\n",
    "        testPredictions = model.predict(X_test_scaled)\n",
    "        \n",
    "        output = pd.DataFrame({\n",
    "            'id': idTest,\n",
    "            'yield': testPredictions\n",
    "        })\n",
    "        \n",
    "        output_file = 'submission.csv'\n",
    "        output.to_csv(output_file, index=False)\n",
    "        print(f\"Predictions saved to {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "X = trainData[features_to_use]  # All features initially\n",
    "y = trainData['yield']\n",
    "\n",
    "# Step 1: Select best features using feature importance\n",
    "selected_features = select_best_features_using_importance(X, y)\n",
    "\n",
    "# Step 2: Train a model with cross-validation using those selected features\n",
    "train_model_with_selected_features_cv(X, y, selected_features, testData=testData, idTest=testData['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9f51f03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features sorted by importance:\n",
      "                 Feature  Importance\n",
      "13              fruitset  947.179706\n",
      "15                 seeds  350.842002\n",
      "10  AverageOfLowerTRange   58.423081\n",
      "8       MaxOfLowerTRange   57.893522\n",
      "12    AverageRainingDays   53.682711\n",
      "6       MinOfUpperTRange   43.404675\n",
      "1               honeybee   26.645786\n",
      "11           RainingDays   23.947383\n",
      "14             fruitmass   15.747348\n",
      "5       MaxOfUpperTRange   13.159125\n",
      "9       MinOfLowerTRange   10.374619\n",
      "4                  osmia    7.815271\n",
      "0              clonesize    4.395355\n",
      "3                andrena    3.725761\n",
      "7   AverageOfUpperTRange    2.710344\n",
      "2                bumbles    0.615279\n",
      "\n",
      "Cross-validated MAE (5-Fold): 252.8029\n",
      "Predictions saved to submission.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import RANSACRegressor, LinearRegression, Lasso, Ridge, LassoCV, RidgeCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Select best features using a robust method\n",
    "def select_best_features_using_robust(X, y, random_state=42, n_features_to_select=10):\n",
    "    # Initialize RANSAC with a LinearRegression estimator\n",
    "    ransac = RANSACRegressor(estimator=RidgeCV(), random_state=random_state, min_samples=20)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    \n",
    "    # Fit RANSAC\n",
    "    ransac.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Get feature support\n",
    "    support = ransac.estimator_.coef_ != 0\n",
    "    \n",
    "    # Sort features by importance (if needed, e.g., by coefficients)\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': np.abs(ransac.estimator_.coef_)\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    print(\"Features sorted by importance:\")\n",
    "    print(feature_importance_df)\n",
    "    \n",
    "    # Select top features\n",
    "    selected_features = feature_importance_df.loc[support, 'Feature'].head(n_features_to_select)\n",
    "    return selected_features\n",
    "\n",
    "# Step 2: Train a model using selected features with Cross-Validation\n",
    "def train_model_with_selected_features_cv(X, y, selected_features, test=None, idTest=None, model_type='random_forest'):\n",
    "    X_selected = X[selected_features]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_selected)\n",
    "\n",
    "    # Select model type\n",
    "    if model_type == 'gradient_boosting':\n",
    "        model = GradientBoostingRegressor(n_estimators=120, learning_rate=0.06, max_depth=5, random_state=42)\n",
    "    elif model_type == 'random_forest':\n",
    "        model = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model type\")\n",
    "\n",
    "    # Perform 5-Fold Cross-Validation\n",
    "    cv_scores = cross_val_score(model, X_scaled, y, cv=5, scoring='neg_mean_absolute_error')\n",
    "\n",
    "    # Since the scoring returns negative MAE, we'll negate it to show positive values\n",
    "    print(f\"\\nCross-validated MAE (5-Fold): {np.mean(-cv_scores):.4f}\")\n",
    "    \n",
    "    # Fit the model on the entire dataset\n",
    "    model.fit(X_scaled, y)\n",
    "    \n",
    "    # Predict on test (optional)\n",
    "    if test is not None:\n",
    "        X_test_scaled = scaler.transform(test[selected_features])\n",
    "        testPredictions = model.predict(X_test_scaled)\n",
    "        \n",
    "        output = pd.DataFrame({\n",
    "            'id': idTest,\n",
    "            'yield': testPredictions\n",
    "        })\n",
    "        \n",
    "        output_file = 'submission.csv'\n",
    "        output.to_csv(output_file, index=False)\n",
    "        print(f\"Predictions saved to {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "X = train[features_to_use]  # All features initially\n",
    "y = train['yield']\n",
    "\n",
    "# Step 1: Select best features using robust feature selection\n",
    "selected_features = select_best_features_using_robust(X, y)\n",
    "\n",
    "# Step 2: Train a model with cross-validation using those selected features\n",
    "train_model_with_selected_features_cv(X, y, selected_features, test=test, idTest=test['id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0474bdf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training models...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000389 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 856\n",
      "[LightGBM] [Info] Number of data points in the train set: 15000, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 6010.883278\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "\n",
      "Model Evaluation Results:\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001649 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 854\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 6011.822672\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000355 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 6007.233677\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000522 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 855\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 6004.718268\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000356 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 6018.203099\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002115 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 855\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 6012.438672\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "              Model  R2_Score  CV_Mean_R2  CV_Std_R2\n",
      "0            Linear  0.904017    0.743434   0.292003\n",
      "1             Lasso  0.903985    0.903711   0.002872\n",
      "2             Ridge  0.903998    0.903714   0.002854\n",
      "3      RandomForest  0.946125    0.917986   0.003447\n",
      "4  GradientBoosting  0.923457    0.918717   0.003463\n",
      "5          LightGBM  0.932430    0.918388   0.003490\n",
      "6           XGBoost  0.932780    0.918949   0.004019\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000323 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 854\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 6011.822672\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001340 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 6007.233677\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000348 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 855\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 6004.718268\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000464 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 6018.203099\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000485 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 855\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 6012.438672\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "\n",
      "Best Model: XGBoost\n",
      "\n",
      "Best Parameters:\n",
      "learning_rate: 0.1\n",
      "max_depth: 5\n",
      "min_child_weight: 1\n",
      "n_estimators: 100\n",
      "\n",
      "Making predictions...\n",
      "\n",
      "Prediction files have been created for all models and ensemble\n",
      "\n",
      "Feature Importances for Best Model:\n",
      "                 Feature  Importance\n",
      "13              fruitset    0.796388\n",
      "15                 seeds    0.145042\n",
      "5       MaxOfUpperTRange    0.014851\n",
      "14             fruitmass    0.009148\n",
      "6       MinOfUpperTRange    0.008703\n",
      "7   AverageOfUpperTRange    0.005200\n",
      "11           RainingDays    0.003969\n",
      "4                  osmia    0.003421\n",
      "3                andrena    0.003319\n",
      "12    AverageRainingDays    0.002342\n",
      "2                bumbles    0.002312\n",
      "1               honeybee    0.001670\n",
      "0              clonesize    0.001644\n",
      "8       MaxOfLowerTRange    0.001227\n",
      "10  AverageOfLowerTRange    0.000762\n",
      "9       MinOfLowerTRange    0.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class YieldPredictor:\n",
    "    def __init__(self, cv_folds=5, random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize the YieldPredictor class\n",
    "        \"\"\"\n",
    "        \n",
    "        self.cv_folds = cv_folds\n",
    "        self.random_state = random_state\n",
    "        self.models = {}\n",
    "        self.best_params = {}\n",
    "        self.results = {}\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def prepare_data(self, train_path, test_path):\n",
    "        \"\"\"\n",
    "        Load and prepare training and test data\n",
    "        \"\"\"\n",
    "        # Load data\n",
    "        train_df = pd.read_csv(train_path)\n",
    "        test_df = pd.read_csv(test_path)\n",
    "        \n",
    "        # Remove 'id' and 'Row#' columns\n",
    "        feature_cols = ['clonesize', 'honeybee', 'bumbles', 'andrena', 'osmia',\n",
    "                       'MaxOfUpperTRange', 'MinOfUpperTRange', 'AverageOfUpperTRange',\n",
    "                       'MaxOfLowerTRange', 'MinOfLowerTRange', 'AverageOfLowerTRange',\n",
    "                       'RainingDays', 'AverageRainingDays', 'fruitset', 'fruitmass', 'seeds']\n",
    "        \n",
    "        # Prepare training data\n",
    "        X_train = train_df[feature_cols]\n",
    "        y_train = train_df['yield']\n",
    "        \n",
    "        # Prepare test data\n",
    "        X_test = test_df[feature_cols]\n",
    "        test_ids = test_df['id']\n",
    "        \n",
    "        # Scale features\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        return X_train_scaled, y_train, X_test_scaled, test_ids, feature_cols\n",
    "\n",
    "    def train_models(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Train multiple regression models with hyperparameter tuning\n",
    "        \"\"\"\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "        \n",
    "        # Linear Regression\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(X_train, y_train)\n",
    "        self.models['Linear'] = lr\n",
    "        \n",
    "        # Lasso\n",
    "        lasso_params = {\n",
    "            'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10],\n",
    "            'max_iter': [3000]\n",
    "        }\n",
    "        lasso = GridSearchCV(Lasso(random_state=self.random_state), \n",
    "                           lasso_params, cv=self.cv_folds, n_jobs=-1)\n",
    "        lasso.fit(X_train, y_train)\n",
    "        self.models['Lasso'] = lasso.best_estimator_\n",
    "        self.best_params['Lasso'] = lasso.best_params_\n",
    "        \n",
    "        # Ridge\n",
    "        ridge_params = {\n",
    "            'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10],\n",
    "            'max_iter': [3000]\n",
    "        }\n",
    "        ridge = GridSearchCV(Ridge(random_state=self.random_state), \n",
    "                           ridge_params, cv=self.cv_folds, n_jobs=-1)\n",
    "        ridge.fit(X_train, y_train)\n",
    "        self.models['Ridge'] = ridge.best_estimator_\n",
    "        self.best_params['Ridge'] = ridge.best_params_\n",
    "        \n",
    "        # Random Forest\n",
    "        rf_params = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [None, 10, 20],\n",
    "            'min_samples_split': [2, 5],\n",
    "            'min_samples_leaf': [1, 2]\n",
    "        }\n",
    "        rf = GridSearchCV(RandomForestRegressor(random_state=self.random_state), \n",
    "                         rf_params, cv=self.cv_folds, n_jobs=-1)\n",
    "        rf.fit(X_train, y_train)\n",
    "        self.models['RandomForest'] = rf.best_estimator_\n",
    "        self.best_params['RandomForest'] = rf.best_params_\n",
    "        \n",
    "        # Gradient Boosting\n",
    "        gb_params = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'learning_rate': [0.01, 0.1],\n",
    "            'max_depth': [3, 4, 5]\n",
    "        }\n",
    "        gb = GridSearchCV(GradientBoostingRegressor(random_state=self.random_state), \n",
    "                         gb_params, cv=self.cv_folds, n_jobs=-1)\n",
    "        gb.fit(X_train, y_train)\n",
    "        self.models['GradientBoosting'] = gb.best_estimator_\n",
    "        self.best_params['GradientBoosting'] = gb.best_params_\n",
    "        \n",
    "        # LightGBM\n",
    "        lgb_params = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'learning_rate': [0.01, 0.1],\n",
    "            'num_leaves': [31, 50],\n",
    "            'max_depth': [-1, 10]\n",
    "        }\n",
    "        lgb_model = GridSearchCV(lgb.LGBMRegressor(random_state=self.random_state), \n",
    "                                lgb_params, cv=self.cv_folds, n_jobs=-1)\n",
    "        lgb_model.fit(X_train, y_train)\n",
    "        self.models['LightGBM'] = lgb_model.best_estimator_\n",
    "        self.best_params['LightGBM'] = lgb_model.best_params_\n",
    "        \n",
    "        # XGBoost\n",
    "        xgb_params = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'learning_rate': [0.01, 0.1],\n",
    "            'max_depth': [3, 4, 5],\n",
    "            'min_child_weight': [1, 3]\n",
    "        }\n",
    "        xgb_model = GridSearchCV(xgb.XGBRegressor(random_state=self.random_state), \n",
    "                                xgb_params, cv=self.cv_folds, n_jobs=-1)\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "        self.models['XGBoost'] = xgb_model.best_estimator_\n",
    "        self.best_params['XGBoost'] = xgb_model.best_params_\n",
    "\n",
    "    def evaluate_models(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Evaluate all trained models using cross-validation\n",
    "        \"\"\"\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        \n",
    "        evaluation_results = []\n",
    "        for name, model in self.models.items():\n",
    "            # Get R2 score\n",
    "            r2 = model.score(X_train, y_train)\n",
    "            \n",
    "            # Get cross-validation scores\n",
    "            cv_scores = cross_val_score(model, X_train, y_train, \n",
    "                                      cv=self.cv_folds, scoring='r2')\n",
    "            \n",
    "            # Get feature importances for models that support it\n",
    "            importances = None\n",
    "            if hasattr(model, 'feature_importances_'):\n",
    "                importances = model.feature_importances_\n",
    "            \n",
    "            evaluation_results.append({\n",
    "                'Model': name,\n",
    "                'R2_Score': r2,\n",
    "                'CV_Mean_R2': cv_scores.mean(),\n",
    "                'CV_Std_R2': cv_scores.std(),\n",
    "                'Feature_Importances': importances\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(evaluation_results)\n",
    "\n",
    "    def get_best_model(self):\n",
    "        \"\"\"\n",
    "        Return the best performing model based on cross-validation R2 score\n",
    "        \"\"\"\n",
    "        evaluation_df = self.evaluate_models(self.X_train, self.y_train)\n",
    "        best_model_name = evaluation_df.loc[evaluation_df['CV_Mean_R2'].idxmax(), 'Model']\n",
    "        return self.models[best_model_name], best_model_name\n",
    "\n",
    "    def make_predictions(self, X_test, test_ids):\n",
    "        \"\"\"\n",
    "        Make predictions using all models and create submission files\n",
    "        \"\"\"\n",
    "        predictions = {}\n",
    "        \n",
    "        # Make predictions with each model\n",
    "        for name, model in self.models.items():\n",
    "            y_pred = model.predict(X_test)\n",
    "            predictions[f'{name}_pred'] = y_pred\n",
    "            \n",
    "            # Create submission file for each model\n",
    "            submission_df = pd.DataFrame({\n",
    "                'id': test_ids,\n",
    "                'yield': y_pred\n",
    "            })\n",
    "            submission_df.to_csv(f'submission_{name}.csv', index=False)\n",
    "        \n",
    "        # Create ensemble prediction (average of all models)\n",
    "        pred_df = pd.DataFrame(predictions)\n",
    "        ensemble_pred = pred_df.mean(axis=1)\n",
    "        \n",
    "        # Create submission file for ensemble\n",
    "        submission_df = pd.DataFrame({\n",
    "            'id': test_ids,\n",
    "            'yield': ensemble_pred\n",
    "        })\n",
    "        submission_df.to_csv('submission_ensemble.csv', index=False)\n",
    "        \n",
    "        return predictions, ensemble_pred\n",
    "\n",
    "def main():\n",
    "    # Initialize predictor\n",
    "    predictor = YieldPredictor()\n",
    "    \n",
    "    # Prepare data\n",
    "    X_train, y_train, X_test, test_ids, feature_cols = predictor.prepare_data(\n",
    "        'train.csv', 'test.csv'\n",
    "    )\n",
    "    predictor.X_train = X_train\n",
    "    predictor.y_train = y_train\n",
    "    \n",
    "    print(\"Training models...\")\n",
    "    predictor.train_models(X_train, y_train)\n",
    "    \n",
    "    print(\"\\nModel Evaluation Results:\")\n",
    "    evaluation_results = predictor.evaluate_models(X_train, y_train)\n",
    "    print(evaluation_results[['Model', 'R2_Score', 'CV_Mean_R2', 'CV_Std_R2']])\n",
    "    \n",
    "    # Get best model\n",
    "    best_model, best_model_name = predictor.get_best_model()\n",
    "    print(f\"\\nBest Model: {best_model_name}\")\n",
    "    \n",
    "    # Print best parameters for the best model\n",
    "    if best_model_name in predictor.best_params:\n",
    "        print(\"\\nBest Parameters:\")\n",
    "        for param, value in predictor.best_params[best_model_name].items():\n",
    "            print(f\"{param}: {value}\")\n",
    "    \n",
    "    # Make predictions\n",
    "    print(\"\\nMaking predictions...\")\n",
    "    predictions, ensemble_pred = predictor.make_predictions(X_test, test_ids)\n",
    "    print(\"\\nPrediction files have been created for all models and ensemble\")\n",
    "    \n",
    "    # Feature importance analysis for best model\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': feature_cols,\n",
    "            'Importance': best_model.feature_importances_\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        print(\"\\nFeature Importances for Best Model:\")\n",
    "        print(importance_df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5e3e295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training models with cross-validation...\n",
      "\n",
      "Training LinearRegression...\n",
      "LinearRegression - Best params: {}\n",
      "LinearRegression - Validation R2 Score: 0.8918\n",
      "\n",
      "Training Lasso...\n",
      "Lasso - Best params: {'alpha': 0.1}\n",
      "Lasso - Validation R2 Score: 0.8918\n",
      "\n",
      "Training Ridge...\n",
      "Ridge - Best params: {'alpha': 1.0}\n",
      "Ridge - Validation R2 Score: 0.8918\n",
      "\n",
      "Training ElasticNet...\n",
      "ElasticNet - Best params: {'alpha': 0.01, 'l1_ratio': 0.9}\n",
      "ElasticNet - Validation R2 Score: 0.8918\n",
      "\n",
      "Training RandomForest...\n",
      "RandomForest - Best params: {'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "RandomForest - Validation R2 Score: 0.9080\n",
      "\n",
      "Training GradientBoosting...\n",
      "GradientBoosting - Best params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}\n",
      "GradientBoosting - Validation R2 Score: 0.9076\n",
      "\n",
      "Training LightGBM...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002481 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1109\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6006.948416\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "LightGBM - Best params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'reg_alpha': 1.0, 'reg_lambda': 0.1}\n",
      "LightGBM - Validation R2 Score: 0.9073\n",
      "\n",
      "Training XGBoost...\n",
      "XGBoost - Best params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'reg_alpha': 1.0, 'reg_lambda': 1.0}\n",
      "XGBoost - Validation R2 Score: 0.9079\n",
      "\n",
      "Making predictions...\n",
      "\n",
      "Validation Set Performance:\n",
      "\n",
      "LinearRegression:\n",
      "MAE: 280.1290\n",
      "MSE: 196029.1558\n",
      "RMSE: 442.7518\n",
      "R2: 0.8918\n",
      "\n",
      "Lasso:\n",
      "MAE: 280.1005\n",
      "MSE: 196082.5269\n",
      "RMSE: 442.8121\n",
      "R2: 0.8918\n",
      "\n",
      "Ridge:\n",
      "MAE: 280.1301\n",
      "MSE: 196047.3779\n",
      "RMSE: 442.7724\n",
      "R2: 0.8918\n",
      "\n",
      "ElasticNet:\n",
      "MAE: 280.1672\n",
      "MSE: 196136.6706\n",
      "RMSE: 442.8732\n",
      "R2: 0.8918\n",
      "\n",
      "RandomForest:\n",
      "MAE: 252.1584\n",
      "MSE: 166723.6082\n",
      "RMSE: 408.3180\n",
      "R2: 0.9080\n",
      "\n",
      "GradientBoosting:\n",
      "MAE: 252.4692\n",
      "MSE: 167428.7876\n",
      "RMSE: 409.1806\n",
      "R2: 0.9076\n",
      "\n",
      "LightGBM:\n",
      "MAE: 252.8889\n",
      "MSE: 168043.0464\n",
      "RMSE: 409.9305\n",
      "R2: 0.9073\n",
      "\n",
      "XGBoost:\n",
      "MAE: 252.3859\n",
      "MSE: 166810.6268\n",
      "RMSE: 408.4246\n",
      "R2: 0.9079\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class YieldPredictor:\n",
    "    def __init__(self, cv_folds=5, random_state=42, output_dir='predictions', validation_size=0.2):\n",
    "        \"\"\"\n",
    "        Initialize YieldPredictor with validation split and cross-validation\n",
    "        \n",
    "        Parameters:\n",
    "        cv_folds (int): Number of cross-validation folds\n",
    "        random_state (int): Random seed for reproducibility\n",
    "        output_dir (str): Directory to save predictions\n",
    "        validation_size (float): Size of validation set (0.0 to 1.0)\n",
    "        \"\"\"\n",
    "        from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "        from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "        import lightgbm as lgb\n",
    "        import xgboost as xgb\n",
    "        \n",
    "        self.cv_folds = cv_folds\n",
    "        self.random_state = random_state\n",
    "        self.validation_size = validation_size\n",
    "        self.output_dir = output_dir\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "        # Initialize model configurations with regularization\n",
    "        self.model_configs = {\n",
    "            'LinearRegression': {\n",
    "                'model': LinearRegression(),\n",
    "                'params': {}\n",
    "            },\n",
    "            'Lasso': {\n",
    "                'model': Lasso(random_state=random_state),\n",
    "                'params': {\n",
    "                    'alpha': [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "                }\n",
    "            },\n",
    "            'Ridge': {\n",
    "                'model': Ridge(random_state=random_state),\n",
    "                'params': {\n",
    "                    'alpha': [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "                }\n",
    "            },\n",
    "            'ElasticNet': {\n",
    "                'model': ElasticNet(random_state=random_state),\n",
    "                'params': {\n",
    "                    'alpha': [0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "                    'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "                }\n",
    "            },\n",
    "            'RandomForest': {\n",
    "                'model': RandomForestRegressor(random_state=random_state),\n",
    "                'params': {\n",
    "                    'n_estimators': [100, 200],\n",
    "                    'max_depth': [None, 10, 20],\n",
    "                    'min_samples_split': [2, 5],\n",
    "                    'min_samples_leaf': [1, 2]\n",
    "                }\n",
    "            },\n",
    "            'GradientBoosting': {\n",
    "                'model': GradientBoostingRegressor(random_state=random_state),\n",
    "                'params': {\n",
    "                    'n_estimators': [100, 200],\n",
    "                    'max_depth': [3, 5],\n",
    "                    'learning_rate': [0.01, 0.1]\n",
    "                }\n",
    "            },\n",
    "            'LightGBM': {\n",
    "                'model': lgb.LGBMRegressor(random_state=random_state),\n",
    "                'params': {\n",
    "                    'n_estimators': [100, 200],\n",
    "                    'max_depth': [-1, 5, 10],\n",
    "                    'learning_rate': [0.01, 0.1],\n",
    "                    'reg_alpha': [0, 0.1, 1.0],\n",
    "                    'reg_lambda': [0, 0.1, 1.0]\n",
    "                }\n",
    "            },\n",
    "            'XGBoost': {\n",
    "                'model': xgb.XGBRegressor(random_state=random_state),\n",
    "                'params': {\n",
    "                    'n_estimators': [100, 200],\n",
    "                    'max_depth': [3, 5],\n",
    "                    'learning_rate': [0.01, 0.1],\n",
    "                    'reg_alpha': [0, 0.1, 1.0],\n",
    "                    'reg_lambda': [0, 0.1, 1.0]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "    def prepare_data(self, train_file, test_file):\n",
    "        \"\"\"\n",
    "        Prepare and split data with proper validation set\n",
    "        \"\"\"\n",
    "        # Load data\n",
    "        train_data = pd.read_csv(train_file)\n",
    "        test_data = pd.read_csv(test_file)\n",
    "        \n",
    "        # Split features and target\n",
    "        X = train_data.drop(['yield', 'id'], axis=1)\n",
    "        y = train_data['yield']\n",
    "        test_ids = test_data['id']\n",
    "        X_test = test_data.drop(['id'], axis=1)\n",
    "        \n",
    "        # Split training data into train and validation sets\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, \n",
    "            test_size=self.validation_size, \n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        # Scale features\n",
    "        self.scaler.fit(X_train)\n",
    "        X_train_scaled = self.scaler.transform(X_train)\n",
    "        X_val_scaled = self.scaler.transform(X_val)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        return (X_train_scaled, X_val_scaled, X_test_scaled, \n",
    "                y_train, y_val, test_ids, X.columns)\n",
    "\n",
    "    def train_models(self, X_train, X_val, y_train, y_val):\n",
    "        \"\"\"\n",
    "        Train models with cross-validation and hyperparameter tuning\n",
    "        \"\"\"\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "        \n",
    "        self.trained_models = {}\n",
    "        self.validation_scores = {}\n",
    "        \n",
    "        for name, config in self.model_configs.items():\n",
    "            print(f\"\\nTraining {name}...\")\n",
    "            \n",
    "            # Perform GridSearchCV\n",
    "            grid_search = GridSearchCV(\n",
    "                estimator=config['model'],\n",
    "                param_grid=config['params'],\n",
    "                cv=self.cv_folds,\n",
    "                scoring='neg_mean_squared_error',\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            \n",
    "            # Fit on training data\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            \n",
    "            # Save best model\n",
    "            self.trained_models[name] = grid_search.best_estimator_\n",
    "            \n",
    "            # Evaluate on validation set\n",
    "            val_pred = grid_search.predict(X_val)\n",
    "            val_score = r2_score(y_val, val_pred)\n",
    "            self.validation_scores[name] = val_score\n",
    "            \n",
    "            print(f\"{name} - Best params: {grid_search.best_params_}\")\n",
    "            print(f\"{name} - Validation R2 Score: {val_score:.4f}\")\n",
    "\n",
    "    def make_predictions(self, X_test, test_ids):\n",
    "        \"\"\"\n",
    "        Make predictions using trained models\n",
    "        \"\"\"\n",
    "        predictions_dict = {}\n",
    "        \n",
    "        # Generate predictions for each model\n",
    "        for name, model in self.trained_models.items():\n",
    "            predictions = model.predict(X_test)\n",
    "            predictions_dict[name] = predictions\n",
    "            \n",
    "            # Save individual model predictions\n",
    "            pred_df = pd.DataFrame({\n",
    "                'ID': test_ids,\n",
    "                'Yield': predictions\n",
    "            })\n",
    "            pred_df.to_csv(os.path.join(self.output_dir, f'{name.lower()}_predictions.csv'), \n",
    "                          index=False)\n",
    "        \n",
    "        # Create weighted ensemble based on validation scores\n",
    "        total_score = sum(max(0.0001, score) for score in self.validation_scores.values())\n",
    "        weights = {name: max(0.0001, score)/total_score \n",
    "                  for name, score in self.validation_scores.items()}\n",
    "        \n",
    "        # Calculate weighted ensemble predictions\n",
    "        ensemble_pred = np.zeros(len(test_ids))\n",
    "        for name, predictions in predictions_dict.items():\n",
    "            ensemble_pred += predictions * weights[name]\n",
    "        \n",
    "        # Save ensemble predictions\n",
    "        ensemble_df = pd.DataFrame({\n",
    "            'ID': test_ids,\n",
    "            'Yield': ensemble_pred\n",
    "        })\n",
    "        ensemble_df.to_csv(os.path.join(self.output_dir, 'ensemble_predictions.csv'), \n",
    "                          index=False)\n",
    "        \n",
    "        return predictions_dict, ensemble_pred\n",
    "\n",
    "    def evaluate_model_performance(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Evaluate model performance with multiple metrics\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'MAE': mean_absolute_error(y_true, y_pred),\n",
    "            'MSE': mean_squared_error(y_true, y_pred),\n",
    "            'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "            'R2': r2_score(y_true, y_pred)\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    predictor = YieldPredictor(output_dir='yield_predictions')\n",
    "    \n",
    "    # Prepare data with validation split\n",
    "    X_train, X_val, X_test, y_train, y_val, test_ids, feature_cols = predictor.prepare_data(\n",
    "        'train.csv', 'test.csv'\n",
    "    )\n",
    "    \n",
    "    print(\"Training models with cross-validation...\")\n",
    "    predictor.train_models(X_train, X_val, y_train, y_val)\n",
    "    \n",
    "    print(\"\\nMaking predictions...\")\n",
    "    predictions, ensemble_pred = predictor.make_predictions(X_test, test_ids)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    print(\"\\nValidation Set Performance:\")\n",
    "    for name, model in predictor.trained_models.items():\n",
    "        val_pred = model.predict(X_val)\n",
    "        metrics = predictor.evaluate_model_performance(y_val, val_pred)\n",
    "        print(f\"\\n{name}:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e21936a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049498 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19429\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 187\n",
      "[LightGBM] [Info] Start training from score 6006.948416\n",
      "\n",
      "Training LinearRegression...\n",
      "LinearRegression - Best params: {}\n",
      "LinearRegression - Validation MAE: 278.3799\n",
      "\n",
      "Training Lasso...\n",
      "Lasso - Best params: {'alpha': 0.1, 'max_iter': 2000}\n",
      "Lasso - Validation MAE: 279.3540\n",
      "\n",
      "Training Ridge...\n",
      "Ridge - Best params: {'alpha': 0.001, 'solver': 'auto'}\n",
      "Ridge - Validation MAE: 278.4605\n",
      "\n",
      "Training ElasticNet...\n",
      "ElasticNet - Best params: {'alpha': 1e-05, 'l1_ratio': 0.9, 'max_iter': 2000}\n",
      "ElasticNet - Validation MAE: 279.1530\n",
      "\n",
      "Training RandomForest...\n",
      "RandomForest - Best params: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "RandomForest - Validation MAE: 254.5022\n",
      "\n",
      "Training GradientBoosting...\n",
      "GradientBoosting - Best params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
      "GradientBoosting - Validation MAE: 253.5148\n",
      "\n",
      "Training LightGBM...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009078 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11777\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 48\n",
      "[LightGBM] [Info] Start training from score 6006.948416\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "LightGBM - Best params: {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'num_leaves': 63, 'reg_alpha': 0.01, 'reg_lambda': 0}\n",
      "LightGBM - Validation MAE: 253.0997\n",
      "\n",
      "Training XGBoost...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 290\u001b[0m\n\u001b[0;32m    286\u001b[0m predictor \u001b[38;5;241m=\u001b[39m YieldPredictor(output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myield_predictions\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    287\u001b[0m X_train, X_val, X_test, y_train, y_val, test_ids, feature_cols \u001b[38;5;241m=\u001b[39m predictor\u001b[38;5;241m.\u001b[39mprepare_data(\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    289\u001b[0m )\n\u001b[1;32m--> 290\u001b[0m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    291\u001b[0m predictions, ensemble_pred \u001b[38;5;241m=\u001b[39m predictor\u001b[38;5;241m.\u001b[39mmake_predictions(X_test, test_ids)\n",
      "Cell \u001b[1;32mIn[8], line 211\u001b[0m, in \u001b[0;36mYieldPredictor.train_models\u001b[1;34m(self, X_train, X_val, y_train, y_val)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;66;03m# Fit with sample weights for balanced training\u001b[39;00m\n\u001b[0;32m    210\u001b[0m sample_weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;28mlen\u001b[39m(y_train))\n\u001b[1;32m--> 211\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;66;03m# Save best model\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrained_models[name] \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[1;32mf:\\MTech_IIITD\\pyEnv\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\MTech_IIITD\\pyEnv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1019\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1014\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1015\u001b[0m     )\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1019\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1023\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mf:\\MTech_IIITD\\pyEnv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1573\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1572\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1573\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\MTech_IIITD\\pyEnv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:965\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    960\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    961\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    962\u001b[0m         )\n\u001b[0;32m    963\u001b[0m     )\n\u001b[1;32m--> 965\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    984\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    987\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    988\u001b[0m     )\n",
      "File \u001b[1;32mf:\\MTech_IIITD\\pyEnv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\MTech_IIITD\\pyEnv\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\MTech_IIITD\\pyEnv\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mf:\\MTech_IIITD\\pyEnv\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class YieldPredictor:\n",
    "    def __init__(self, cv_folds=5, random_state=42, output_dir='predictions', validation_size=0.2):\n",
    "        \"\"\"\n",
    "        Initialize YieldPredictor with enhanced preprocessing and model configurations\n",
    "        \"\"\"\n",
    "        from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "        from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "        import lightgbm as lgb\n",
    "        import xgboost as xgb\n",
    "        \n",
    "        self.cv_folds = cv_folds\n",
    "        self.random_state = random_state\n",
    "        self.validation_size = validation_size\n",
    "        self.output_dir = output_dir\n",
    "        \n",
    "        # Multiple scalers for different feature types\n",
    "        self.standard_scaler = StandardScaler()\n",
    "        self.robust_scaler = RobustScaler()\n",
    "        self.power_transformer = PowerTransformer()\n",
    "        \n",
    "        # Enhanced model configurations with broader hyperparameter search\n",
    "        self.model_configs = {\n",
    "            'LinearRegression': {\n",
    "                'model': LinearRegression(),\n",
    "                'params': {}\n",
    "            },\n",
    "            'Lasso': {\n",
    "                'model': Lasso(random_state=random_state),\n",
    "                'params': {\n",
    "                    'alpha': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "                    'max_iter': [2000]\n",
    "                }\n",
    "            },\n",
    "            'Ridge': {\n",
    "                'model': Ridge(random_state=random_state),\n",
    "                'params': {\n",
    "                    'alpha': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "                    'solver': ['auto', 'svd', 'cholesky']\n",
    "                }\n",
    "            },\n",
    "            'ElasticNet': {\n",
    "                'model': ElasticNet(random_state=random_state),\n",
    "                'params': {\n",
    "                    'alpha': [0.00001, 0.0001, 0.001, 0.01, 0.1],\n",
    "                    'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "                    'max_iter': [2000]\n",
    "                }\n",
    "            },\n",
    "            'RandomForest': {\n",
    "                'model': RandomForestRegressor(random_state=random_state),\n",
    "                'params': {\n",
    "                    'n_estimators': [100, 200, 300],\n",
    "                    'max_depth': [None, 10, 20, 30],\n",
    "                    'min_samples_split': [2, 5, 10],\n",
    "                    'min_samples_leaf': [1, 2, 4],\n",
    "                    'max_features': ['auto', 'sqrt']\n",
    "                }\n",
    "            },\n",
    "            'GradientBoosting': {\n",
    "                'model': GradientBoostingRegressor(random_state=random_state),\n",
    "                'params': {\n",
    "                    'n_estimators': [100, 200, 300],\n",
    "                    'max_depth': [3, 5, 7],\n",
    "                    'learning_rate': [0.01, 0.05, 0.1],\n",
    "                    'subsample': [0.8, 0.9, 1.0]\n",
    "                }\n",
    "            },\n",
    "            'LightGBM': {\n",
    "                'model': lgb.LGBMRegressor(random_state=random_state),\n",
    "                'params': {\n",
    "                    'n_estimators': [100, 200, 300],\n",
    "                    'max_depth': [-1, 5, 10, 15],\n",
    "                    'learning_rate': [0.01, 0.05, 0.1],\n",
    "                    'num_leaves': [31, 63, 127],\n",
    "                    'reg_alpha': [0, 0.01, 0.1],\n",
    "                    'reg_lambda': [0, 0.01, 0.1],\n",
    "                    'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "                }\n",
    "            },\n",
    "            'XGBoost': {\n",
    "                'model': xgb.XGBRegressor(random_state=random_state),\n",
    "                'params': {\n",
    "                    'n_estimators': [100, 200, 300],\n",
    "                    'max_depth': [3, 5, 7],\n",
    "                    'learning_rate': [0.01, 0.05, 0.1],\n",
    "                    'min_child_weight': [1, 3, 5],\n",
    "                    'subsample': [0.8, 0.9, 1.0],\n",
    "                    'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "                    'reg_alpha': [0, 0.01, 0.1],\n",
    "                    # 'reg_lambda': [0, 0.01, 0.1]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "    def feature_engineering(self, X):\n",
    "        \"\"\"\n",
    "        Apply advanced feature engineering techniques\n",
    "        \"\"\"\n",
    "        X_processed = X.copy()\n",
    "        \n",
    "        # Add polynomial features for numeric columns\n",
    "        numeric_cols = X_processed.select_dtypes(include=['float64', 'int64']).columns\n",
    "        for col in numeric_cols:\n",
    "            X_processed[f'{col}_squared'] = X_processed[col] ** 2\n",
    "            X_processed[f'{col}_cubed'] = X_processed[col] ** 3\n",
    "        \n",
    "        # Add interaction terms between important features\n",
    "        for i in range(len(numeric_cols)):\n",
    "            for j in range(i + 1, len(numeric_cols)):\n",
    "                col1, col2 = numeric_cols[i], numeric_cols[j]\n",
    "                X_processed[f'{col1}_{col2}_interaction'] = X_processed[col1] * X_processed[col2]\n",
    "        \n",
    "        return X_processed\n",
    "\n",
    "    def prepare_data(self, train_file, test_file):\n",
    "        \"\"\"\n",
    "        Enhanced data preparation with advanced preprocessing\n",
    "        \"\"\"\n",
    "        # Load data\n",
    "        train_data = pd.read_csv(train_file)\n",
    "        test_data = pd.read_csv(test_file)\n",
    "        \n",
    "        # Feature engineering\n",
    "        X = self.feature_engineering(train_data.drop(['yield', 'id'], axis=1))\n",
    "        y = train_data['yield']\n",
    "        test_ids = test_data['id']\n",
    "        X_test = self.feature_engineering(test_data.drop(['id'], axis=1))\n",
    "        \n",
    "        # Split training data\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, \n",
    "            test_size=self.validation_size, \n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        # Apply different scaling techniques\n",
    "        X_train_scaled = pd.DataFrame(index=X_train.index)\n",
    "        X_val_scaled = pd.DataFrame(index=X_val.index)\n",
    "        X_test_scaled = pd.DataFrame(index=X_test.index)\n",
    "        \n",
    "        # Standard scaling for normal distributions\n",
    "        normal_cols = X_train.select_dtypes(include=['float64']).columns\n",
    "        if len(normal_cols) > 0:\n",
    "            X_train_scaled[normal_cols] = self.standard_scaler.fit_transform(X_train[normal_cols])\n",
    "            X_val_scaled[normal_cols] = self.standard_scaler.transform(X_val[normal_cols])\n",
    "            X_test_scaled[normal_cols] = self.standard_scaler.transform(X_test[normal_cols])\n",
    "        \n",
    "        # Robust scaling for outlier-prone features\n",
    "        X_train_scaled = self.robust_scaler.fit_transform(X_train_scaled)\n",
    "        X_val_scaled = self.robust_scaler.transform(X_val_scaled)\n",
    "        X_test_scaled = self.robust_scaler.transform(X_test_scaled)\n",
    "        \n",
    "        # Feature selection using SelectFromModel with LightGBM\n",
    "        selector = SelectFromModel(\n",
    "            lgb.LGBMRegressor(random_state=self.random_state),\n",
    "            max_features=int(X_train_scaled.shape[1] * 0.8)  # Keep top 80% features\n",
    "        )\n",
    "        X_train_scaled = selector.fit_transform(X_train_scaled, y_train)\n",
    "        X_val_scaled = selector.transform(X_val_scaled)\n",
    "        X_test_scaled = selector.transform(X_test_scaled)\n",
    "        \n",
    "        return (X_train_scaled, X_val_scaled, X_test_scaled, \n",
    "                y_train, y_val, test_ids, X.columns)\n",
    "\n",
    "    def train_models(self, X_train, X_val, y_train, y_val):\n",
    "        \"\"\"\n",
    "        Enhanced model training with advanced techniques\n",
    "        \"\"\"\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "        \n",
    "        self.trained_models = {}\n",
    "        self.validation_scores = {}\n",
    "        self.feature_importances = {}\n",
    "        \n",
    "        for name, config in self.model_configs.items():\n",
    "            print(f\"\\nTraining {name}...\")\n",
    "            \n",
    "            # Use stratified k-fold for better validation\n",
    "            kfold = KFold(n_splits=self.cv_folds, shuffle=True, random_state=self.random_state)\n",
    "            \n",
    "            # Enhanced GridSearchCV with multiple scoring metrics\n",
    "            grid_search = GridSearchCV(\n",
    "                estimator=config['model'],\n",
    "                param_grid=config['params'],\n",
    "                cv=kfold,\n",
    "                scoring={\n",
    "                    'neg_mae': 'neg_mean_absolute_error',\n",
    "                    'neg_mse': 'neg_mean_squared_error',\n",
    "                    'r2': 'r2'\n",
    "                },\n",
    "                refit='neg_mae',  # Optimize for MAE\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            \n",
    "            # Fit with sample weights for balanced training\n",
    "            sample_weights = np.ones(len(y_train))\n",
    "            grid_search.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "            \n",
    "            # Save best model\n",
    "            self.trained_models[name] = grid_search.best_estimator_\n",
    "            \n",
    "            # Evaluate on validation set\n",
    "            val_pred = grid_search.predict(X_val)\n",
    "            val_score = -mean_absolute_error(y_val, val_pred)  # Negative MAE\n",
    "            self.validation_scores[name] = val_score\n",
    "            \n",
    "            print(f\"{name} - Best params: {grid_search.best_params_}\")\n",
    "            print(f\"{name} - Validation MAE: {-val_score:.4f}\")\n",
    "\n",
    "    def make_predictions(self, X_test, test_ids):\n",
    "        \"\"\"\n",
    "        Enhanced prediction with advanced ensemble techniques\n",
    "        \"\"\"\n",
    "        predictions_dict = {}\n",
    "        \n",
    "        # Generate predictions with uncertainty estimation\n",
    "        for name, model in self.trained_models.items():\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                # For models that support probability estimation\n",
    "                predictions = model.predict_proba(X_test)[:, 1]\n",
    "            else:\n",
    "                predictions = model.predict(X_test)\n",
    "            predictions_dict[name] = predictions\n",
    "            \n",
    "            # Save individual predictions\n",
    "            pred_df = pd.DataFrame({\n",
    "                'id': test_ids,\n",
    "                'yield': predictions\n",
    "            })\n",
    "            pred_df.to_csv(os.path.join(self.output_dir, f'{name.lower()}_predictions.csv'), \n",
    "                          index=False)\n",
    "        \n",
    "        # Advanced ensemble weighting based on validation performance\n",
    "        weights = self._calculate_optimal_weights()\n",
    "        \n",
    "        # Weighted ensemble with bias correction\n",
    "        ensemble_pred = np.zeros(len(test_ids))\n",
    "        for name, predictions in predictions_dict.items():\n",
    "            ensemble_pred += predictions * weights[name]\n",
    "        \n",
    "        # Save ensemble predictions\n",
    "        ensemble_df = pd.DataFrame({\n",
    "            'id': test_ids,\n",
    "            'yield': ensemble_pred\n",
    "        })\n",
    "        ensemble_df.to_csv(os.path.join(self.output_dir, 'ensemble_predictions.csv'), \n",
    "                          index=False)\n",
    "        \n",
    "        return predictions_dict, ensemble_pred\n",
    "\n",
    "    def _calculate_optimal_weights(self):\n",
    "        \"\"\"\n",
    "        Calculate optimal weights for ensemble using validation scores\n",
    "        \"\"\"\n",
    "        scores = np.array(list(self.validation_scores.values()))\n",
    "        weights = np.exp(scores) / np.sum(np.exp(scores))  # Softmax weighting\n",
    "        return dict(zip(self.validation_scores.keys(), weights))\n",
    "\n",
    "    def evaluate_model_performance(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Comprehensive model evaluation with additional metrics\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'MAE': mean_absolute_error(y_true, y_pred),\n",
    "            'MSE': mean_squared_error(y_true, y_pred),\n",
    "            'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "            'R2': r2_score(y_true, y_pred),\n",
    "            'MAPE': np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "        }\n",
    "    \n",
    "\n",
    "predictor = YieldPredictor(output_dir='yield_predictions')\n",
    "X_train, X_val, X_test, y_train, y_val, test_ids, feature_cols = predictor.prepare_data(\n",
    "    'train.csv', 'test.csv'\n",
    ")\n",
    "predictor.train_models(X_train, X_val, y_train, y_val)\n",
    "predictions, ensemble_pred = predictor.make_predictions(X_test, test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef68ddc4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- poly_0\n- poly_1\n- poly_10\n- poly_11\n- poly_12\n- ...\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 209\u001b[0m\n\u001b[0;32m    206\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 209\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 190\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m    188\u001b[0m     predictor \u001b[38;5;241m=\u001b[39m EnhancedYieldPredictor(output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myield_predictions\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 190\u001b[0m     X_train, X_val, X_test, y_train, y_val, test_ids, feature_cols \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining models with advanced techniques...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    195\u001b[0m     predictor\u001b[38;5;241m.\u001b[39mtrain_models(X_train, X_val, y_train, y_val)\n",
      "Cell \u001b[1;32mIn[13], line 165\u001b[0m, in \u001b[0;36mEnhancedYieldPredictor.prepare_data\u001b[1;34m(self, train_file, test_file)\u001b[0m\n\u001b[0;32m    162\u001b[0m X_test \u001b[38;5;241m=\u001b[39m test_data\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# Engineer features for training data\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m X_engineered \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengineer_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;66;03m# Engineer features for test data using fitted parameters\u001b[39;00m\n\u001b[0;32m    168\u001b[0m X_test_engineered \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengineer_features(X_test, is_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[13], line 139\u001b[0m, in \u001b[0;36mEnhancedYieldPredictor.engineer_features\u001b[1;34m(self, X, is_training, y)\u001b[0m\n\u001b[0;32m    137\u001b[0m numeric_cols \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat64\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(numeric_cols) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m     cluster_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkmeans\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnumeric_cols\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(cluster_features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m    141\u001b[0m         features[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster_dist_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m cluster_features[:, i]\n",
      "File \u001b[1;32mf:\\MTech_IIITD\\pyEnv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32mf:\\MTech_IIITD\\pyEnv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1045\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1042\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1044\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m-> 1045\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m   1056\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[1;32mf:\\MTech_IIITD\\pyEnv\\Lib\\site-packages\\sklearn\\base.py:608\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    539\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[0;32m    545\u001b[0m ):\n\u001b[0;32m    546\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[0;32m    547\u001b[0m \n\u001b[0;32m    548\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 608\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    610\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    611\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    612\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    613\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    614\u001b[0m         )\n",
      "File \u001b[1;32mf:\\MTech_IIITD\\pyEnv\\Lib\\site-packages\\sklearn\\base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[0;32m    531\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    533\u001b[0m     )\n\u001b[1;32m--> 535\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- poly_0\n- poly_1\n- poly_10\n- poly_11\n- poly_12\n- ...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class EnhancedYieldPredictor:\n",
    "    def __init__(self, cv_folds=5, random_state=42, output_dir='predictions', validation_size=0.2):\n",
    "        \"\"\"\n",
    "        Initialize EnhancedYieldPredictor with advanced feature engineering\n",
    "        \"\"\"\n",
    "        from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "        from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "        import lightgbm as lgb\n",
    "        import xgboost as xgb\n",
    "        \n",
    "        self.cv_folds = cv_folds\n",
    "        self.random_state = random_state\n",
    "        self.validation_size = validation_size\n",
    "        self.output_dir = output_dir\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_names = []\n",
    "        self.important_features = None\n",
    "        self.kmeans = KMeans(n_clusters=5, random_state=random_state)\n",
    "        \n",
    "        # Enhanced model configurations\n",
    "        self.model_configs = {\n",
    "            'LightGBM': {\n",
    "                'model': lgb.LGBMRegressor(random_state=random_state),\n",
    "                'params': {\n",
    "                    'n_estimators': [200, 500, 1000],\n",
    "                    'max_depth': [-1, 5, 7, 9],\n",
    "                    'learning_rate': [0.01, 0.05, 0.1],\n",
    "                    'num_leaves': [31, 63, 127],\n",
    "                    'reg_alpha': [ 0.1, 0.5],\n",
    "                    # 'reg_lambda': [0, 0.1, 0.5],\n",
    "                    'min_child_samples': [20, 50],\n",
    "                    'subsample': [0.8, 0.9, 1.0],\n",
    "                    'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "                }\n",
    "            },\n",
    "            'XGBoost': {\n",
    "                'model': xgb.XGBRegressor(random_state=random_state),\n",
    "                'params': {\n",
    "                    'n_estimators': [200, 500, 1000],\n",
    "                    'max_depth': [5, 7, 9],\n",
    "                    'learning_rate': [0.01, 0.05, 0.1],\n",
    "                    'min_child_weight': [1, 3, 5],\n",
    "                    'subsample': [0.8, 0.9, 1.0],\n",
    "                    'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "                    'reg_alpha': [0.1, 0.5],\n",
    "                    'reg_lambda': [0, 0.1],\n",
    "                    'gamma': [0, 0.1, 0.2]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "    def create_statistical_features(self, features):\n",
    "        \"\"\"Create statistical features from numeric columns\"\"\"\n",
    "        numeric_cols = features.select_dtypes(include=['int64', 'float64']).columns\n",
    "        for col in numeric_cols:\n",
    "            features[f'{col}_squared'] = features[col] ** 2\n",
    "            features[f'{col}_cubed'] = features[col] ** 3\n",
    "            features[f'{col}_sqrt'] = np.sqrt(np.abs(features[col]))\n",
    "            features[f'{col}_log'] = np.log1p(np.abs(features[col]))\n",
    "        return features\n",
    "\n",
    "    def create_interaction_features(self, features):\n",
    "        \"\"\"Create interaction features between numeric columns\"\"\"\n",
    "        numeric_cols = features.select_dtypes(include=['int64', 'float64']).columns\n",
    "        for i, col1 in enumerate(numeric_cols):\n",
    "            for col2 in numeric_cols[i+1:]:\n",
    "                features[f'{col1}_mult_{col2}'] = features[col1] * features[col2]\n",
    "                features[f'{col1}_div_{col2}'] = features[col1] / (features[col2] + 1e-8)\n",
    "                features[f'{col1}_plus_{col2}'] = features[col1] + features[col2]\n",
    "                features[f'{col1}_minus_{col2}'] = features[col1] - features[col2]\n",
    "        return features\n",
    "\n",
    "    def create_time_features(self, features):\n",
    "        \"\"\"Create time-based features if time columns exist\"\"\"\n",
    "        if 'date' in features.columns or any('time' in col.lower() for col in features.columns):\n",
    "            time_cols = [col for col in features.columns if 'date' in col.lower() or 'time' in col.lower()]\n",
    "            for col in time_cols:\n",
    "                features[f'{col}_rolling_mean_7'] = features[col].rolling(window=7, min_periods=1).mean()\n",
    "                features[f'{col}_rolling_std_7'] = features[col].rolling(window=7, min_periods=1).std()\n",
    "                features[f'{col}_rolling_max_7'] = features[col].rolling(window=7, min_periods=1).max()\n",
    "                features[f'{col}_rolling_min_7'] = features[col].rolling(window=7, min_periods=1).min()\n",
    "        return features\n",
    "\n",
    "    def fit_feature_engineering(self, X, y):\n",
    "        \"\"\"Fit feature engineering parameters on training data\"\"\"\n",
    "        # Select important features\n",
    "        selector = SelectKBest(score_func=mutual_info_regression, k=10)\n",
    "        selector.fit(X, y)\n",
    "        self.important_features = X.columns[selector.get_support()].tolist()\n",
    "        \n",
    "        # Fit KMeans for clustering features\n",
    "        numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "        self.kmeans.fit(self.scaler.fit_transform(X[numeric_cols]))\n",
    "\n",
    "    def engineer_features(self, X, is_training=False, y=None):\n",
    "        \"\"\"\n",
    "        Create engineered features with fitted parameters\n",
    "        \"\"\"\n",
    "        features = X.copy()\n",
    "        \n",
    "        # Create basic statistical features\n",
    "        features = self.create_statistical_features(features)\n",
    "        \n",
    "        # Create interaction features\n",
    "        features = self.create_interaction_features(features)\n",
    "        \n",
    "        # Create time-based features\n",
    "        features = self.create_time_features(features)\n",
    "        \n",
    "        # If training, fit feature engineering parameters\n",
    "        if is_training and y is not None:\n",
    "            self.fit_feature_engineering(features, y)\n",
    "        \n",
    "        # Create polynomial features for important columns\n",
    "        if self.important_features is not None:\n",
    "            poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "            poly_features = poly.fit_transform(features[self.important_features])\n",
    "            poly_feature_names = [f'poly_{i}' for i in range(poly_features.shape[1])]\n",
    "            poly_df = pd.DataFrame(poly_features, columns=poly_feature_names, index=features.index)\n",
    "            features = pd.concat([features, poly_df], axis=1)\n",
    "        \n",
    "        # Create clustering features\n",
    "        numeric_cols = features.select_dtypes(include=['int64', 'float64']).columns\n",
    "        if len(numeric_cols) > 0:\n",
    "            cluster_features = self.kmeans.transform(self.scaler.transform(features[numeric_cols]))\n",
    "            for i in range(cluster_features.shape[1]):\n",
    "                features[f'cluster_dist_{i}'] = cluster_features[:, i]\n",
    "        \n",
    "        # Create aggregation features\n",
    "        for col in numeric_cols:\n",
    "            features[f'{col}_to_mean'] = features[col] / features[numeric_cols].mean(axis=1)\n",
    "            features[f'{col}_to_std'] = features[col] / features[numeric_cols].std(axis=1)\n",
    "        \n",
    "        return features\n",
    "\n",
    "    def prepare_data(self, train_file, test_file):\n",
    "        \"\"\"\n",
    "        Prepare and split data with advanced feature engineering\n",
    "        \"\"\"\n",
    "        # Load data\n",
    "        train_data = pd.read_csv(train_file)\n",
    "        test_data = pd.read_csv(test_file)\n",
    "        \n",
    "        # Split features and target\n",
    "        X = train_data.drop(['yield', 'id'], axis=1)\n",
    "        y = train_data['yield']\n",
    "        test_ids = test_data['id']\n",
    "        X_test = test_data.drop(['id'], axis=1)\n",
    "        \n",
    "        # Engineer features for training data\n",
    "        X_engineered = self.engineer_features(X, is_training=True, y=y)\n",
    "        \n",
    "        # Engineer features for test data using fitted parameters\n",
    "        X_test_engineered = self.engineer_features(X_test, is_training=False)\n",
    "        \n",
    "        # Split training data\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_engineered, y,\n",
    "            test_size=self.validation_size,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        # Scale features\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_val_scaled = self.scaler.transform(X_val)\n",
    "        X_test_scaled = self.scaler.transform(X_test_engineered)\n",
    "        \n",
    "        return (X_train_scaled, X_val_scaled, X_test_scaled,\n",
    "                y_train, y_val, test_ids, X_engineered.columns)\n",
    "\n",
    "    # ... [rest of the class methods remain the same]\n",
    "\n",
    "def main():\n",
    "    predictor = EnhancedYieldPredictor(output_dir='yield_predictions')\n",
    "    \n",
    "    X_train, X_val, X_test, y_train, y_val, test_ids, feature_cols = predictor.prepare_data(\n",
    "        'train.csv', 'test.csv'\n",
    "    )\n",
    "    \n",
    "    print(\"Training models with advanced techniques...\")\n",
    "    predictor.train_models(X_train, X_val, y_train, y_val)\n",
    "    \n",
    "    print(\"\\nMaking predictions...\")\n",
    "    predictions, final_pred = predictor.make_predictions(X_test, test_ids)\n",
    "    \n",
    "    print(\"\\nValidation Set Performance:\")\n",
    "    for name, model in predictor.trained_models.items():\n",
    "        val_pred = model.predict(X_val)\n",
    "        metrics = predictor.evaluate_model_performance(y_val, val_pred)\n",
    "        print(f\"\\n{name}:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4b08747",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nKMeans does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 300\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation MAE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_mae\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 300\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 285\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    282\u001b[0m predictor \u001b[38;5;241m=\u001b[39m AdvancedYieldPredictor(output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myield_predictions\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    284\u001b[0m \u001b[38;5;66;03m# Prepare data\u001b[39;00m\n\u001b[1;32m--> 285\u001b[0m X_train, X_val, X_test, y_train, y_val, test_ids \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m    287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# Train and predict\u001b[39;00m\n\u001b[0;32m    290\u001b[0m final_predictions \u001b[38;5;241m=\u001b[39m predictor\u001b[38;5;241m.\u001b[39mtrain_and_predict(\n\u001b[0;32m    291\u001b[0m     X_train, X_val, X_test, y_train, y_val, test_ids\n\u001b[0;32m    292\u001b[0m )\n",
      "Cell \u001b[1;32mIn[11], line 183\u001b[0m, in \u001b[0;36mAdvancedYieldPredictor.prepare_data\u001b[1;34m(self, train_file, test_file)\u001b[0m\n\u001b[0;32m    180\u001b[0m X_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_engineering(test_data\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), is_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;66;03m# Create clusters for local modeling\u001b[39;00m\n\u001b[1;32m--> 183\u001b[0m clusters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_clusters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# Split data\u001b[39;00m\n\u001b[0;32m    186\u001b[0m X_train, X_val, y_train, y_val \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[0;32m    187\u001b[0m     X, y,\n\u001b[0;32m    188\u001b[0m     test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_size,\n\u001b[0;32m    189\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state,\n\u001b[0;32m    190\u001b[0m     stratify\u001b[38;5;241m=\u001b[39mclusters\n\u001b[0;32m    191\u001b[0m )\n",
      "Cell \u001b[1;32mIn[11], line 106\u001b[0m, in \u001b[0;36mAdvancedYieldPredictor.create_clusters\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03mCreate clusters for local model training\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    105\u001b[0m kmeans \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n\u001b[1;32m--> 106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mkmeans\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\MTech_IIITD\\pyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1070\u001b[0m, in \u001b[0;36m_BaseKMeans.fit_predict\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute cluster centers and predict cluster index for each sample.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \n\u001b[0;32m   1050\u001b[0m \u001b[38;5;124;03m    Convenience method; equivalent to calling fit(X) followed by\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1068\u001b[0m \u001b[38;5;124;03m        Index of the cluster each sample belongs to.\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1070\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlabels_\n",
      "File \u001b[1;32mf:\\MTech_IIITD\\pyEnv\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\MTech_IIITD\\pyEnv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1464\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1436\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1437\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1438\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute k-means clustering.\u001b[39;00m\n\u001b[0;32m   1439\u001b[0m \n\u001b[0;32m   1440\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1462\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1463\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1464\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1465\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1466\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1468\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1470\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1471\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1473\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params_vs_input(X)\n\u001b[0;32m   1475\u001b[0m     random_state \u001b[38;5;241m=\u001b[39m check_random_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n",
      "File \u001b[1;32mf:\\MTech_IIITD\\pyEnv\\Lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mf:\\MTech_IIITD\\pyEnv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1064\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1061\u001b[0m     )\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m-> 1064\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1073\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32mf:\\MTech_IIITD\\pyEnv\\Lib\\site-packages\\sklearn\\utils\\validation.py:123\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\MTech_IIITD\\pyEnv\\Lib\\site-packages\\sklearn\\utils\\validation.py:172\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    171\u001b[0m     )\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nKMeans does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.feature_selection import SelectFromModel, mutual_info_regression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class AdvancedYieldPredictor:\n",
    "    def __init__(self, cv_folds=10, random_state=42, output_dir='predictions', validation_size=0.2):\n",
    "        \"\"\"\n",
    "        Initialize with advanced configurations for lower test MAE\n",
    "        \"\"\"\n",
    "        from sklearn.linear_model import LassoCV, RidgeCV, ElasticNetCV, HuberRegressor\n",
    "        from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "        import lightgbm as lgb\n",
    "        import xgboost as xgb\n",
    "        \n",
    "        self.cv_folds = cv_folds\n",
    "        self.random_state = random_state\n",
    "        self.validation_size = validation_size\n",
    "        self.output_dir = output_dir\n",
    "        \n",
    "        # Initialize imputers for different strategies\n",
    "        self.imputers = {\n",
    "            'mean': SimpleImputer(strategy='mean'),\n",
    "            'median': SimpleImputer(strategy='median'),\n",
    "            'most_frequent': SimpleImputer(strategy='most_frequent')\n",
    "        }\n",
    "        \n",
    "        # Multiple scalers for different data characteristics\n",
    "        self.scalers = {\n",
    "            'standard': StandardScaler(),\n",
    "            'robust': RobustScaler(),\n",
    "            'quantile': QuantileTransformer(output_distribution='normal')\n",
    "        }\n",
    "        \n",
    "        # Base models with optimized configurations\n",
    "        self.base_models = {\n",
    "            'lgb_1': lgb.LGBMRegressor(\n",
    "                n_estimators=1000,\n",
    "                learning_rate=0.01,\n",
    "                num_leaves=31,\n",
    "                colsample_bytree=0.8,\n",
    "                subsample=0.8,\n",
    "                subsample_freq=1,\n",
    "                min_child_samples=20,\n",
    "                random_state=random_state\n",
    "            ),\n",
    "            'lgb_2': lgb.LGBMRegressor(\n",
    "                n_estimators=1000,\n",
    "                learning_rate=0.005,\n",
    "                num_leaves=63,\n",
    "                colsample_bytree=0.7,\n",
    "                subsample=0.7,\n",
    "                subsample_freq=1,\n",
    "                min_child_samples=30,\n",
    "                random_state=random_state+1\n",
    "            ),\n",
    "            'xgb_1': xgb.XGBRegressor(\n",
    "                n_estimators=1000,\n",
    "                learning_rate=0.01,\n",
    "                max_depth=6,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                min_child_weight=3,\n",
    "                random_state=random_state\n",
    "            ),\n",
    "            'xgb_2': xgb.XGBRegressor(\n",
    "                n_estimators=1000,\n",
    "                learning_rate=0.005,\n",
    "                max_depth=7,\n",
    "                subsample=0.7,\n",
    "                colsample_bytree=0.7,\n",
    "                min_child_weight=5,\n",
    "                random_state=random_state+1\n",
    "            ),\n",
    "            'rf': RandomForestRegressor(\n",
    "                n_estimators=500,\n",
    "                max_depth=12,\n",
    "                min_samples_split=5,\n",
    "                min_samples_leaf=2,\n",
    "                max_features='sqrt',\n",
    "                random_state=random_state\n",
    "            ),\n",
    "            'gbm': GradientBoostingRegressor(\n",
    "                n_estimators=500,\n",
    "                learning_rate=0.01,\n",
    "                max_depth=5,\n",
    "                subsample=0.8,\n",
    "                random_state=random_state\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # Advanced stacking configuration\n",
    "        self.meta_model = HuberRegressor(epsilon=1.75)\n",
    "        self.stacking = StackingRegressor(\n",
    "            estimators=[(name, model) for name, model in self.base_models.items()],\n",
    "            final_estimator=self.meta_model,\n",
    "            cv=5,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    \n",
    "    def handle_missing_values(self, X, strategy='advanced'):\n",
    "        \"\"\"\n",
    "        Handle missing values using various strategies\n",
    "        \n",
    "        Parameters:\n",
    "        X : DataFrame\n",
    "            Input data\n",
    "        strategy : str\n",
    "            'advanced' for column-specific imputation\n",
    "            'simple' for median imputation\n",
    "        \"\"\"\n",
    "        X_imputed = X.copy()\n",
    "        \n",
    "        if strategy == 'advanced':\n",
    "            # Handle different columns with different strategies\n",
    "            numeric_cols = X_imputed.select_dtypes(include=['float64', 'int64']).columns\n",
    "            \n",
    "            for col in numeric_cols:\n",
    "                # Choose imputation strategy based on distribution\n",
    "                if X_imputed[col].skew() > 1:\n",
    "                    imputer = self.imputers['median']\n",
    "                elif X_imputed[col].dtype in ['float64', 'float32']:\n",
    "                    imputer = self.imputers['mean']\n",
    "                else:\n",
    "                    imputer = self.imputers['most_frequent']\n",
    "                \n",
    "                X_imputed[col] = imputer.fit_transform(X_imputed[[col]])\n",
    "        else:\n",
    "            # Simple median imputation for all columns\n",
    "            X_imputed = self.imputers['median'].fit_transform(X_imputed)\n",
    "            X_imputed = pd.DataFrame(X_imputed, columns=X.columns)\n",
    "        \n",
    "        return X_imputed\n",
    "\n",
    "    def create_clusters(self, X):\n",
    "        \"\"\"\n",
    "        Create clusters for local model training with missing value handling\n",
    "        \"\"\"\n",
    "        # Handle missing values before clustering\n",
    "        X_clean = self.handle_missing_values(X)\n",
    "        \n",
    "        # Scale the data for better clustering\n",
    "        X_scaled = self.scalers['standard'].fit_transform(X_clean)\n",
    "        \n",
    "        # Perform clustering\n",
    "        kmeans = KMeans(n_clusters=5, random_state=self.random_state)\n",
    "        return kmeans.fit_predict(X_scaled)\n",
    "\n",
    "    def feature_engineering(self, X, is_training=True):\n",
    "        \"\"\"\n",
    "        Advanced feature engineering with importance-based selection and missing value handling\n",
    "        \"\"\"\n",
    "        X_processed = X.copy()\n",
    "        \n",
    "        # Handle missing values first\n",
    "        X_processed = self.handle_missing_values(X_processed)\n",
    "        \n",
    "        # Numeric columns\n",
    "        numeric_cols = X_processed.select_dtypes(include=['float64', 'int64']).columns\n",
    "        \n",
    "        # Basic transformations\n",
    "        for col in numeric_cols:\n",
    "            X_processed[f'{col}_log'] = np.log1p(np.abs(X_processed[col]))\n",
    "            X_processed[f'{col}_sqrt'] = np.sqrt(np.abs(X_processed[col]))\n",
    "            X_processed[f'{col}_squared'] = X_processed[col] ** 2\n",
    "        \n",
    "        # Statistical features\n",
    "        for col in numeric_cols:\n",
    "            X_processed[f'{col}_rolling_mean'] = X_processed[col].rolling(window=3, min_periods=1).mean()\n",
    "            X_processed[f'{col}_rolling_std'] = X_processed[col].rolling(window=3, min_periods=1).std()\n",
    "        \n",
    "        # Interaction features for top features only\n",
    "        if is_training:\n",
    "            self.top_features = list(numeric_cols[:5])\n",
    "        \n",
    "        for i in range(len(self.top_features)):\n",
    "            for j in range(i + 1, len(self.top_features)):\n",
    "                col1, col2 = self.top_features[i], self.top_features[j]\n",
    "                X_processed[f'{col1}_{col2}_interaction'] = X_processed[col1] * X_processed[col2]\n",
    "                X_processed[f'{col1}_{col2}_ratio'] = X_processed[col1] / (X_processed[col2] + 1e-6)\n",
    "        \n",
    "        return X_processed\n",
    "\n",
    "    # [Rest of the class methods remain the same...]\n",
    "\n",
    "    def select_features(self, X, y=None, threshold='median'):\n",
    "        \"\"\"\n",
    "        Advanced feature selection using multiple methods\n",
    "        \"\"\"\n",
    "        if y is not None:\n",
    "            # Mutual information based selection\n",
    "            mi_scores = mutual_info_regression(X, y)\n",
    "            mi_selector = SelectFromModel(estimator='precomputed',\n",
    "                                        threshold=threshold,\n",
    "                                        prefit=True)\n",
    "            mi_selector.estimator_ = mi_scores\n",
    "            mi_selector.feature_importances_ = mi_scores\n",
    "            \n",
    "            # Combine with model-based selection\n",
    "            model_selector = SelectFromModel(\n",
    "                estimator=lgb.LGBMRegressor(random_state=self.random_state),\n",
    "                threshold=threshold\n",
    "            )\n",
    "            model_selector.fit(X, y)\n",
    "            \n",
    "            # Combine both selections\n",
    "            selected_features_mi = mi_selector.get_support()\n",
    "            selected_features_model = model_selector.get_support()\n",
    "            final_selection = selected_features_mi | selected_features_model\n",
    "            \n",
    "            return X[:, final_selection]\n",
    "        return X\n",
    "\n",
    "    def prepare_data(self, train_file, test_file):\n",
    "        \"\"\"\n",
    "        Enhanced data preparation with advanced preprocessing\n",
    "        \"\"\"\n",
    "        # Load data\n",
    "        train_data = pd.read_csv(train_file)\n",
    "        test_data = pd.read_csv(test_file)\n",
    "        \n",
    "        # Initial feature engineering\n",
    "        X = self.feature_engineering(train_data.drop(['yield', 'id'], axis=1), is_training=True)\n",
    "        y = train_data['yield']\n",
    "        test_ids = test_data['id']\n",
    "        X_test = self.feature_engineering(test_data.drop(['id'], axis=1), is_training=False)\n",
    "        \n",
    "        # Create clusters for local modeling\n",
    "        clusters = self.create_clusters(X)\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y,\n",
    "            test_size=self.validation_size,\n",
    "            random_state=self.random_state,\n",
    "            stratify=clusters\n",
    "        )\n",
    "        \n",
    "        # Advanced scaling\n",
    "        X_train_scaled = self.advanced_scaling(X_train)\n",
    "        X_val_scaled = self.advanced_scaling(X_val)\n",
    "        X_test_scaled = self.advanced_scaling(X_test)\n",
    "        \n",
    "        # Feature selection\n",
    "        X_train_selected = self.select_features(X_train_scaled, y_train)\n",
    "        X_val_selected = self.select_features(X_val_scaled)\n",
    "        X_test_selected = self.select_features(X_test_scaled)\n",
    "        \n",
    "        return (X_train_selected, X_val_selected, X_test_selected,\n",
    "                y_train, y_val, test_ids)\n",
    "\n",
    "    def advanced_scaling(self, X):\n",
    "        \"\"\"\n",
    "        Apply multiple scaling techniques based on feature distributions\n",
    "        \"\"\"\n",
    "        X_scaled = X.copy()\n",
    "        \n",
    "        # Apply different scalers based on feature characteristics\n",
    "        for col in X_scaled.columns:\n",
    "            skewness = X_scaled[col].skew()\n",
    "            if abs(skewness) > 1:\n",
    "                X_scaled[col] = self.scalers['quantile'].fit_transform(X_scaled[[col]])\n",
    "            elif X_scaled[col].std() > 10:\n",
    "                X_scaled[col] = self.scalers['robust'].fit_transform(X_scaled[[col]])\n",
    "            else:\n",
    "                X_scaled[col] = self.scalers['standard'].fit_transform(X_scaled[[col]])\n",
    "        \n",
    "        return X_scaled\n",
    "\n",
    "    def train_and_predict(self, X_train, X_val, X_test, y_train, y_val, test_ids):\n",
    "        \"\"\"\n",
    "        Train models and make predictions with advanced techniques\n",
    "        \"\"\"\n",
    "        # Train base models with early stopping\n",
    "        base_predictions = {}\n",
    "        val_predictions = {}\n",
    "        \n",
    "        for name, model in self.base_models.items():\n",
    "            if isinstance(model, (lgb.LGBMRegressor, xgb.XGBRegressor)):\n",
    "                model.fit(\n",
    "                    X_train, y_train,\n",
    "                    eval_set=[(X_val, y_val)],\n",
    "                    early_stopping_rounds=50,\n",
    "                    verbose=False\n",
    "                )\n",
    "            else:\n",
    "                model.fit(X_train, y_train)\n",
    "            \n",
    "            base_predictions[name] = model.predict(X_test)\n",
    "            val_predictions[name] = model.predict(X_val)\n",
    "        \n",
    "        # Train stacking model\n",
    "        self.stacking.fit(X_train, y_train)\n",
    "        stacking_pred = self.stacking.predict(X_test)\n",
    "        \n",
    "        # Optimize ensemble weights using validation performance\n",
    "        weights = self._optimize_weights(val_predictions, y_val)\n",
    "        \n",
    "        # Create weighted ensemble prediction\n",
    "        ensemble_pred = np.zeros_like(stacking_pred)\n",
    "        for name, pred in base_predictions.items():\n",
    "            ensemble_pred += weights[name] * pred\n",
    "        \n",
    "        # Blend stacking and weighted ensemble\n",
    "        final_pred = 0.6 * stacking_pred + 0.4 * ensemble_pred\n",
    "        \n",
    "        # Save predictions\n",
    "        pd.DataFrame({\n",
    "            'id': test_ids,\n",
    "            'yield': final_pred\n",
    "        }).to_csv(os.path.join(self.output_dir, 'final_predictions.csv'), index=False)\n",
    "        \n",
    "        return final_pred\n",
    "\n",
    "    def _optimize_weights(self, predictions, y_true):\n",
    "        \"\"\"\n",
    "        Optimize ensemble weights using validation performance\n",
    "        \"\"\"\n",
    "        maes = {}\n",
    "        for name, pred in predictions.items():\n",
    "            mae = mean_absolute_error(y_true, pred)\n",
    "            maes[name] = 1 / (mae + 1e-6)\n",
    "        \n",
    "        total = sum(maes.values())\n",
    "        return {name: score/total for name, score in maes.items()}\n",
    "\n",
    "def main():\n",
    "    predictor = AdvancedYieldPredictor(output_dir='yield_predictions')\n",
    "    \n",
    "    # Prepare data\n",
    "    X_train, X_val, X_test, y_train, y_val, test_ids = predictor.prepare_data(\n",
    "        'train.csv', 'test.csv'\n",
    "    )\n",
    "    \n",
    "    # Train and predict\n",
    "    final_predictions = predictor.train_and_predict(\n",
    "        X_train, X_val, X_test, y_train, y_val, test_ids\n",
    "    )\n",
    "    \n",
    "    # Evaluate validation performance\n",
    "    val_pred = predictor.stacking.predict(X_val)\n",
    "    val_mae = mean_absolute_error(y_val, val_pred)\n",
    "    print(f\"Validation MAE: {val_mae:.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b4482d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
