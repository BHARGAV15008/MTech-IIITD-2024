{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "trainData = pd.read_csv('train.csv')\n",
    "testData = pd.read_csv('test.csv')\n",
    "\n",
    "features_to_use = trainData.columns[2:18]\n",
    "features_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import RobustScaler\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "# import xgboost as xgb\n",
    "# from lightgbm import LGBMRegressor\n",
    "# from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# def load_and_preprocess():\n",
    "#     train = pd.read_csv('train.csv')\n",
    "#     test = pd.read_csv('test.csv')\n",
    "#     test_ids = test['id'].copy()\n",
    "#     all_data = pd.concat([train.drop('yield', axis=1), test], axis=0, ignore_index=True)\n",
    "#     return train, test, all_data, test_ids\n",
    "\n",
    "# def create_features(df):\n",
    "#     df = df.copy()\n",
    "    \n",
    "#     # Core bee features - from best performing version\n",
    "#     df['total_bees'] = df['honeybee'] + df['bumbles'] + df['andrena'] + df['osmia']\n",
    "#     df['wild_bees'] = df['bumbles'] + df['andrena'] + df['osmia']\n",
    "#     df['bee_ratio'] = df['honeybee'] / (df['total_bees'] + 1)\n",
    "    \n",
    "#     # Temperature features\n",
    "#     df['upper_temp_range'] = df['MaxOfUpperTRange'] - df['MinOfUpperTRange']\n",
    "#     df['lower_temp_range'] = df['MaxOfLowerTRange'] - df['MinOfLowerTRange']\n",
    "#     df['avg_temp'] = (df['AverageOfUpperTRange'] + df['AverageOfLowerTRange']) / 2\n",
    "    \n",
    "#     # Fruit and pollination features\n",
    "#     df['fruit_efficiency'] = df['fruitset'] * df['fruitmass']\n",
    "#     df['seed_density'] = df['seeds'] / (df['fruitmass'] + 1)\n",
    "#     df['clone_fruit'] = df['clonesize'] * df['fruitset']\n",
    "    \n",
    "#     # Weather interactions\n",
    "#     df['rain_temp_effect'] = df['RainingDays'] * df['avg_temp']\n",
    "#     df['bee_weather'] = df['total_bees'] * df['avg_temp'] * (1 / (df['RainingDays'] + 1))\n",
    "    \n",
    "#     # Log transforms for skewed features\n",
    "#     for col in ['clonesize', 'fruitmass', 'seeds']:\n",
    "#         df[f'log_{col}'] = np.log1p(df[col])\n",
    "    \n",
    "#     # Squared terms for key variables\n",
    "#     for col in ['fruitset', 'seeds', 'fruit_efficiency']:\n",
    "#         df[f'{col}_squared'] = df[col] ** 2\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# def train_models(X_train, y_train, X_val, y_val):\n",
    "#     models = {\n",
    "#         'xgb': xgb.XGBRegressor(\n",
    "#             n_estimators=3000,\n",
    "#             learning_rate=0.003,\n",
    "#             max_depth=6,\n",
    "#             subsample=0.85,\n",
    "#             colsample_bytree=0.85,\n",
    "#             min_child_weight=5,\n",
    "#             gamma=0.1,\n",
    "#             reg_alpha=0.1,\n",
    "#             reg_lambda=1,\n",
    "#             objective='reg:absoluteerror',\n",
    "#             tree_method='hist',\n",
    "#             random_state=42\n",
    "#         ),\n",
    "#         'lgbm': LGBMRegressor(\n",
    "#             n_estimators=3000,\n",
    "#             learning_rate=0.003,\n",
    "#             num_leaves=64,\n",
    "#             subsample=0.85,\n",
    "#             colsample_bytree=0.85,\n",
    "#             min_child_samples=20,\n",
    "#             objective='mae',\n",
    "#             random_state=42,\n",
    "#             force_col_wise=True\n",
    "#         ),\n",
    "#         'rf': RandomForestRegressor(\n",
    "#             n_estimators=500,\n",
    "#             max_depth=15,\n",
    "#             min_samples_split=5,\n",
    "#             min_samples_leaf=4,\n",
    "#             max_features='sqrt',\n",
    "#             criterion='absolute_error',\n",
    "#             random_state=42,\n",
    "#             n_jobs=-1\n",
    "#         ),\n",
    "#         'gb': GradientBoostingRegressor(\n",
    "#             n_estimators=500,\n",
    "#             learning_rate=0.03,\n",
    "#             max_depth=6,\n",
    "#             min_samples_split=5,\n",
    "#             min_samples_leaf=4,\n",
    "#             subsample=0.8,\n",
    "#             loss='absolute_error',\n",
    "#             random_state=42\n",
    "#         )\n",
    "#     }\n",
    "    \n",
    "#     val_predictions = {}\n",
    "#     trained_models = {}\n",
    "    \n",
    "#     for name, model in models.items():\n",
    "#         model.fit(X_train, y_train)\n",
    "#         val_pred = model.predict(X_val)\n",
    "#         val_predictions[name] = val_pred\n",
    "#         trained_models[name] = model\n",
    "        \n",
    "#         val_score = mean_absolute_error(y_val, val_pred)\n",
    "#         print(f\"{name} Validation MAE: {val_score:.4f}\")\n",
    "    \n",
    "#     return trained_models, val_predictions\n",
    "\n",
    "# def blend_predictions(val_predictions, y_val):\n",
    "#     base_weights = {\n",
    "#         'xgb': 0.35,\n",
    "#         'lgbm': 0.30,\n",
    "#         'rf': 0.15,\n",
    "#         'gb': 0.20\n",
    "#     }\n",
    "    \n",
    "#     best_mae = float('inf')\n",
    "#     best_weights = base_weights.copy()\n",
    "    \n",
    "#     # Fine-tune around the base weights that worked well before\n",
    "#     for xgb_w in np.arange(0.33, 0.37, 0.002):  # Centered around 0.35\n",
    "#         for lgbm_w in np.arange(0.28, 0.32, 0.002):  # Centered around 0.30\n",
    "#             for rf_w in np.arange(0.13, 0.17, 0.002):  # Centered around 0.15\n",
    "#                 gb_w = 1 - xgb_w - lgbm_w - rf_w\n",
    "#                 if 0.18 <= gb_w <= 0.22:  # Centered around 0.20\n",
    "#                     weights = {\n",
    "#                         'xgb': xgb_w,\n",
    "#                         'lgbm': lgbm_w,\n",
    "#                         'rf': rf_w,\n",
    "#                         'gb': gb_w\n",
    "#                     }\n",
    "                    \n",
    "#                     pred = np.zeros(len(y_val))\n",
    "#                     for model, weight in weights.items():\n",
    "#                         pred += val_predictions[model] * weight\n",
    "                    \n",
    "#                     mae = mean_absolute_error(y_val, pred)\n",
    "#                     if mae < best_mae:\n",
    "#                         best_mae = mae\n",
    "#                         best_weights = weights.copy()\n",
    "    \n",
    "#     print(f\"\\nBest weights found: {best_weights}\")\n",
    "#     print(f\"Best Validation MAE: {best_mae:.4f}\")\n",
    "#     return best_weights\n",
    "\n",
    "# def main():\n",
    "#     # Load data\n",
    "#     train, test, all_data, test_ids = load_and_preprocess()\n",
    "    \n",
    "#     # Create features\n",
    "#     all_data_featured = create_features(all_data)\n",
    "    \n",
    "#     # Drop unnecessary columns\n",
    "#     drop_cols = ['id', 'Row#']\n",
    "#     all_data_featured = all_data_featured.drop(drop_cols, axis=1)\n",
    "    \n",
    "#     # Split data\n",
    "#     X = all_data_featured.iloc[:len(train)]\n",
    "#     X_test = all_data_featured.iloc[len(train):]\n",
    "#     y = train['yield']\n",
    "    \n",
    "#     X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "#     # Scale features\n",
    "#     scaler = RobustScaler()\n",
    "#     X_train_scaled = scaler.fit_transform(X_train)\n",
    "#     X_val_scaled = scaler.transform(X_val)\n",
    "#     X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "#     X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "#     X_val_scaled = pd.DataFrame(X_val_scaled, columns=X_val.columns)\n",
    "#     X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "    \n",
    "#     # Train models\n",
    "#     trained_models, val_predictions = train_models(X_train_scaled, y_train, X_val_scaled, y_val)\n",
    "    \n",
    "#     # Find optimal weights\n",
    "#     best_weights = blend_predictions(val_predictions, y_val)\n",
    "    \n",
    "#     # Generate predictions\n",
    "#     test_predictions = {}\n",
    "#     for name, model in trained_models.items():\n",
    "#         test_predictions[name] = model.predict(X_test_scaled)\n",
    "    \n",
    "#     # Create final predictions\n",
    "#     final_predictions = np.zeros(len(X_test))\n",
    "#     for model, weight in best_weights.items():\n",
    "#         final_predictions += test_predictions[model] * weight\n",
    "    \n",
    "#     # Create submission file\n",
    "#     submission = pd.DataFrame({\n",
    "#         'id': test_ids,\n",
    "#         'yield': final_predictions\n",
    "#     })\n",
    "#     submission['id'] = submission['id'].astype(int)\n",
    "#     submission = submission.sort_values('id').reset_index(drop=True)\n",
    "    \n",
    "#     submission.to_csv('submission.csv', index=False)\n",
    "#     print(\"\\nFirst few rows of submission file:\")\n",
    "#     print(submission.head())\n",
    "#     print(\"\\nSubmission file shape:\", submission.shape)\n",
    "#     print(\"Submission file created successfully!\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Creating features...\n",
      "\n",
      "Data shape after feature engineering: (25000, 33)\n",
      "\n",
      "Performing Random Forest feature selection...\n",
      "\n",
      "Feature Importances:\n",
      "fruitset_squared: 0.4505\n",
      "fruitset: 0.4020\n",
      "seeds: 0.0400\n",
      "log_seeds: 0.0352\n",
      "seeds_squared: 0.0311\n",
      "fruit_efficiency: 0.0058\n",
      "seed_density: 0.0055\n",
      "fruit_efficiency_squared: 0.0052\n",
      "bee_weather: 0.0040\n",
      "clone_fruit: 0.0035\n",
      "fruitmass: 0.0027\n",
      "log_fruitmass: 0.0027\n",
      "bee_ratio: 0.0018\n",
      "rain_temp_effect: 0.0014\n",
      "total_bees: 0.0011\n",
      "wild_bees: 0.0009\n",
      "andrena: 0.0008\n",
      "osmia: 0.0007\n",
      "AverageOfLowerTRange: 0.0005\n",
      "lower_temp_range: 0.0005\n",
      "avg_temp: 0.0005\n",
      "MinOfLowerTRange: 0.0005\n",
      "MaxOfUpperTRange: 0.0004\n",
      "bumbles: 0.0004\n",
      "AverageOfUpperTRange: 0.0004\n",
      "upper_temp_range: 0.0004\n",
      "MaxOfLowerTRange: 0.0004\n",
      "MinOfUpperTRange: 0.0003\n",
      "AverageRainingDays: 0.0003\n",
      "RainingDays: 0.0002\n",
      "honeybee: 0.0001\n",
      "log_clonesize: 0.0001\n",
      "clonesize: 0.0001\n",
      "\n",
      "Selected 27 features out of 33\n",
      "Selected features: ['fruitset_squared', 'fruitset', 'seeds', 'log_seeds', 'seeds_squared', 'fruit_efficiency', 'seed_density', 'fruit_efficiency_squared', 'bee_weather', 'clone_fruit', 'fruitmass', 'log_fruitmass', 'bee_ratio', 'rain_temp_effect', 'total_bees', 'wild_bees', 'andrena', 'osmia', 'AverageOfLowerTRange', 'lower_temp_range', 'avg_temp', 'MinOfLowerTRange', 'MaxOfUpperTRange', 'bumbles', 'AverageOfUpperTRange', 'upper_temp_range', 'MaxOfLowerTRange']\n",
      "\n",
      "Feature importances saved to 'feature_importances.csv'\n",
      "\n",
      "Training with 27 selected features\n",
      "Scaling features...\n",
      "\n",
      "Training models...\n",
      "\n",
      "Training XGB model...\n",
      "XGB Validation MAE: 246.3322\n",
      "\n",
      "Training LGBM model...\n",
      "[LightGBM] [Info] Total Bins 3235\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score 6079.085449\n",
      "LGBM Validation MAE: 248.0108\n",
      "\n",
      "Training RF model...\n",
      "RF Validation MAE: 247.4577\n",
      "\n",
      "Training GB model...\n",
      "GB Validation MAE: 246.0335\n",
      "\n",
      "Finding optimal blend weights...\n",
      "\n",
      "Best weights found: {'xgb': 0.33, 'lgbm': 0.28, 'rf': 0.17000000000000004, 'gb': 0.21999999999999986}\n",
      "Best Validation MAE: 245.4479\n",
      "\n",
      "Generating final predictions...\n",
      "\n",
      "First few rows of submission file:\n",
      "      id        yield\n",
      "0  15000  6082.030909\n",
      "1  15001  6848.236217\n",
      "2  15002  6836.328748\n",
      "3  15003  7313.513168\n",
      "4  15004  6680.006197\n",
      "\n",
      "Submission file shape: (10000, 2)\n",
      "Submission files created successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def select_features(X, y, importance_threshold=0.001):\n",
    "    \"\"\"\n",
    "    Select features using Random Forest importance scores\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : DataFrame\n",
    "        Features DataFrame\n",
    "    y : Series\n",
    "        Target variable\n",
    "    importance_threshold : float\n",
    "        Minimum importance score for feature selection\n",
    "    \"\"\"\n",
    "    print(\"\\nPerforming Random Forest feature selection...\")\n",
    "    \n",
    "    # Initialize Random Forest for feature selection\n",
    "    rf_selector = RandomForestRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf_selector.fit(X, y)\n",
    "    \n",
    "    # Get feature importances\n",
    "    importances = pd.Series(\n",
    "        rf_selector.feature_importances_,\n",
    "        index=X.columns\n",
    "    ).sort_values(ascending=False)\n",
    "    \n",
    "    # Select features based on importance threshold\n",
    "    selected_features = importances[importances >= importance_threshold].index.tolist()\n",
    "    \n",
    "    # Print feature importance summary\n",
    "    print(\"\\nFeature Importances:\")\n",
    "    for feat, imp in importances.items():\n",
    "        print(f\"{feat}: {imp:.4f}\")\n",
    "    \n",
    "    print(f\"\\nSelected {len(selected_features)} features out of {len(X.columns)}\")\n",
    "    print(\"Selected features:\", selected_features)\n",
    "    \n",
    "    # Save feature importances to CSV\n",
    "    importances.to_frame('importance').to_csv('feature_importances.csv')\n",
    "    print(\"\\nFeature importances saved to 'feature_importances.csv'\")\n",
    "    \n",
    "    return selected_features\n",
    "\n",
    "def load_and_preprocess():\n",
    "    train = pd.read_csv('train.csv')\n",
    "    test = pd.read_csv('test.csv')\n",
    "    test_ids = test['id'].copy()\n",
    "    all_data = pd.concat([train.drop('yield', axis=1), test], axis=0, ignore_index=True)\n",
    "    return train, test, all_data, test_ids\n",
    "\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Core bee features\n",
    "    df['total_bees'] = df['honeybee'] + df['bumbles'] + df['andrena'] + df['osmia']\n",
    "    df['wild_bees'] = df['bumbles'] + df['andrena'] + df['osmia']\n",
    "    df['bee_ratio'] = df['honeybee'] / (df['total_bees'] + 1)\n",
    "    \n",
    "    # Temperature features\n",
    "    df['upper_temp_range'] = df['MaxOfUpperTRange'] - df['MinOfUpperTRange']\n",
    "    df['lower_temp_range'] = df['MaxOfLowerTRange'] - df['MinOfLowerTRange']\n",
    "    df['avg_temp'] = (df['AverageOfUpperTRange'] + df['AverageOfLowerTRange']) / 2\n",
    "    \n",
    "    # Fruit and pollination features\n",
    "    df['fruit_efficiency'] = df['fruitset'] * df['fruitmass']\n",
    "    df['seed_density'] = df['seeds'] / (df['fruitmass'] + 1)\n",
    "    df['clone_fruit'] = df['clonesize'] * df['fruitset']\n",
    "    \n",
    "    # Weather interactions\n",
    "    df['rain_temp_effect'] = df['RainingDays'] * df['avg_temp']\n",
    "    df['bee_weather'] = df['total_bees'] * df['avg_temp'] * (1 / (df['RainingDays'] + 1))\n",
    "    \n",
    "    # Log transforms for skewed features\n",
    "    for col in ['clonesize', 'fruitmass', 'seeds']:\n",
    "        df[f'log_{col}'] = np.log1p(df[col])\n",
    "    \n",
    "    # Squared terms for key variables\n",
    "    for col in ['fruitset', 'seeds', 'fruit_efficiency']:\n",
    "        df[f'{col}_squared'] = df[col] ** 2\n",
    "    \n",
    "    return df\n",
    "\n",
    "def train_models(X_train, y_train, X_val, y_val):\n",
    "    models = {\n",
    "        'xgb': xgb.XGBRegressor(\n",
    "            n_estimators=3000,\n",
    "            learning_rate=0.003,\n",
    "            max_depth=6,\n",
    "            subsample=0.85,\n",
    "            colsample_bytree=0.85,\n",
    "            min_child_weight=5,\n",
    "            gamma=0.1,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=1,\n",
    "            objective='reg:absoluteerror',\n",
    "            tree_method='hist',\n",
    "            random_state=42\n",
    "        ),\n",
    "        'lgbm': LGBMRegressor(\n",
    "            n_estimators=3000,\n",
    "            learning_rate=0.003,\n",
    "            num_leaves=64,\n",
    "            subsample=0.85,\n",
    "            colsample_bytree=0.85,\n",
    "            min_child_samples=20,\n",
    "            objective='mae',\n",
    "            random_state=42,\n",
    "            force_col_wise=True\n",
    "        ),\n",
    "        'rf': RandomForestRegressor(\n",
    "            n_estimators=500,\n",
    "            max_depth=15,\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=4,\n",
    "            max_features='sqrt',\n",
    "            criterion='absolute_error',\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        'gb': GradientBoostingRegressor(\n",
    "            n_estimators=500,\n",
    "            learning_rate=0.03,\n",
    "            max_depth=6,\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=4,\n",
    "            subsample=0.8,\n",
    "            loss='absolute_error',\n",
    "            random_state=42\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    val_predictions = {}\n",
    "    trained_models = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining {name.upper()} model...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        val_pred = model.predict(X_val)\n",
    "        val_predictions[name] = val_pred\n",
    "        trained_models[name] = model\n",
    "        \n",
    "        val_score = mean_absolute_error(y_val, val_pred)\n",
    "        print(f\"{name.upper()} Validation MAE: {val_score:.4f}\")\n",
    "    \n",
    "    return trained_models, val_predictions\n",
    "\n",
    "def blend_predictions(val_predictions, y_val):\n",
    "    base_weights = {\n",
    "        'xgb': 0.35,\n",
    "        'lgbm': 0.30,\n",
    "        'rf': 0.15,\n",
    "        'gb': 0.20\n",
    "    }\n",
    "    \n",
    "    best_mae = float('inf')\n",
    "    best_weights = base_weights.copy()\n",
    "    \n",
    "    for xgb_w in np.arange(0.33, 0.37, 0.002):\n",
    "        for lgbm_w in np.arange(0.28, 0.32, 0.002):\n",
    "            for rf_w in np.arange(0.13, 0.17, 0.002):\n",
    "                gb_w = 1 - xgb_w - lgbm_w - rf_w\n",
    "                if 0.18 <= gb_w <= 0.22:\n",
    "                    weights = {\n",
    "                        'xgb': xgb_w,\n",
    "                        'lgbm': lgbm_w,\n",
    "                        'rf': rf_w,\n",
    "                        'gb': gb_w\n",
    "                    }\n",
    "                    \n",
    "                    pred = np.zeros(len(y_val))\n",
    "                    for model, weight in weights.items():\n",
    "                        pred += val_predictions[model] * weight\n",
    "                    \n",
    "                    mae = mean_absolute_error(y_val, pred)\n",
    "                    if mae < best_mae:\n",
    "                        best_mae = mae\n",
    "                        best_weights = weights.copy()\n",
    "    \n",
    "    print(f\"\\nBest weights found: {best_weights}\")\n",
    "    print(f\"Best Validation MAE: {best_mae:.4f}\")\n",
    "    return best_weights\n",
    "\n",
    "def main():\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    train, test, all_data, test_ids = load_and_preprocess()\n",
    "    \n",
    "    print(\"Creating features...\")\n",
    "    all_data_featured = create_features(all_data)\n",
    "    \n",
    "    drop_cols = ['id', 'Row#']\n",
    "    all_data_featured = all_data_featured.drop(drop_cols, axis=1)\n",
    "    \n",
    "    print(\"\\nData shape after feature engineering:\", all_data_featured.shape)\n",
    "    \n",
    "    # Split data for feature selection\n",
    "    X = all_data_featured.iloc[:len(train)]\n",
    "    X_test = all_data_featured.iloc[len(train):]\n",
    "    y = train['yield']\n",
    "    \n",
    "    # Perform feature selection\n",
    "    selected_features = select_features(X, y, importance_threshold=0.0004)\n",
    "    \n",
    "    # Filter features based on selection\n",
    "    X = X[selected_features]\n",
    "    X_test = X_test[selected_features]\n",
    "    \n",
    "    print(f\"\\nTraining with {len(selected_features)} selected features\")\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.2, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(\"Scaling features...\")\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "    X_val_scaled = pd.DataFrame(X_val_scaled, columns=X_val.columns)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "    \n",
    "    print(\"\\nTraining models...\")\n",
    "    trained_models, val_predictions = train_models(X_train_scaled, y_train, X_val_scaled, y_val)\n",
    "    \n",
    "    print(\"\\nFinding optimal blend weights...\")\n",
    "    best_weights = blend_predictions(val_predictions, y_val)\n",
    "    \n",
    "    print(\"\\nGenerating final predictions...\")\n",
    "    test_predictions = {}\n",
    "    for name, model in trained_models.items():\n",
    "        test_predictions[name] = model.predict(X_test_scaled)\n",
    "    \n",
    "    final_predictions = np.zeros(len(X_test))\n",
    "    for model, weight in best_weights.items():\n",
    "        final_predictions += test_predictions[model] * weight\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_ids,\n",
    "        'yield': final_predictions\n",
    "    })\n",
    "    submission['id'] = submission['id'].astype(int)\n",
    "    submission = submission.sort_values('id').reset_index(drop=True)\n",
    "    \n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    print(\"\\nFirst few rows of submission file:\")\n",
    "    print(submission.head())\n",
    "    print(\"\\nSubmission file shape:\", submission.shape)\n",
    "    print(\"Submission files created successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weste Paper but usefull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.model_selection import train_test_split, cross_val_score\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "# import lightgbm as lgb\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# def select_best_features_using_importance(X, y, n_estimators=200, max_depth=5, random_state=42):\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "    \n",
    "#     scaler = StandardScaler()\n",
    "#     X_train_scaled = scaler.fit_transform(X_train)\n",
    "#     X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "#     # Fit RandomForest to find feature importance\n",
    "#     rf_model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, random_state=random_state)\n",
    "#     rf_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "#     # Get feature importance\n",
    "#     feature_importances = rf_model.feature_importances_\n",
    "    \n",
    "#     # Sort features by importance\n",
    "#     feature_importance_df = pd.DataFrame({\n",
    "#         'Feature': X.columns,\n",
    "#         'Importance': feature_importances\n",
    "#     }).sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "#     print(\"Features sorted by importance:\")\n",
    "#     print(feature_importance_df)\n",
    "    \n",
    "#     # Select top features\n",
    "#     selected_features = feature_importance_df['Feature'].head(5)  # Selecting top 5 features\n",
    "#     return selected_features\n",
    "\n",
    "# def train_model_with_selected_features_cv(X, y, selected_features, testData=None, idTest=None):\n",
    "#     X_selected = X[selected_features]\n",
    "    \n",
    "#     # Split the data for early stopping\n",
    "#     X_train, X_val, y_train, y_val = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "#     scaler = StandardScaler()\n",
    "#     X_train_scaled = scaler.fit_transform(X_train)\n",
    "#     X_val_scaled = scaler.transform(X_val)\n",
    "    \n",
    "#     # Create LightGBM datasets\n",
    "#     train_data = lgb.Dataset(X_train_scaled, label=y_train)\n",
    "#     val_data = lgb.Dataset(X_val_scaled, label=y_val, reference=train_data)\n",
    "    \n",
    "#     # LightGBM parameters\n",
    "#     params = {\n",
    "#         'objective': 'regression',\n",
    "#         'metric': 'mae',\n",
    "#         'boosting_type': 'gbdt',\n",
    "#         'num_leaves': 31,\n",
    "#         'learning_rate': 0.05,\n",
    "#         'feature_fraction': 0.9,\n",
    "#         'bagging_fraction': 0.8,\n",
    "#         'bagging_freq': 5,\n",
    "#         'verbose': -1\n",
    "#     }\n",
    "    \n",
    "#     # Training callbacks for early stopping\n",
    "#     callbacks = [\n",
    "#         lgb.early_stopping(stopping_rounds=50),\n",
    "#         lgb.log_evaluation(period=100)\n",
    "#     ]\n",
    "    \n",
    "#     # Train with early stopping\n",
    "#     model = lgb.train(\n",
    "#         params,\n",
    "#         train_data,\n",
    "#         valid_sets=[train_data, val_data],\n",
    "#         num_boost_round=1000,\n",
    "#         callbacks=callbacks\n",
    "#     )\n",
    "    \n",
    "#     # Cross-validation for performance estimation\n",
    "#     cv_results = lgb.cv(\n",
    "#         params,\n",
    "#         lgb.Dataset(X_selected, label=y),\n",
    "#         num_boost_round=1000,\n",
    "#         nfold=5,\n",
    "#         stratified=False,\n",
    "#         metrics=['mae'],\n",
    "#         callbacks=[\n",
    "#             lgb.early_stopping(stopping_rounds=50),\n",
    "#             lgb.log_evaluation(period=100)\n",
    "#         ],\n",
    "#         seed=42,\n",
    "#         return_cvbooster=True\n",
    "#     )\n",
    "    \n",
    "#     # Debug: print available keys in cv_results\n",
    "#     print(\"\\nAvailable keys in cv_results:\", cv_results.keys())\n",
    "    \n",
    "#     try:\n",
    "#         # Try different possible key formats\n",
    "#         if 'valid mae-mean' in cv_results:\n",
    "#             scores_key = 'valid mae-mean'\n",
    "#             std_key = 'valid mae-stdv'\n",
    "#         elif 'valid_0 mae-mean' in cv_results:\n",
    "#             scores_key = 'valid_0 mae-mean'\n",
    "#             std_key = 'valid_0 mae-stdv'\n",
    "#         elif 'l1-mean' in cv_results:\n",
    "#             scores_key = 'l1-mean'\n",
    "#             std_key = 'l1-stdv'\n",
    "#         else:\n",
    "#             # If none of the expected keys are found, use the first available metric\n",
    "#             metric_keys = [k for k in cv_results.keys() if k.endswith('-mean')]\n",
    "#             scores_key = metric_keys[0] if metric_keys else None\n",
    "#             std_key = scores_key.replace('-mean', '-stdv') if scores_key else None\n",
    "        \n",
    "#         if scores_key:\n",
    "#             valid_scores = cv_results[scores_key]\n",
    "#             best_iteration = np.argmin(valid_scores)\n",
    "#             best_mae = valid_scores[best_iteration]\n",
    "#             mae_stddev = cv_results[std_key][best_iteration] if std_key else 0\n",
    "            \n",
    "#             print(f\"\\nBest CV MAE: {best_mae:.4f} ± {mae_stddev:.4f}\")\n",
    "#             print(f\"Best iteration: {best_iteration + 1}\")\n",
    "#         else:\n",
    "#             print(\"\\nWarning: Could not find valid metric keys in CV results\")\n",
    "            \n",
    "#     except Exception as e:\n",
    "#         print(f\"\\nError processing CV results: {str(e)}\")\n",
    "#         print(\"Continuing with trained model...\")\n",
    "    \n",
    "#     # Predict on test data if provided\n",
    "#     if testData is not None:\n",
    "#         X_test_scaled = scaler.transform(testData[selected_features])\n",
    "#         test_predictions = model.predict(X_test_scaled)\n",
    "        \n",
    "#         output = pd.DataFrame({\n",
    "#             'id': idTest,\n",
    "#             'yield': test_predictions\n",
    "#         })\n",
    "        \n",
    "#         output_file = 'submission.csv'\n",
    "#         output.to_csv(output_file, index=False)\n",
    "#         print(f\"\\nPredictions saved to {output_file}\")\n",
    "        \n",
    "#         return model, output\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# # Example usage\n",
    "# X = trainData[features_to_use]  # All features initially\n",
    "# y = trainData['yield']\n",
    "\n",
    "# # Step 1: Select best features using feature importance\n",
    "# selected_features = select_best_features_using_importance(X, y)\n",
    "\n",
    "# # Step 2: Train LightGBM model with early stopping\n",
    "# model, predictions = train_model_with_selected_features_cv(\n",
    "#     X, y, selected_features, \n",
    "#     testData=testData, \n",
    "#     idTest=testData['id']\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.model_selection import train_test_split, KFold\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "# import lightgbm as lgb\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# from typing import List, Tuple, Dict, Optional\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# def plot_feature_importance(importance_df: pd.DataFrame, title: str = \"Feature Importance\") -> None:\n",
    "#     \"\"\"Plot feature importance using a horizontal bar chart.\"\"\"\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.barh(importance_df['Feature'][:10], importance_df['Importance'][:10])\n",
    "#     plt.title(title)\n",
    "#     plt.xlabel('Importance')\n",
    "#     plt.ylabel('Features')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# def select_best_features_using_importance(\n",
    "#     X: pd.DataFrame, \n",
    "#     y: pd.Series, \n",
    "#     n_features: int = 5,\n",
    "#     n_estimators: int = 200, \n",
    "#     max_depth: int = 5, \n",
    "#     random_state: int = 42\n",
    "# ) -> List[str]:\n",
    "#     \"\"\"\n",
    "#     Select the best features using Random Forest importance.\n",
    "    \n",
    "#     Args:\n",
    "#         X: Feature DataFrame\n",
    "#         y: Target Series\n",
    "#         n_features: Number of top features to select\n",
    "#         n_estimators: Number of trees in Random Forest\n",
    "#         max_depth: Maximum depth of trees\n",
    "#         random_state: Random seed\n",
    "        \n",
    "#     Returns:\n",
    "#         List of selected feature names\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         X_train, _, y_train, _ = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "        \n",
    "#         scaler = StandardScaler()\n",
    "#         X_train_scaled = scaler.fit_transform(X_train)\n",
    "        \n",
    "#         rf_model = RandomForestRegressor(\n",
    "#             n_estimators=n_estimators, \n",
    "#             max_depth=max_depth, \n",
    "#             random_state=random_state,\n",
    "#             n_jobs=-1\n",
    "#         )\n",
    "#         rf_model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "#         feature_importance_df = pd.DataFrame({\n",
    "#             'Feature': X.columns,\n",
    "#             'Importance': rf_model.feature_importances_\n",
    "#         }).sort_values(by='Importance', ascending=False)\n",
    "        \n",
    "#         print(\"\\nTop 10 Features by Importance:\")\n",
    "#         print(feature_importance_df.head(10))\n",
    "        \n",
    "#         plot_feature_importance(feature_importance_df)\n",
    "        \n",
    "#         return feature_importance_df['Feature'].head(n_features).tolist()\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in feature selection: {str(e)}\")\n",
    "#         raise\n",
    "\n",
    "# def get_default_params() -> Dict:\n",
    "#     \"\"\"Get default LightGBM parameters.\"\"\"\n",
    "#     return {\n",
    "#         'objective': 'regression',\n",
    "#         'metric': 'mae',\n",
    "#         'boosting_type': 'gbdt',\n",
    "#         'num_leaves': 31,\n",
    "#         'learning_rate': 0.05,\n",
    "#         'feature_fraction': 0.9,\n",
    "#         'bagging_fraction': 0.8,\n",
    "#         'bagging_freq': 5,\n",
    "#         'verbose': -1,\n",
    "#         'n_jobs': -1,\n",
    "#         'min_child_samples': 20,\n",
    "#         'max_depth': -1  # Let LightGBM optimize the depth\n",
    "#     }\n",
    "\n",
    "# def train_model_with_selected_features_cv(\n",
    "#     X: pd.DataFrame, \n",
    "#     y: pd.Series, \n",
    "#     selected_features: List[str],\n",
    "#     testData: Optional[pd.DataFrame] = None,\n",
    "#     idTest: Optional[pd.Series] = None,\n",
    "#     n_folds: int = 5,\n",
    "#     params: Optional[Dict] = None,\n",
    "#     random_state: int = 42\n",
    "# ) -> Tuple:\n",
    "#     \"\"\"\n",
    "#     Train LightGBM model with cross-validation and early stopping.\n",
    "    \n",
    "#     Args:\n",
    "#         X: Feature DataFrame\n",
    "#         y: Target Series\n",
    "#         selected_features: List of features to use\n",
    "#         testData: Test DataFrame\n",
    "#         idTest: Test IDs\n",
    "#         n_folds: Number of CV folds\n",
    "#         params: LightGBM parameters\n",
    "#         random_state: Random seed\n",
    "        \n",
    "#     Returns:\n",
    "#         Tuple of (model, predictions)\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         X_selected = X[selected_features]\n",
    "#         params = params or get_default_params()\n",
    "        \n",
    "#         # Initialize metrics storage\n",
    "#         oof_predictions = np.zeros(len(X_selected))\n",
    "#         feature_importance = np.zeros(len(selected_features))\n",
    "#         scores = []\n",
    "        \n",
    "#         # Prepare KFold\n",
    "#         kf = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "        \n",
    "#         print(f\"\\nStarting {n_folds}-fold cross-validation...\")\n",
    "        \n",
    "#         for fold, (train_idx, val_idx) in enumerate(kf.split(X_selected), 1):\n",
    "#             print(f\"\\nFold {fold}/{n_folds}\")\n",
    "            \n",
    "#             # Split data\n",
    "#             X_train = X_selected.iloc[train_idx]\n",
    "#             y_train = y.iloc[train_idx]\n",
    "#             X_val = X_selected.iloc[val_idx]\n",
    "#             y_val = y.iloc[val_idx]\n",
    "            \n",
    "#             # Scale features\n",
    "#             scaler = StandardScaler()\n",
    "#             X_train_scaled = scaler.fit_transform(X_train)\n",
    "#             X_val_scaled = scaler.transform(X_val)\n",
    "            \n",
    "#             # Create datasets\n",
    "#             train_data = lgb.Dataset(X_train_scaled, label=y_train)\n",
    "#             val_data = lgb.Dataset(X_val_scaled, label=y_val, reference=train_data)\n",
    "            \n",
    "#             # Train model\n",
    "#             model = lgb.train(\n",
    "#                 params,\n",
    "#                 train_data,\n",
    "#                 valid_sets=[train_data, val_data],\n",
    "#                 num_boost_round=1000,\n",
    "#                 callbacks=[\n",
    "#                     lgb.early_stopping(stopping_rounds=50),\n",
    "#                     lgb.log_evaluation(period=100)\n",
    "#                 ]\n",
    "#             )\n",
    "            \n",
    "#             # Make validation predictions\n",
    "#             oof_predictions[val_idx] = model.predict(X_val_scaled)\n",
    "            \n",
    "#             # Accumulate feature importance\n",
    "#             fold_importance = model.feature_importance(importance_type='gain')\n",
    "#             feature_importance += fold_importance\n",
    "            \n",
    "#             # Calculate fold score\n",
    "#             fold_score = mean_absolute_error(y_val, oof_predictions[val_idx])\n",
    "#             scores.append(fold_score)\n",
    "#             print(f\"Fold {fold} MAE: {fold_score:.4f}\")\n",
    "        \n",
    "#         # Calculate and print overall metrics\n",
    "#         mean_score = np.mean(scores)\n",
    "#         std_score = np.std(scores)\n",
    "#         print(f\"\\nOverall CV MAE: {mean_score:.4f} ± {std_score:.4f}\")\n",
    "        \n",
    "#         # Plot feature importance\n",
    "#         importance_df = pd.DataFrame({\n",
    "#             'Feature': selected_features,\n",
    "#             'Importance': feature_importance / n_folds\n",
    "#         }).sort_values(by='Importance', ascending=False)\n",
    "        \n",
    "#         plot_feature_importance(importance_df, \"LightGBM Feature Importance\")\n",
    "        \n",
    "#         # Train final model on full dataset\n",
    "#         print(\"\\nTraining final model on full dataset...\")\n",
    "#         scaler = StandardScaler()\n",
    "#         X_scaled = scaler.fit_transform(X_selected)\n",
    "        \n",
    "#         final_train_data = lgb.Dataset(X_scaled, label=y)\n",
    "#         final_model = lgb.train(\n",
    "#             params,\n",
    "#             final_train_data,\n",
    "#             num_boost_round=model.best_iteration,\n",
    "#             callbacks=[lgb.log_evaluation(period=100)]\n",
    "#         )\n",
    "        \n",
    "#         # Make predictions on test data if provided\n",
    "#         if testData is not None and idTest is not None:\n",
    "#             print(\"\\nMaking predictions on test data...\")\n",
    "#             X_test_scaled = scaler.transform(testData[selected_features])\n",
    "#             test_predictions = final_model.predict(X_test_scaled)\n",
    "            \n",
    "#             output = pd.DataFrame({\n",
    "#                 'id': idTest,\n",
    "#                 'yield': test_predictions\n",
    "#             })\n",
    "            \n",
    "#             output_file = 'submission.csv'\n",
    "#             output.to_csv(output_file, index=False)\n",
    "#             print(f\"Predictions saved to {output_file}\")\n",
    "            \n",
    "#             return final_model, output\n",
    "        \n",
    "#         return final_model, None\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in model training: {str(e)}\")\n",
    "#         raise\n",
    "\n",
    "# # Example usage\n",
    "# def main():\n",
    "#     try:\n",
    "#         X = trainData[features_to_use]\n",
    "#         y = trainData['yield']\n",
    "        \n",
    "#         print(\"Starting feature selection...\")\n",
    "#         selected_features = select_best_features_using_importance(X, y)\n",
    "        \n",
    "#         print(\"\\nStarting model training...\")\n",
    "#         model, predictions = train_model_with_selected_features_cv(\n",
    "#             X, y, selected_features, \n",
    "#             testData=testData, \n",
    "#             idTest=testData['id']\n",
    "#         )\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in main execution: {str(e)}\")\n",
    "#         raise\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.model_selection import train_test_split, KFold\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "# import lightgbm as lgb\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from typing import List, Tuple, Dict, Optional, Union, Any\n",
    "# import logging\n",
    "# import joblib\n",
    "# from pathlib import Path\n",
    "# from datetime import datetime\n",
    "# import yaml\n",
    "# from dataclasses import dataclass\n",
    "# import warnings\n",
    "# import os\n",
    "\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# # Configure logging\n",
    "# logging.basicConfig(\n",
    "#     level=logging.INFO,\n",
    "#     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "#     handlers=[\n",
    "#         logging.FileHandler('ml_pipeline.log'),\n",
    "#         logging.StreamHandler()\n",
    "#     ]\n",
    "# )\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "# @dataclass\n",
    "# class ModelConfig:\n",
    "#     \"\"\"Data class for model configuration.\"\"\"\n",
    "#     n_features: int = 5\n",
    "#     n_estimators: int = 200\n",
    "#     max_depth: int = 5\n",
    "#     n_folds: int = 5\n",
    "#     random_state: int = 42\n",
    "#     model_dir: str = \"models\"\n",
    "    \n",
    "#     @classmethod\n",
    "#     def from_yaml(cls, yaml_path: str) -> 'ModelConfig':\n",
    "#         \"\"\"Load configuration from YAML file.\"\"\"\n",
    "#         with open(yaml_path, 'r') as f:\n",
    "#             config_dict = yaml.safe_load(f)\n",
    "#         return cls(**config_dict)\n",
    "\n",
    "# class FeatureSelector:\n",
    "#     \"\"\"Class for feature selection operations.\"\"\"\n",
    "    \n",
    "#     def __init__(self, config: ModelConfig):\n",
    "#         self.config = config\n",
    "#         self.feature_importance_df: Optional[pd.DataFrame] = None\n",
    "        \n",
    "#     def plot_feature_importance(self, importance_df: pd.DataFrame, title: str = \"Feature Importance\") -> None:\n",
    "#         \"\"\"Enhanced feature importance visualization.\"\"\"\n",
    "#         plt.figure(figsize=(12, 8))\n",
    "#         sns.barplot(\n",
    "#             data=importance_df.head(10),\n",
    "#             x='Importance',\n",
    "#             y='Feature',\n",
    "#             palette='viridis'\n",
    "#         )\n",
    "#         plt.title(title, fontsize=14, pad=20)\n",
    "#         plt.xlabel('Importance Score', fontsize=12)\n",
    "#         plt.ylabel('Features', fontsize=12)\n",
    "#         plt.tight_layout()\n",
    "        \n",
    "#         # Save plot\n",
    "#         timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "#         plt.savefig(f'feature_importance_{timestamp}.png', dpi=300, bbox_inches='tight')\n",
    "#         plt.close()\n",
    "\n",
    "#     def select_features(\n",
    "#         self,\n",
    "#         X: pd.DataFrame,\n",
    "#         y: pd.Series,\n",
    "#         correlation_threshold: float = 0.95\n",
    "#     ) -> List[str]:\n",
    "#         \"\"\"\n",
    "#         Enhanced feature selection using both importance and correlation analysis.\n",
    "#         \"\"\"\n",
    "#         try:\n",
    "#             logger.info(\"Starting feature selection process...\")\n",
    "            \n",
    "#             # Remove highly correlated features\n",
    "#             correlation_matrix = X.corr().abs()\n",
    "#             upper = correlation_matrix.where(\n",
    "#                 np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)\n",
    "#             )\n",
    "#             to_drop = [\n",
    "#                 column for column in upper.columns \n",
    "#                 if any(upper[column] > correlation_threshold)\n",
    "#             ]\n",
    "#             X_uncorrelated = X.drop(columns=to_drop)\n",
    "#             logger.info(f\"Removed {len(to_drop)} highly correlated features\")\n",
    "            \n",
    "#             # Split and scale data\n",
    "#             X_train, X_val, y_train, y_val = train_test_split(\n",
    "#                 X_uncorrelated,\n",
    "#                 y,\n",
    "#                 test_size=0.2,\n",
    "#                 random_state=self.config.random_state\n",
    "#             )\n",
    "            \n",
    "#             scaler = StandardScaler()\n",
    "#             X_train_scaled = scaler.fit_transform(X_train)\n",
    "#             X_val_scaled = scaler.transform(X_val)\n",
    "            \n",
    "#             # Train Random Forest for feature importance\n",
    "#             rf_model = RandomForestRegressor(\n",
    "#                 n_estimators=self.config.n_estimators,\n",
    "#                 max_depth=self.config.max_depth,\n",
    "#                 random_state=self.config.random_state,\n",
    "#                 n_jobs=-1\n",
    "#             )\n",
    "#             rf_model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "#             # Calculate feature importance\n",
    "#             self.feature_importance_df = pd.DataFrame({\n",
    "#                 'Feature': X_uncorrelated.columns,\n",
    "#                 'Importance': rf_model.feature_importances_,\n",
    "#                 'Cumulative_Importance': np.cumsum(rf_model.feature_importances_)\n",
    "#             }).sort_values(by='Importance', ascending=False)\n",
    "            \n",
    "#             logger.info(\"\\nTop 10 Features by Importance:\")\n",
    "#             logger.info(self.feature_importance_df.head(10))\n",
    "            \n",
    "#             self.plot_feature_importance(self.feature_importance_df)\n",
    "            \n",
    "#             # Select features that explain 95% of the variance\n",
    "#             selected_features = self.feature_importance_df[\n",
    "#                 self.feature_importance_df['Cumulative_Importance'] <= 0.95\n",
    "#             ]['Feature'].tolist()\n",
    "            \n",
    "#             return selected_features[:self.config.n_features]\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             logger.error(f\"Error in feature selection: {str(e)}\")\n",
    "#             raise\n",
    "\n",
    "# class ModelTrainer:\n",
    "#     \"\"\"Class for model training and evaluation.\"\"\"\n",
    "    \n",
    "#     def __init__(self, config: ModelConfig):\n",
    "#         self.config = config\n",
    "#         self._ensure_model_dir()\n",
    "        \n",
    "#     def _ensure_model_dir(self) -> None:\n",
    "#         \"\"\"Ensure model directory exists.\"\"\"\n",
    "#         Path(self.config.model_dir).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "#     def _get_default_params(self) -> Dict:\n",
    "#         \"\"\"Get enhanced LightGBM parameters.\"\"\"\n",
    "#         return {\n",
    "#             'objective': 'regression',\n",
    "#             'metric': ['mae', 'rmse'],\n",
    "#             'boosting_type': 'gbdt',\n",
    "#             'num_leaves': 31,\n",
    "#             'learning_rate': 0.05,\n",
    "#             'feature_fraction': 0.9,\n",
    "#             'bagging_fraction': 0.8,\n",
    "#             'bagging_freq': 5,\n",
    "#             'verbose': -1,\n",
    "#             'n_jobs': -1,\n",
    "#             'min_child_samples': 20,\n",
    "#             'max_depth': -1,\n",
    "#             'reg_alpha': 0.1,\n",
    "#             'reg_lambda': 0.1,\n",
    "#             'min_split_gain': 0.01\n",
    "#         }\n",
    "    \n",
    "#     def train_and_evaluate(\n",
    "#         self,\n",
    "#         X: pd.DataFrame,\n",
    "#         y: pd.Series,\n",
    "#         selected_features: List[str],\n",
    "#         test_data: Optional[pd.DataFrame] = None,\n",
    "#         test_ids: Optional[pd.Series] = None,\n",
    "#         params: Optional[Dict] = None\n",
    "#     ) -> Tuple[Any, Optional[pd.DataFrame], Dict[str, float]]:\n",
    "#         \"\"\"\n",
    "#         Enhanced training with cross-validation and comprehensive evaluation.\n",
    "#         \"\"\"\n",
    "#         try:\n",
    "#             X_selected = X[selected_features]\n",
    "#             params = params or self._get_default_params()\n",
    "            \n",
    "#             # Initialize arrays for predictions and importance\n",
    "#             oof_predictions = np.zeros(len(X_selected))\n",
    "#             feature_importance = np.zeros(len(selected_features))\n",
    "#             metrics = {\n",
    "#                 'mae_scores': [],\n",
    "#                 'rmse_scores': [],\n",
    "#                 'r2_scores': []\n",
    "#             }\n",
    "            \n",
    "#             # Prepare KFold\n",
    "#             kf = KFold(\n",
    "#                 n_splits=self.config.n_folds,\n",
    "#                 shuffle=True,\n",
    "#                 random_state=self.config.random_state\n",
    "#             )\n",
    "            \n",
    "#             logger.info(f\"\\nStarting {self.config.n_folds}-fold cross-validation...\")\n",
    "            \n",
    "#             # Cross-validation loop\n",
    "#             for fold, (train_idx, val_idx) in enumerate(kf.split(X_selected), 1):\n",
    "#                 logger.info(f\"\\nFold {fold}/{self.config.n_folds}\")\n",
    "                \n",
    "#                 # Split and scale data\n",
    "#                 X_train = X_selected.iloc[train_idx]\n",
    "#                 y_train = y.iloc[train_idx]\n",
    "#                 X_val = X_selected.iloc[val_idx]\n",
    "#                 y_val = y.iloc[val_idx]\n",
    "                \n",
    "#                 scaler = StandardScaler()\n",
    "#                 X_train_scaled = scaler.fit_transform(X_train)\n",
    "#                 X_val_scaled = scaler.transform(X_val)\n",
    "                \n",
    "#                 # Create datasets\n",
    "#                 train_data = lgb.Dataset(X_train_scaled, label=y_train)\n",
    "#                 val_data = lgb.Dataset(X_val_scaled, label=y_val, reference=train_data)\n",
    "                \n",
    "#                 # Train model\n",
    "#                 model = lgb.train(\n",
    "#                     params,\n",
    "#                     train_data,\n",
    "#                     valid_sets=[train_data, val_data],\n",
    "#                     num_boost_round=1000,\n",
    "#                     callbacks=[\n",
    "#                         lgb.early_stopping(stopping_rounds=50),\n",
    "#                         lgb.log_evaluation(period=100)\n",
    "#                     ]\n",
    "#                 )\n",
    "                \n",
    "#                 # Make predictions\n",
    "#                 fold_preds = model.predict(X_val_scaled)\n",
    "#                 oof_predictions[val_idx] = fold_preds\n",
    "                \n",
    "#                 # Calculate metrics\n",
    "#                 metrics['mae_scores'].append(\n",
    "#                     mean_absolute_error(y_val, fold_preds)\n",
    "#                 )\n",
    "#                 metrics['rmse_scores'].append(\n",
    "#                     np.sqrt(mean_squared_error(y_val, fold_preds))\n",
    "#                 )\n",
    "#                 metrics['r2_scores'].append(\n",
    "#                     r2_score(y_val, fold_preds)\n",
    "#                 )\n",
    "                \n",
    "#                 # Update feature importance\n",
    "#                 feature_importance += model.feature_importance(importance_type='gain')\n",
    "                \n",
    "#                 logger.info(\n",
    "#                     f\"Fold {fold} - MAE: {metrics['mae_scores'][-1]:.4f}, \"\n",
    "#                     f\"RMSE: {metrics['rmse_scores'][-1]:.4f}, \"\n",
    "#                     f\"R2: {metrics['r2_scores'][-1]:.4f}\"\n",
    "#                 )\n",
    "            \n",
    "#             # Calculate and log overall metrics\n",
    "#             final_metrics = {\n",
    "#                 'mean_mae': np.mean(metrics['mae_scores']),\n",
    "#                 'std_mae': np.std(metrics['mae_scores']),\n",
    "#                 'mean_rmse': np.mean(metrics['rmse_scores']),\n",
    "#                 'std_rmse': np.std(metrics['rmse_scores']),\n",
    "#                 'mean_r2': np.mean(metrics['r2_scores']),\n",
    "#                 'std_r2': np.std(metrics['r2_scores'])\n",
    "#             }\n",
    "            \n",
    "#             logger.info(f\"\\nFinal Model Metrics:\\n{final_metrics}\")\n",
    "            \n",
    "#             # Save final model\n",
    "#             timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "#             final_model_path = os.path.join(self.config.model_dir, f'model_{timestamp}.pkl')\n",
    "#             joblib.dump(model, final_model_path)\n",
    "#             logger.info(f\"Model saved at: {final_model_path}\")\n",
    "            \n",
    "#             # Make predictions on test data if available\n",
    "#             predictions_df = None\n",
    "#             if test_data is not None and test_ids is not None:\n",
    "#                 logger.info(\"\\nMaking predictions on test data...\")\n",
    "#                 X_test_scaled = scaler.transform(test_data[selected_features])\n",
    "#                 test_predictions = model.predict(X_test_scaled)\n",
    "\n",
    "#                 predictions_df = pd.DataFrame({\n",
    "#                     'id': test_ids,\n",
    "#                     'yield': test_predictions\n",
    "#                 })\n",
    "\n",
    "#                 # Ensure output directory exists\n",
    "#                 output_dir = 'output_model'\n",
    "#                 if not os.path.exists(output_dir):\n",
    "#                     os.makedirs(output_dir)\n",
    "\n",
    "#                 # Save predictions to CSV file\n",
    "#                 output_file = os.path.join(output_dir, f'predictions_{timestamp}.csv')\n",
    "#                 predictions_df.to_csv(output_file, index=False)\n",
    "#                 logger.info(f\"Predictions saved to {output_file}\")\n",
    "                \n",
    "#             return model, predictions_df, final_metrics\n",
    "        \n",
    "#         except Exception as e:\n",
    "#             logger.error(f\"Error during training and evaluation: {str(e)}\")\n",
    "#             raise\n",
    "\n",
    "# class MLPipeline:\n",
    "#     \"\"\"Main pipeline class.\"\"\"\n",
    "    \n",
    "#     def __init__(self, config: ModelConfig):\n",
    "#         self.config = config\n",
    "#         self.feature_selector = FeatureSelector(config)\n",
    "#         self.model_trainer = ModelTrainer(config)\n",
    "        \n",
    "#     def run(\n",
    "#         self,\n",
    "#         train_data: pd.DataFrame,\n",
    "#         target_column: str,\n",
    "#         test_data: Optional[pd.DataFrame] = None,\n",
    "#         feature_columns: Optional[List[str]] = None\n",
    "#     ) -> Tuple[Any, Optional[pd.DataFrame], Dict[str, float]]:\n",
    "#         \"\"\"\n",
    "#         Enhanced pipeline execution process.\n",
    "#         \"\"\"\n",
    "#         try:\n",
    "#             # Feature selection\n",
    "#             X = train_data.drop(columns=[target_column])\n",
    "#             y = train_data[target_column]\n",
    "#             selected_features = (\n",
    "#                 feature_columns if feature_columns \n",
    "#                 else self.feature_selector.select_features(X, y)\n",
    "#             )\n",
    "            \n",
    "#             logger.info(f\"Selected Features: {selected_features}\")\n",
    "            \n",
    "#             # Train and evaluate the model\n",
    "#             model, predictions, metrics = self.model_trainer.train_and_evaluate(\n",
    "#                 X,\n",
    "#                 y,\n",
    "#                 selected_features,\n",
    "#                 test_data=test_data,\n",
    "#                 test_ids=test_data.index if test_data is not None else None\n",
    "#             )\n",
    "            \n",
    "#             return model, predictions, metrics\n",
    "        \n",
    "#         except Exception as e:\n",
    "#             logger.error(f\"Error in pipeline run: {str(e)}\")\n",
    "#             raise\n",
    "\n",
    "# def main():\n",
    "#     # Load configuration\n",
    "#     config = ModelConfig.from_yaml(\"config.yaml\")\n",
    "    \n",
    "#     # Load data\n",
    "#     trainData = pd.read_csv('train.csv')\n",
    "#     testData = pd.read_csv('test.csv')  # If available\n",
    "    \n",
    "#     # Run pipeline\n",
    "#     pipeline = MLPipeline(config)\n",
    "#     model, predictions, metrics = pipeline.run(\n",
    "#         train_data=trainData,\n",
    "#         target_column='yield',\n",
    "#         test_data=testData\n",
    "#     )\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip uninstall lightgbm -y\n",
    "# %pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "# from lightgbm import LGBMRegressor\n",
    "\n",
    "# # Step 1: Select best features using Random Forest Feature Importance\n",
    "# def select_best_features_using_importance(X, y, n_estimators=200, max_depth=5, random_state=42):\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "    \n",
    "#     # Normalize features\n",
    "#     scaler = StandardScaler()\n",
    "#     X_train_scaled = scaler.fit_transform(X_train)\n",
    "#     X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "#     # Fit RandomForest to find feature importance\n",
    "#     rf_model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, random_state=random_state)\n",
    "#     rf_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "#     # Get feature importance\n",
    "#     feature_importances = rf_model.feature_importances_\n",
    "    \n",
    "#     # Sort features by importance\n",
    "#     feature_importance_df = pd.DataFrame({\n",
    "#         'Feature': X.columns,\n",
    "#         'Importance': feature_importances\n",
    "#     }).sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "#     print(\"Features sorted by importance:\")\n",
    "#     print(feature_importance_df)\n",
    "    \n",
    "#     # Select top features\n",
    "#     selected_features = feature_importance_df['Feature'].head(5)  # Selecting top 5 features\n",
    "#     return selected_features\n",
    "\n",
    "# # Step 2: Train a model using selected features with Cross-Validation\n",
    "# def train_model_with_selected_features_cv(X, y, selected_features, testData=None, idTest=None):\n",
    "#     # Prepare the selected features dataframe\n",
    "#     X_selected = X[selected_features]\n",
    "\n",
    "#     # Normalize the features\n",
    "#     scaler = StandardScaler()\n",
    "#     X_scaled = scaler.fit_transform(X_selected)\n",
    "\n",
    "#     # Split the dataset into training and validation sets\n",
    "#     X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#     # LightGBM Model\n",
    "#     model = LGBMRegressor(n_estimators=1000, learning_rate=0.09, max_depth=5, random_state=42)\n",
    "\n",
    "#     # Fit the model with early stopping\n",
    "#     model.fit(X_train, y_train, \n",
    "#               eval_set=[(X_val, y_val)], \n",
    "#               eval_metric='mae', \n",
    "#               early_stopping_rounds=50, \n",
    "#               verbose=100)\n",
    "\n",
    "#     # Validation Predictions\n",
    "#     val_predictions = model.predict(X_val)\n",
    "#     mae = mean_absolute_error(y_val, val_predictions)\n",
    "#     print(f\"\\nValidation MAE: {mae:.4f}\")\n",
    "\n",
    "#     # Fit the model on the entire dataset if test data is provided\n",
    "#     model.fit(X_scaled, y)\n",
    "\n",
    "#     # Predicting on test data\n",
    "#     if testData is not None:\n",
    "#         X_test_scaled = scaler.transform(testData[selected_features])\n",
    "#         testPredictions = model.predict(X_test_scaled)\n",
    "        \n",
    "#         output = pd.DataFrame({\n",
    "#             'id': idTest,\n",
    "#             'yield': testPredictions\n",
    "#         })\n",
    "        \n",
    "#         output_file = 'submission.csv'\n",
    "#         output.to_csv(output_file, index=False)\n",
    "#         print(f\"Predictions saved to {output_file}\")\n",
    "\n",
    "# # Example usage with your datasets\n",
    "# # Ensure you have your trainData and testData DataFrame populated with data\n",
    "# # trainData = pd.read_csv('path_to_train_data.csv')  # Uncomment and specify path\n",
    "# # testData = pd.read_csv('path_to_test_data.csv')    # Uncomment and specify path\n",
    "\n",
    "# # Specify features you want to use\n",
    "# features_to_use = trainData.columns.drop('yield')  # Adjust according to your dataset\n",
    "# features_to_use = features_to_use.drop('id')  # Adjust according to your dataset\n",
    "# features_to_use = features_to_use.drop('Row#')  # Adjust according to your dataset\n",
    "# X = trainData[features_to_use]  # All features initially\n",
    "# y = trainData['yield']\n",
    "\n",
    "# # Step 1: Select best features using feature importance\n",
    "# selected_features = select_best_features_using_importance(X, y)\n",
    "\n",
    "# # Step 2: Train a model with cross-validation using those selected features\n",
    "# train_model_with_selected_features_cv(\n",
    "#     X, \n",
    "#     y, \n",
    "#     selected_features, \n",
    "#     testData=testData, \n",
    "#     idTest=testData['id']  # Ensure 'id' column exists in testData\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import lightgbm as lgb\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# # Load the data\n",
    "# train_data = pd.read_csv('train.csv')\n",
    "# test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# # Separate features and target\n",
    "# X = train_data.drop(columns=['yield', 'id', 'Row#'])\n",
    "# y = train_data['yield']\n",
    "\n",
    "# # Split data into training and validation sets\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Create LightGBM dataset structures\n",
    "# train_set = lgb.Dataset(X_train, label=y_train)\n",
    "# val_set = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "# # Set model parameters\n",
    "# params = {\n",
    "#     'objective': 'regression',\n",
    "#     'metric': 'rmse',  # Root mean squared error\n",
    "#     'boosting_type': 'gbdt',\n",
    "#     'verbose': -1\n",
    "# }\n",
    "\n",
    "# # Define the parameter grid\n",
    "# param_grid = {\n",
    "#     'learning_rate': [0.01, 0.05, 0.1],\n",
    "#     'num_leaves': [20, 31, 50],\n",
    "#     'max_depth': [-1, 10, 20],\n",
    "#     'feature_fraction': [0.8, 0.9, 1.0],\n",
    "#     'bagging_fraction': [0.8, 0.9, 1.0],\n",
    "#     'bagging_freq': [1, 5, 10],\n",
    "# }\n",
    "\n",
    "# # Initialize the model\n",
    "# lgb_model = lgb.LGBMRegressor(**params)\n",
    "\n",
    "# # Use GridSearchCV for hyperparameter tuning\n",
    "# grid_search = GridSearchCV(lgb_model, param_grid, scoring='neg_mean_squared_error', cv=3, verbose=2)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Best parameters\n",
    "# print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "# # Train the model with the best parameters\n",
    "# best_model = grid_search.best_estimator_\n",
    "\n",
    "# # Validate the model\n",
    "# y_pred_val = best_model.predict(X_val)\n",
    "# val_rmse = mean_squared_error(y_val, y_pred_val, squared=False)\n",
    "# val_mae = mean_absolute_error(y_val, y_pred_val)\n",
    "\n",
    "# print(f'Validation RMSE: {val_rmse}')\n",
    "# print(f'Validation MAE: {val_mae}')\n",
    "\n",
    "# # Predict on the test set\n",
    "# X_test = test_data.drop(columns=['id', 'Row#'])\n",
    "# y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "# # Create a DataFrame for the output\n",
    "# output = pd.DataFrame({\n",
    "#     'id': test_data['id'],\n",
    "#     'prediction': y_pred_test\n",
    "# })\n",
    "\n",
    "# # Save the predictions to a CSV file\n",
    "# output.to_csv('test_predictions.csv', index=False)\n",
    "# print(\"Predictions saved to 'test_predictions.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lightgbm as lgb\n",
    "# from lightgbm import early_stopping\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # Step 1: Train a model using all features with LightGBM and early stopping\n",
    "# def train_model_with_all_features_cv(X, y, testData=None, idTest=None, model_type='lgbm'):\n",
    "#     scaler = StandardScaler()\n",
    "#     X_scaled = scaler.fit_transform(X)  # Standardize the features\n",
    "\n",
    "#     # Select model type\n",
    "#     if model_type == 'lgbm':\n",
    "#         model = lgb.LGBMRegressor(n_estimators=1000, learning_rate=0.05, max_depth=5, random_state=42)\n",
    "#     else:\n",
    "#         raise ValueError(\"Unsupported model type\")\n",
    "\n",
    "#     # Split into train and validation set for early stopping\n",
    "#     X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "#     # Fit model with early stopping using callbacks\n",
    "#     model.fit(X_train, y_train, \n",
    "#               eval_set=[(X_val, y_val)], \n",
    "#               callbacks=[early_stopping(stopping_rounds=50, verbose=100)],  # Stops if no improvement over 50 rounds\n",
    "#               eval_metric='mae')  # Monitor mean absolute error\n",
    "    \n",
    "#     # Predict on validation set\n",
    "#     val_predictions = model.predict(X_val)\n",
    "    \n",
    "#     # Evaluate performance on validation set\n",
    "#     val_mae = mean_absolute_error(y_val, val_predictions)\n",
    "#     print(f\"\\nValidation MAE: {val_mae:.4f}\")\n",
    "    \n",
    "#     # Fit the model on the entire dataset (without validation split) after early stopping\n",
    "#     model.fit(X_scaled, y)\n",
    "    \n",
    "#     # Predict on testData (optional)\n",
    "#     if testData is not None:\n",
    "#         X_test_scaled = scaler.transform(testData)\n",
    "#         testPredictions = model.predict(X_test_scaled)\n",
    "        \n",
    "#         output = pd.DataFrame({\n",
    "#             'id': idTest,\n",
    "#             'yield': testPredictions\n",
    "#         })\n",
    "        \n",
    "#         output_file = 'submission.csv'\n",
    "#         output.to_csv(output_file, index=False)\n",
    "#         print(f\"Predictions saved to {output_file}\")\n",
    "\n",
    "# # Example usage\n",
    "# X = trainData.drop(columns=['yield'])  # All features except the target variable\n",
    "# y = trainData['yield']\n",
    "\n",
    "# # Step 1: Train a model with cross-validation using all features and early stopping\n",
    "# train_model_with_all_features_cv(X, y, testData=testData, idTest=testData['id'], model_type='lgbm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "# from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# def select_best_features_using_importance(X, y, n_estimators=200, max_depth=5, random_state=42):\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "    \n",
    "#     scaler = StandardScaler()\n",
    "#     X_train_scaled = scaler.fit_transform(X_train)\n",
    "#     X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "#     rf_model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, random_state=random_state)\n",
    "#     rf_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "#     feature_importances = rf_model.feature_importances_\n",
    "    \n",
    "#     feature_importance_df = pd.DataFrame({\n",
    "#         'Feature': X.columns,\n",
    "#         'Importance': feature_importances\n",
    "#     }).sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "#     print(\"Features sorted by importance:\")\n",
    "#     print(feature_importance_df)\n",
    "    \n",
    "#     selected_features = feature_importance_df['Feature'].head(5)\n",
    "#     return selected_features\n",
    "\n",
    "# def train_model_with_selected_features_cv(X, y, selected_features, testData=None, idTest=None, model_type='gradient_boosting'):\n",
    "#     X_selected = X[selected_features]\n",
    "    \n",
    "#     scaler = StandardScaler()\n",
    "#     X_scaled = scaler.fit_transform(X_selected)\n",
    "\n",
    "#     if model_type == 'gradient_boosting':\n",
    "#         # Define the parameter grid for GridSearchCV\n",
    "#         param_grid = {\n",
    "#             'n_estimators': [100, 120, 150],\n",
    "#             'learning_rate': [0.05, 0.06, 0.09, 0.1],\n",
    "#             'max_depth': [3, 4, 5, 6],\n",
    "#             'min_samples_split': [2, 5, 7],\n",
    "#             'min_samples_leaf': [1, 2, 4, 5],\n",
    "#             'subsample': [0.5, 0.7, 0.8, 0.9, 1.0]\n",
    "#         }\n",
    "        \n",
    "#         base_model = GradientBoostingRegressor(random_state=42)\n",
    "#     else:\n",
    "#         raise ValueError(\"Unsupported model type\")\n",
    "\n",
    "#     # Perform GridSearchCV\n",
    "#     grid_search = GridSearchCV(\n",
    "#         estimator=base_model,\n",
    "#         param_grid=param_grid,\n",
    "#         cv=5,\n",
    "#         scoring='neg_mean_absolute_error',\n",
    "#         n_jobs=-1,\n",
    "#         verbose=2\n",
    "#     )\n",
    "    \n",
    "#     # Fit GridSearchCV\n",
    "#     grid_search.fit(X_scaled, y)\n",
    "    \n",
    "#     # Print best parameters and score\n",
    "#     print(\"\\nBest parameters found:\")\n",
    "#     print(grid_search.best_params_)\n",
    "#     print(f\"Best cross-validated MAE: {-grid_search.best_score_:.4f}\")\n",
    "    \n",
    "#     # Use best model for predictions\n",
    "#     best_model = grid_search.best_estimator_\n",
    "    \n",
    "#     # Perform 5-Fold Cross-Validation with best model\n",
    "#     cv_scores = cross_val_score(best_model, X_scaled, y, cv=5, scoring='neg_mean_absolute_error')\n",
    "#     print(f\"\\nFinal Cross-validated MAE (5-Fold): {np.mean(-cv_scores):.4f}\")\n",
    "    \n",
    "#     # Fit the best model on the entire dataset\n",
    "#     best_model.fit(X_scaled, y)\n",
    "    \n",
    "#     # Predict on testData if provided\n",
    "#     if testData is not None:\n",
    "#         X_test_scaled = scaler.transform(testData[selected_features])\n",
    "#         testPredictions = best_model.predict(X_test_scaled)\n",
    "        \n",
    "#         output = pd.DataFrame({\n",
    "#             'id': idTest,\n",
    "#             'yield': testPredictions\n",
    "#         })\n",
    "        \n",
    "#         output_file = 'submission.csv'\n",
    "#         output.to_csv(output_file, index=False)\n",
    "#         print(f\"Predictions saved to {output_file}\")\n",
    "        \n",
    "#     return best_model, grid_search\n",
    "\n",
    "# # Example usage\n",
    "# X = trainData[features_to_use]  # All features initially\n",
    "# y = trainData['yield']\n",
    "\n",
    "# # Step 1: Select best features using feature importance\n",
    "# selected_features = select_best_features_using_importance(X, y)\n",
    "\n",
    "# # Step 2: Train a model with GridSearchCV using those selected features\n",
    "# best_model, grid_search = train_model_with_selected_features_cv(\n",
    "#     X, y, selected_features, testData=testData, idTest=testData['id']\n",
    "# )\n",
    "\n",
    "# # Additional analysis of GridSearchCV results (optional)\n",
    "# cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "# print(\"\\nTop 5 parameter combinations:\")\n",
    "# print(cv_results[['params', 'mean_test_score', 'rank_test_score']]\n",
    "#       .sort_values('rank_test_score')\n",
    "#       .head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "# from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# def select_best_features_using_importance(X, y, n_estimators=200, max_depth=5, random_state=42):\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "    \n",
    "#     scaler = StandardScaler()\n",
    "#     X_train_scaled = scaler.fit_transform(X_train)\n",
    "    \n",
    "#     rf_model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, random_state=random_state)\n",
    "#     rf_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "#     feature_importances = rf_model.feature_importances_\n",
    "    \n",
    "#     feature_importance_df = pd.DataFrame({\n",
    "#         'Feature': X.columns,\n",
    "#         'Importance': feature_importances\n",
    "#     }).sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "#     print(\"Features sorted by importance:\")\n",
    "#     print(feature_importance_df)\n",
    "    \n",
    "#     selected_features = feature_importance_df['Feature'].head(5)\n",
    "#     return selected_features\n",
    "\n",
    "# def train_model_with_selected_features_cv(X, y, selected_features, testData=None, idTest=None):\n",
    "#     X_selected = X[selected_features]\n",
    "    \n",
    "#     scaler = StandardScaler()\n",
    "#     X_scaled = scaler.fit_transform(X_selected)\n",
    "\n",
    "#     # Split the data into training and validation sets\n",
    "#     X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#     # Define the model\n",
    "#     model = GradientBoostingRegressor(n_estimators=120, learning_rate=0.09, max_depth=5, random_state=42)\n",
    "\n",
    "#     # Fit the model with early stopping\n",
    "#     model.fit(X_train, y_train)\n",
    "\n",
    "#     # Early stopping\n",
    "#     best_iter = 0\n",
    "#     best_score = float('inf')\n",
    "#     for i, y_pred in enumerate(model.staged_predict(X_val)):\n",
    "#         val_score = mean_absolute_error(y_val, y_pred)\n",
    "#         if val_score < best_score:\n",
    "#             best_score = val_score\n",
    "#             best_iter = i + 1  # +1 because staged_predict starts from 0\n",
    "#         else:\n",
    "#             break  # Stop if the validation score doesn't improve\n",
    "\n",
    "#     print(f\"Best iteration: {best_iter}, Best MAE: {best_score:.4f}\")\n",
    "\n",
    "#     # Fit the model again with the best iteration\n",
    "#     model = GradientBoostingRegressor(n_estimators=best_iter, learning_rate=0.09, max_depth=5, random_state=42)\n",
    "#     model.fit(X_scaled, y)\n",
    "\n",
    "#     # Predict on testData if provided\n",
    "#     if testData is not None:\n",
    "#         X_test_scaled = scaler.transform(testData[selected_features])\n",
    "#         testPredictions = model.predict(X_test_scaled)\n",
    "        \n",
    "#         output = pd.DataFrame({\n",
    "#             'id': idTest,\n",
    "#             'yield': testPredictions\n",
    "#         })\n",
    "        \n",
    "#         output_file = 'submission_ES.csv'\n",
    "#         output.to_csv(output_file, index=False)\n",
    "#         print(f\"Predictions saved to {output_file}\")\n",
    "\n",
    "# # Example usage\n",
    "# X = trainData[features_to_use]  # All features initially\n",
    "# y = trainData['yield']\n",
    "\n",
    "# # Step 1: Select best features using feature importance\n",
    "# selected_features = select_best_features_using_importance(X, y)\n",
    "\n",
    "# # Step 2: Train a model with early stopping using those selected features\n",
    "# train_model_with_selected_features_cv(X, y, selected_features, testData=testData, idTest=testData['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Separate features and target variable from training data\n",
    "X_train = train_data.drop(columns=['id', 'Row#', 'yield'])  # Drop non-feature columns\n",
    "y_train = train_data['yield']\n",
    "X_test = test_data.drop(columns=['id', 'Row#'])  # Drop non-feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_model(model, X, y):\n",
    "#     kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "#     scores = []\n",
    "    \n",
    "#     for train_index, val_index in kf.split(X):\n",
    "#         X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[val_index]\n",
    "#         y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "#         model.fit(X_train_fold, y_train_fold, \n",
    "#                   eval_set=[(X_val_fold, y_val_fold)], \n",
    "#                   early_stopping_rounds=50, \n",
    "#                   verbose=False)\n",
    "        \n",
    "#         y_pred = model.predict(X_val_fold)\n",
    "#         score = mean_squared_error(y_val_fold, y_pred, squared=False)  # RMSE\n",
    "#         scores.append(score)\n",
    "    \n",
    "#     return np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LightGBM\n",
    "# lgb_model = lgb.LGBMRegressor()\n",
    "# lgb_mean_score, lgb_std_score = evaluate_model(lgb_model, X_train, y_train)\n",
    "# print(f'LightGBM: Mean RMSE = {lgb_mean_score:.4f}, Std = {lgb_std_score:.4f}')\n",
    "\n",
    "# # XGBoost\n",
    "# xgb_model = xgb.XGBRegressor(eval_metric='rmse')\n",
    "# xgb_mean_score, xgb_std_score = evaluate_model(xgb_model, X_train, y_train)\n",
    "# print(f'XGBoost: Mean RMSE = {xgb_mean_score:.4f}, Std = {xgb_std_score:.4f}')\n",
    "\n",
    "# # Gradient Boosting\n",
    "# gb_model = GradientBoostingRegressor()\n",
    "# gb_mean_score, gb_std_score = evaluate_model(gb_model, X_train, y_train)\n",
    "# print(f'Gradient Boosting: Mean RMSE = {gb_mean_score:.4f}, Std = {gb_std_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Choose the best model based on the evaluation\n",
    "# best_model = lgb_model  # or xgb_model, or gb_model based on your evaluation\n",
    "\n",
    "# # Fit the best model on the entire training data\n",
    "# best_model.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions on the test set\n",
    "# predictions = best_model.predict(X_test)\n",
    "\n",
    "# # Create a submission DataFrame\n",
    "# submission = pd.DataFrame({\n",
    "#     'id': test_data['id'],\n",
    "#     'predicted_yield': predictions\n",
    "# })\n",
    "\n",
    "# # Save predictions to a CSV file\n",
    "# submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "# import lightgbm as lgb\n",
    "# import xgboost as xgb\n",
    "# from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# # Load the data\n",
    "# train_data = pd.read_csv('train.csv')\n",
    "# test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# # Separate features and target variable from training data\n",
    "# X_train = train_data.drop(columns=['id', 'Row#', 'yield'])\n",
    "# y_train = train_data['yield']\n",
    "# X_test = test_data.drop(columns=['id', 'Row#'])\n",
    "\n",
    "# # Define separate evaluation functions for each model type\n",
    "# def evaluate_lgb(X, y):\n",
    "#     kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "#     rmse_scores = []\n",
    "#     mae_scores = []\n",
    "    \n",
    "#     for train_index, val_index in kf.split(X):\n",
    "#         X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[val_index]\n",
    "#         y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "#         model = lgb.LGBMRegressor(n_estimators=1000)\n",
    "#         model.fit(X_train_fold, y_train_fold,\n",
    "#                  eval_set=[(X_val_fold, y_val_fold)],\n",
    "#                  callbacks=[lgb.early_stopping(50)])\n",
    "        \n",
    "#         y_pred = model.predict(X_val_fold)\n",
    "#         rmse = mean_squared_error(y_val_fold, y_pred, squared=False)\n",
    "#         mae = mean_absolute_error(y_val_fold, y_pred)\n",
    "#         rmse_scores.append(rmse)\n",
    "#         mae_scores.append(mae)\n",
    "    \n",
    "#     return (np.mean(rmse_scores), np.std(rmse_scores), \n",
    "#             np.mean(mae_scores), np.std(mae_scores))\n",
    "\n",
    "# def evaluate_xgb(X, y):\n",
    "#     kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "#     rmse_scores = []\n",
    "#     mae_scores = []\n",
    "    \n",
    "#     for train_index, val_index in kf.split(X):\n",
    "#         X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[val_index]\n",
    "#         y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "#         model = xgb.XGBRegressor(n_estimators=1000)\n",
    "#         model.fit(X_train_fold, y_train_fold)  # Simple fit without early stopping\n",
    "        \n",
    "#         y_pred = model.predict(X_val_fold)\n",
    "#         rmse = mean_squared_error(y_val_fold, y_pred, squared=False)\n",
    "#         mae = mean_absolute_error(y_val_fold, y_pred)\n",
    "#         rmse_scores.append(rmse)\n",
    "#         mae_scores.append(mae)\n",
    "    \n",
    "#     return (np.mean(rmse_scores), np.std(rmse_scores), \n",
    "#             np.mean(mae_scores), np.std(mae_scores))\n",
    "\n",
    "# def evaluate_gb(X, y):\n",
    "#     kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "#     rmse_scores = []\n",
    "#     mae_scores = []\n",
    "    \n",
    "#     for train_index, val_index in kf.split(X):\n",
    "#         X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[val_index]\n",
    "#         y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "#         model = GradientBoostingRegressor(n_estimators=100)\n",
    "#         model.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "#         y_pred = model.predict(X_val_fold)\n",
    "#         rmse = mean_squared_error(y_val_fold, y_pred, squared=False)\n",
    "#         mae = mean_absolute_error(y_val_fold, y_pred)\n",
    "#         rmse_scores.append(rmse)\n",
    "#         mae_scores.append(mae)\n",
    "    \n",
    "#     return (np.mean(rmse_scores), np.std(rmse_scores), \n",
    "#             np.mean(mae_scores), np.std(mae_scores))\n",
    "\n",
    "# # Evaluate models\n",
    "# print(\"Evaluating LightGBM...\")\n",
    "# lgb_rmse_mean, lgb_rmse_std, lgb_mae_mean, lgb_mae_std = evaluate_lgb(X_train, y_train)\n",
    "# print(f'LightGBM: RMSE = {lgb_rmse_mean:.4f} (±{lgb_rmse_std:.4f})')\n",
    "# print(f'LightGBM: MAE = {lgb_mae_mean:.4f} (±{lgb_mae_std:.4f})')\n",
    "\n",
    "# print(\"\\nEvaluating XGBoost...\")\n",
    "# xgb_rmse_mean, xgb_rmse_std, xgb_mae_mean, xgb_mae_std = evaluate_xgb(X_train, y_train)\n",
    "# print(f'XGBoost: RMSE = {xgb_rmse_mean:.4f} (±{xgb_rmse_std:.4f})')\n",
    "# print(f'XGBoost: MAE = {xgb_mae_mean:.4f} (±{xgb_mae_std:.4f})')\n",
    "\n",
    "# print(\"\\nEvaluating Gradient Boosting...\")\n",
    "# gb_rmse_mean, gb_rmse_std, gb_mae_mean, gb_mae_std = evaluate_gb(X_train, y_train)\n",
    "# print(f'Gradient Boosting: RMSE = {gb_rmse_mean:.4f} (±{gb_rmse_std:.4f})')\n",
    "# print(f'Gradient Boosting: MAE = {gb_mae_mean:.4f} (±{gb_mae_std:.4f})')\n",
    "\n",
    "# # Find the best model based on mean RMSE\n",
    "# models_scores = {\n",
    "#     'LightGBM': lgb_rmse_mean,\n",
    "#     'XGBoost': xgb_rmse_mean,\n",
    "#     'Gradient Boosting': gb_rmse_mean\n",
    "# }\n",
    "\n",
    "# best_model_name = min(models_scores, key=models_scores.get)\n",
    "# print(f\"\\nBest model is {best_model_name} with RMSE: {models_scores[best_model_name]:.4f}\")\n",
    "\n",
    "# # Train the best model and make predictions\n",
    "# if best_model_name == 'LightGBM':\n",
    "#     final_model = lgb.LGBMRegressor(n_estimators=1000)\n",
    "#     final_model.fit(X_train, y_train,\n",
    "#                    eval_set=[(X_train.iloc[:100], y_train.iloc[:100])],\n",
    "#                    callbacks=[lgb.early_stopping(50)])\n",
    "# elif best_model_name == 'XGBoost':\n",
    "#     final_model = xgb.XGBRegressor(n_estimators=1000)\n",
    "#     final_model.fit(X_train, y_train)\n",
    "# else:\n",
    "#     final_model = GradientBoostingRegressor(n_estimators=100)\n",
    "#     final_model.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions on test set\n",
    "# predictions = final_model.predict(X_test)\n",
    "\n",
    "# # Create submission file\n",
    "# submission = pd.DataFrame({\n",
    "#     'id': test_data['id'],\n",
    "#     'predicted_yield': predictions\n",
    "# })\n",
    "# submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "# import lightgbm as lgb\n",
    "# import xgboost as xgb\n",
    "# from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# # Load the data\n",
    "# train_data = pd.read_csv('train.csv')\n",
    "# test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# # Separate features and target variable from training data\n",
    "# X = train_data.drop(columns=['id', 'Row#', 'yield'])\n",
    "# y = train_data['yield']\n",
    "# X_test = test_data.drop(columns=['id', 'Row#'])\n",
    "\n",
    "# # Split the data into training and validation sets (80% train, 20% validation)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Define separate evaluation functions for each model type\n",
    "# def evaluate_lgb(X_train, y_train, X_val, y_val):\n",
    "#     model = lgb.LGBMRegressor(n_estimators=1000)\n",
    "#     model.fit(X_train, y_train,\n",
    "#               eval_set=[(X_val, y_val)],\n",
    "#               callbacks=[lgb.early_stopping(50)])\n",
    "    \n",
    "#     y_pred = model.predict(X_val)\n",
    "#     rmse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "#     mae = mean_absolute_error(y_val, y_pred)\n",
    "    \n",
    "#     return rmse, mae\n",
    "\n",
    "# def evaluate_xgb(X_train, y_train, X_val, y_val):\n",
    "#     model = xgb.XGBRegressor(n_estimators=1000)\n",
    "#     eval_set = [(X_val, y_val)]\n",
    "#     model.fit(X_train, y_train,\n",
    "#               eval_set=eval_set,\n",
    "#               verbose=False)\n",
    "    \n",
    "#     y_pred = model.predict(X_val)\n",
    "#     rmse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "#     mae = mean_absolute_error(y_val, y_pred)\n",
    "    \n",
    "#     return rmse, mae\n",
    "\n",
    "# def evaluate_gb(X_train, y_train, X_val, y_val):\n",
    "#     model = GradientBoostingRegressor(n_estimators=100, validation_fraction=0.1, n_iter_no_change=50)\n",
    "#     model.fit(X_train, y_train)\n",
    "    \n",
    "#     y_pred = model.predict(X_val)\n",
    "#     rmse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "#     mae = mean_absolute_error(y_val, y_pred)\n",
    "    \n",
    "#     return rmse, mae\n",
    "\n",
    "# # Evaluate models\n",
    "# print(\"Evaluating LightGBM...\")\n",
    "# lgb_rmse, lgb_mae = evaluate_lgb(X_train, y_train, X_val, y_val)\n",
    "# print(f'LightGBM: RMSE = {lgb_rmse:.4f}')\n",
    "# print(f'LightGBM: MAE = {lgb_mae:.4f}')\n",
    "\n",
    "# print(\"\\nEvaluating XGBoost...\")\n",
    "# xgb_rmse, xgb_mae = evaluate_xgb(X_train, y_train, X_val, y_val)\n",
    "# print(f'XGBoost: RMSE = {xgb_rmse:.4f}')\n",
    "# print(f'XGBoost: MAE = {xgb_mae:.4f}')\n",
    "\n",
    "# print(\"\\nEvaluating Gradient Boosting...\")\n",
    "# gb_rmse, gb_mae = evaluate_gb(X_train, y_train, X_val, y_val)\n",
    "# print(f'Gradient Boosting: RMSE = {gb_rmse:.4f}')\n",
    "# print(f'Gradient Boosting: MAE = {gb_mae:.4f}')\n",
    "\n",
    "# # Find the best model based on mean RMSE\n",
    "# models_scores = {\n",
    "#     'LightGBM': lgb_rmse,\n",
    "#     'XGBoost': xgb_rmse,\n",
    "#     'Gradient Boosting': gb_rmse\n",
    "# }\n",
    "\n",
    "# best_model_name = min(models_scores, key=models_scores.get)\n",
    "# print(f\"\\nBest model is {best_model_name} with RMSE: {models_scores[best_model_name]:.4f}\")\n",
    "\n",
    "# # Train the best model and make predictions\n",
    "# if best_model_name == 'LightGBM':\n",
    "#     final_model = lgb.LGBMRegressor(n_estimators=1000)\n",
    "#     final_model.fit(X, y,\n",
    "#                    eval_set=[(X_val, y_val)],\n",
    "#                    callbacks=[lgb.early_stopping(50)])\n",
    "# elif best_model_name == 'XGBoost':\n",
    "#     final_model = xgb.XGBRegressor(n_estimators=1000)\n",
    "#     eval_set = [(X_val, y_val)]\n",
    "#     final_model.fit(X, y,\n",
    "#                    eval_set=eval_set,\n",
    "#                    verbose=False)\n",
    "# else:\n",
    "#     final_model = GradientBoostingRegressor(n_estimators=100, validation_fraction=0.1, n_iter_no_change=50)\n",
    "#     final_model.fit(X, y)\n",
    "\n",
    "# # Make predictions on test set\n",
    "# predictions = final_model.predict(X_test)\n",
    "\n",
    "# # Create submission file\n",
    "# submission = pd.DataFrame({\n",
    "#     'id': test_data['id'],\n",
    "#     'predicted_yield': predictions\n",
    "# })\n",
    "# submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import lightgbm as lgb\n",
    "# from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "# from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import numpy as np\n",
    "\n",
    "# # Load train and test datasets\n",
    "# train = pd.read_csv('train.csv')\n",
    "# test = pd.read_csv('test.csv')\n",
    "\n",
    "# # Prepare the data (Feature Scaling)\n",
    "# X = train.drop(columns=['id', 'Row#', 'yield'])  # Features for training\n",
    "# y = train['yield']  # Target variable\n",
    "\n",
    "# # Standardize features for better performance\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# # Convert test data features (excluding 'id' and 'Row#')\n",
    "# X_test = test.drop(columns=['id', 'Row#'])\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# # Split the data into training and validation sets\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Define a parameter grid for GridSearchCV\n",
    "# param_grid = {\n",
    "#     'learning_rate': [0.01, 0.05, 0.1],\n",
    "#     'num_leaves': [31, 50, 70],\n",
    "#     'min_data_in_leaf': [20, 40, 60],\n",
    "#     'feature_fraction': [0.7, 0.8, 0.9],\n",
    "#     'bagging_fraction': [0.7, 0.8, 0.9],\n",
    "#     'lambda_l1': [0.0, 0.1, 0.5],\n",
    "#     'lambda_l2': [0.0, 0.1, 0.5]\n",
    "# }\n",
    "\n",
    "# # Initialize the LightGBM estimator\n",
    "# lgb_estimator = lgb.LGBMRegressor(boosting_type='gbdt', objective='regression', n_estimators=1000)\n",
    "\n",
    "# # Perform GridSearchCV without early stopping\n",
    "# grid_search = GridSearchCV(estimator=lgb_estimator, param_grid=param_grid, cv=5, scoring='neg_mean_absolute_error', verbose=1)\n",
    "\n",
    "# # Fit the Grid Search (no early stopping here)\n",
    "# grid_search.fit(X_scaled, y)\n",
    "\n",
    "# # Best parameters from grid search\n",
    "# best_params = grid_search.best_params_\n",
    "# print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "# # Retrain the model using the best parameters and apply early stopping\n",
    "# best_model = lgb.LGBMRegressor(**best_params, n_estimators=1000)\n",
    "\n",
    "# best_model.fit(X_train, y_train,\n",
    "#                eval_set=[(X_valid, y_valid)],\n",
    "#                early_stopping_rounds=100,\n",
    "#                verbose=100)\n",
    "\n",
    "# # Cross-validation with the best parameters\n",
    "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# mae_scores = []\n",
    "# rmse_scores = []\n",
    "\n",
    "# for train_index, val_index in kf.split(X_scaled):\n",
    "#     X_train_fold, X_val_fold = X_scaled[train_index], X_scaled[val_index]\n",
    "#     y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "#     # Create LightGBM model with early stopping in each fold\n",
    "#     model_fold = lgb.LGBMRegressor(**best_params, n_estimators=1000)\n",
    "#     model_fold.fit(X_train_fold, y_train_fold, \n",
    "#                    eval_set=[(X_val_fold, y_val_fold)], \n",
    "#                    early_stopping_rounds=100, \n",
    "#                    verbose=0)  # Suppress verbose logging during cross-validation\n",
    "    \n",
    "#     # Predict and evaluate\n",
    "#     y_pred_fold = model_fold.predict(X_val_fold, num_iteration=model_fold.best_iteration_)\n",
    "    \n",
    "#     mae = mean_absolute_error(y_val_fold, y_pred_fold)\n",
    "#     rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred_fold))\n",
    "    \n",
    "#     mae_scores.append(mae)\n",
    "#     rmse_scores.append(rmse)\n",
    "\n",
    "# # Print MAE and RMSE scores\n",
    "# print(f\"Mean MAE: {np.mean(mae_scores):.4f}\")\n",
    "# print(f\"Mean RMSE: {np.mean(rmse_scores):.4f}\")\n",
    "\n",
    "# # Final model prediction on the test set\n",
    "# y_test_pred = best_model.predict(X_test_scaled, num_iteration=best_model.best_iteration_)\n",
    "\n",
    "# # Save predictions in a CSV file\n",
    "# output_df = pd.DataFrame({\n",
    "#     'id': test['id'],  # Use the 'id' column from test data\n",
    "#     'yield': y_test_pred  # Predicted 'yield'\n",
    "# })\n",
    "# output_df.to_csv('yield_predictions.csv', index=False)\n",
    "\n",
    "# print(\"Predictions saved to 'yield_predictions.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
