{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **_Beautify Code_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style type='text/css'>\n",
    ".Í¼o .cm-scroller {\n",
    "    font-family: 'Monaspace Neon';\n",
    "    font-size: 16px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **_Mark 1_**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **_Input File Format_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We have this type of datasets train.csv\n",
    "# id,CustomerId,Surname,CreditScore,Geography,Gender,Age,Tenure,Balance,NumOfProducts,HasCrCard,IsActiveMember,EstimatedSalary,Exited\n",
    "# 0,15570087.0,Chigozie,571.0,Germany,Female,42.0,4.0,127290.61,1.0,1.0,1.0,25669.1,0.0\n",
    "# 1,15809837.0,Chubb,558.0,France,Male,38.0,2.0,0.0,1.0,1.0,1.0,138849.06,0.0\n",
    "# 2,15766776.0,Ch'ien,644.0,France,Female,44.0,3.0,0.0,1.0,1.0,0.0,121408.46,1.0\n",
    "# 3,15649536.0,Chikelu,714.0,France,Female,27.0,6.0,0.0,2.0,1.0,0.0,121151.1,0.0\n",
    "# 4,15637644.0,Cremonesi,676.0,Spain,Female,33.0,8.0,0.0,2.0,0.0,1.0,170392.59,0.0\n",
    "# 5,15662908.0,Y?,561.0,France,Male,30.0,1.0,0.0,2.0,1.0,0.0,44335.54,0.0\n",
    "# 6,15773852.0,Oluchukwu,645.0,France,Female,41.0,1.0,0.0,1.0,1.0,0.0,81452.29,1.0\n",
    "# 7,15747534.0,Chinomso,607.0,France,Female,34.0,10.0,0.0,2.0,1.0,0.0,111342.66,0.0\n",
    "# 8,15757895.0,DeRose,687.0,France,Female,35.0,5.0,99610.92,1.0,1.0,1.0,107815.31,0.0\n",
    "# 9,15592578.0,Walker,642.0,Germany,Female,42.0,3.0,104015.54,1.0,1.0,0.0,159334.93,1.0\n",
    "# 10,15783629.0,Rizzo,678.0,France,Male,38.0,9.0,0.0,2.0,0.0,0.0,179631.85,0.0\n",
    "\n",
    "# # We have this type of datasets test.csv\n",
    "# id,CustomerId,Surname,CreditScore,Geography,Gender,Age,Tenure,Balance,NumOfProducts,HasCrCard,IsActiveMember,EstimatedSalary\n",
    "# 15000,15767954.0,Smith,701.0,France,Female,33.0,10.0,0.0,2.0,1.0,0.0,62402.38\n",
    "# 15001,15641110.0,Ch'ang,757.0,France,Male,32.0,10.0,104469.58,1.0,1.0,1.0,63795.8\n",
    "# 15002,15589496.0,Chukwujekwu,613.0,France,Male,34.0,4.0,0.0,2.0,1.0,0.0,136983.77\n",
    "# 15003,15777892.0,Nkemjika,684.0,France,Female,41.0,8.0,0.0,2.0,1.0,1.0,147090.9\n",
    "# 15004,15652914.0,Lucciano,648.0,Spain,Male,38.0,2.0,0.0,2.0,1.0,1.0,54495.82\n",
    "# 15005,15632576.0,Chidiegwu,663.0,Germany,Female,44.0,1.0,127847.86,1.0,1.0,0.0,103726.71\n",
    "# 15006,15734999.0,Chukwudi,624.0,France,Female,51.0,8.0,0.0,3.0,1.0,0.0,187985.85\n",
    "# 15007,15813916.0,Chukwufumnanya,627.0,France,Male,31.0,5.0,0.0,2.0,1.0,1.0,161465.31\n",
    "# 15008,15681180.0,Chiemenam,464.0,Germany,Female,43.0,0.0,124576.65,1.0,1.0,0.0,80190.36\n",
    "# 15009,15596713.0,Chiang,718.0,France,Male,36.0,2.0,0.0,2.0,1.0,0.0,162643.15\n",
    "# 15010,15773971.0,T'ien,686.0,France,Male,35.0,2.0,0.0,2.0,0.0,1.0,110114.19\n",
    "# 15011,15780142.0,Mazzi,641.0,Germany,Female,51.0,1.0,102827.44,1.0,1.0,1.0,159418.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **DataLoader: Load data and Preprocessing it**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **_Commented Code_**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **_Actual Code_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderPipeline:\n",
    "    def __init__(self, train_path, test_path):\n",
    "        self.train_path = train_path\n",
    "        self.test_path = test_path\n",
    "\n",
    "    def load_data(self):\n",
    "        train_data = pd.read_csv(self.train_path, index_col=0)\n",
    "        test_data = pd.read_csv(self.test_path, index_col=0)\n",
    "        return train_data, test_data\n",
    "    \n",
    "    def label_encode_data(self, train_data, test_data):\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        categorical_columns = train_data.select_dtypes(include=['object']).columns\n",
    "        label_encoders = {}\n",
    "        for col in categorical_columns:\n",
    "            label_encoders[col] = LabelEncoder()\n",
    "            train_data[col] = label_encoders[col].fit_transform(train_data[col])\n",
    "\n",
    "        categorical_columns = test_data.select_dtypes(include=['object']).columns\n",
    "        label_encoders = {}\n",
    "        for col in categorical_columns:\n",
    "            label_encoders[col] = LabelEncoder()\n",
    "            test_data[col] = label_encoders[col].fit_transform(test_data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class OutlierPipeline:\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def detect_outliers_zscore(self, column, threshold=3):\n",
    "        \"\"\"Detect outliers using Z-score method.\"\"\"\n",
    "        z_scores = (self.dataset[column] - self.dataset[column].mean()) / self.dataset[column].std()\n",
    "        return self.dataset[np.abs(z_scores) > threshold]\n",
    "\n",
    "    def detect_outliers_iqr(self, column):\n",
    "        \"\"\"Detect outliers using IQR method.\"\"\"\n",
    "        Q1 = self.dataset[column].quantile(0.25)\n",
    "        Q3 = self.dataset[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        return self.dataset[(self.dataset[column] < lower_bound) | (self.dataset[column] > upper_bound)]\n",
    "\n",
    "    def remove_outliers(self, outlier_indices):\n",
    "        \"\"\"Remove outliers from the dataset.\"\"\"\n",
    "        return self.dataset[~self.dataset.index.isin(outlier_indices)]\n",
    "\n",
    "    def detect_outliers_mad(self, column):\n",
    "        \"\"\"Detect outliers using MAD (Median Absolute Deviation) method.\"\"\"\n",
    "        median = self.dataset[column].median()\n",
    "        mad = np.median(np.abs(self.dataset[column] - median))\n",
    "        threshold = 3  # You can adjust this threshold\n",
    "        return self.dataset[np.abs(self.dataset[column] - median) > threshold * mad]\n",
    "\n",
    "    def detect_outliers_percentile(self, column, lower_percentile=1, upper_percentile=99):\n",
    "        \"\"\"Detect outliers based on percentiles.\"\"\"\n",
    "        lower_bound = self.dataset[column].quantile(lower_percentile / 100)\n",
    "        upper_bound = self.dataset[column].quantile(upper_percentile / 100)\n",
    "        return self.dataset[(self.dataset[column] < lower_bound) | (self.dataset[column] > upper_bound)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cross Validation: For Training Model**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **_Commented Code_**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **_Actual Code_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import (\n",
    "    KFold, StratifiedKFold, LeaveOneOut, TimeSeriesSplit,\n",
    "    RepeatedKFold, RepeatedStratifiedKFold, GroupKFold,\n",
    "    ShuffleSplit, StratifiedShuffleSplit\n",
    ")\n",
    "\n",
    "class CrossValidator:\n",
    "    def __init__(self, n_splits=5, shuffle=False, random_state=None):\n",
    "        self.n_splits = n_splits\n",
    "        self.shuffle = shuffle\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def k_fold(self, X, y):\n",
    "        \"\"\"K-Fold Cross-Validation.\"\"\"\n",
    "        kf = KFold(n_splits=self.n_splits, shuffle=self.shuffle, random_state=self.random_state if self.shuffle else None)\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            yield X.iloc[train_index], X.iloc[test_index], y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    def stratified_k_fold(self, X, y):\n",
    "        \"\"\"Stratified K-Fold Cross-Validation.\"\"\"\n",
    "        skf = StratifiedKFold(n_splits=self.n_splits, shuffle=self.shuffle, random_state=self.random_state if self.shuffle else None)\n",
    "        for train_index, test_index in skf.split(X, y):\n",
    "            yield X.iloc[train_index], X.iloc[test_index], y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    def leave_one_out(self, X, y):\n",
    "        \"\"\"Leave-One-Out Cross-Validation.\"\"\"\n",
    "        loo = LeaveOneOut()\n",
    "        for train_index, test_index in loo.split(X):\n",
    "            yield X.iloc[train_index], X.iloc[test_index], y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    def time_series_split(self, X, y):\n",
    "        \"\"\"Time Series Cross-Validation.\"\"\"\n",
    "        tscv = TimeSeriesSplit(n_splits=self.n_splits)\n",
    "        for train_index, test_index in tscv.split(X):\n",
    "            yield X.iloc[train_index], X.iloc[test_index], y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    def repeated_k_fold(self, X, y, n_repeats=2):\n",
    "        \"\"\"Repeated K-Fold Cross-Validation.\"\"\"\n",
    "        rkf = RepeatedKFold(n_splits=self.n_splits, n_repeats=n_repeats, random_state=self.random_state)\n",
    "        for train_index, test_index in rkf.split(X):\n",
    "            yield X.iloc[train_index], X.iloc[test_index], y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    def repeated_stratified_k_fold(self, X, y, n_repeats=2):\n",
    "        \"\"\"Repeated Stratified K-Fold Cross-Validation.\"\"\"\n",
    "        rskf = RepeatedStratifiedKFold(n_splits=self.n_splits, n_repeats=n_repeats, random_state=self.random_state)\n",
    "        for train_index, test_index in rskf.split(X, y):\n",
    "            yield X.iloc[train_index], X.iloc[test_index], y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    def group_k_fold(self, X, y, groups):\n",
    "        \"\"\"Group K-Fold Cross-Validation.\"\"\"\n",
    "        gkf = GroupKFold(n_splits=self.n_splits)\n",
    "        for train_index, test_index in gkf.split(X, y, groups):\n",
    "            yield X.iloc[train_index], X.iloc[test_index], y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    def shuffle_split(self, X, y):\n",
    "        \"\"\"Shuffle Split Cross-Validation.\"\"\"\n",
    "        ss = ShuffleSplit(n_splits=self.n_splits, test_size=0.2, random_state=self.random_state)\n",
    "        for train_index, test_index in ss.split(X):\n",
    "            yield X.iloc[train_index], X.iloc[test_index], y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    def stratified_shuffle_split(self, X, y):\n",
    "        \"\"\"Stratified Shuffle Split Cross-Validation.\"\"\"\n",
    "        sss = StratifiedShuffleSplit(n_splits=self .n_splits, test_size=0.2, random_state=self.random_state)\n",
    "        for train_index, test_index in sss.split(X, y):\n",
    "            yield X.iloc[train_index], X.iloc[test_index], y.iloc[train_index], y.iloc[test_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Classifier Model: Defining Different Models here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **_Commented Code_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Model:\n",
    "#     def __init__(self, model_type):\n",
    "#         self.model_type = model_type\n",
    "#         self.model = None\n",
    "    \n",
    "#     def train_model(self, model, X_train, y_train):\n",
    "#         self.model = model\n",
    "#         self.model.fit(X_train, y_train)\n",
    "    \n",
    "#     def predict(self, X_test):\n",
    "#         \"\"\" Make predictions using the trained model; Also create csv file for huge (large) number of predictions datasets \"\"\"\n",
    "#         import pandas as pd\n",
    "#         y_pred = self.model.predict(X_test)\n",
    "#         # output_df = pd.DataFrame({\n",
    "#         #     'id': X_test.index,\n",
    "#         #     'yield': y_pred\n",
    "#         # })\n",
    "\n",
    "#         # output_df.to_csv('Predictions.csv', index=False)\n",
    "#         return y_pred\n",
    "    \n",
    "#     def evaluate(self, y_true, y_pred):\n",
    "#         from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "#         accuracy = accuracy_score(y_test, y_pred)\n",
    "#         precision = precision_score(y_test, y_pred, average='weighted')\n",
    "#         recall = recall_score(y_test, y_pred, average='weighted')\n",
    "#         f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "#         mse = mean_squared_error(y_test, y_pred)\n",
    "#         mae = mean_absolute_error(y_test, y_pred)\n",
    "#         r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "#         print(f\"Accuracy: {accuracy}\")\n",
    "#         print(f\"Precision: {precision}\")\n",
    "#         print(f\"Recall: {recall}\")\n",
    "#         print(f\"F1: {f1}\")\n",
    "#         print(f\"MSE: {mse}\")\n",
    "#         print(f\"MAE: {mae}\")\n",
    "#         print(f\"R-squared (R2): {r2}\")\n",
    "    \n",
    "#     def save_model(self, model, model_path):\n",
    "#         import joblib\n",
    "#         joblib.dump(model, model_path)\n",
    "\n",
    "#     def load_model(self, model_path):\n",
    "#         import joblib\n",
    "#         return joblib.load(model_path)\n",
    "    \n",
    "\n",
    "# class ClassifierModel(Model):\n",
    "#     def __init__(self):\n",
    "#         super().__init__(model_type=None)\n",
    "        \n",
    "\n",
    "#     def select_classifier(self, model_type):\n",
    "#         if model_type == \"GradientBoostingClassifier\":\n",
    "#             return self.gradients_boosting\n",
    "#         elif model_type == \"RandomForestClassifier\":\n",
    "#             return self.random_forest\n",
    "#         elif model_type == \"XGBClassifier\":\n",
    "#             return self.xgboost\n",
    "#         elif model_type == \"LogisticRegression\":\n",
    "#             return self.logistic_regression\n",
    "#         elif model_type == \"DecisionTreeClassifier\":\n",
    "#             return self.decision_tree\n",
    "#         elif model_type == \"LGBMClassifier\":\n",
    "#             return self.lightgbm\n",
    "#         elif model_type == \"CatBoostClassifier\":\n",
    "#             return self.catboost\n",
    "#         elif model_type == \"KNeighborsClassifier\":\n",
    "#             return self.k_nearest_neighbors\n",
    "#         elif model_type == \"SVC\":\n",
    "#             return self.support_vector_machines\n",
    "#         elif model_type == \"NaiveBayes\":\n",
    "#             return self.naive_bayes\n",
    "#         else:\n",
    "#             raise ValueError(f\"Invalid model type: {model_type}\")\n",
    "#     def gradients_boosting(self, X_train, y_train):\n",
    "#         from sklearn.ensemble import GradientBoostingClassifier\n",
    "#         model = GradientBoostingClassifier()\n",
    "#         self.train_model(model, X_train, y_train)\n",
    "#         return model\n",
    "    \n",
    "#     def random_forest(self, X_train, y_train):\n",
    "#         from sklearn.ensemble import RandomForestClassifier\n",
    "#         model = RandomForestClassifier()\n",
    "#         self.train_model(model, X_train, y_train)\n",
    "#         return model\n",
    "    \n",
    "#     def xgboost(self, X_train, y_train):\n",
    "#         from xgboost import XGBClassifier\n",
    "#         model = XGBClassifier()\n",
    "#         self.train_model(model, X_train, y_train)\n",
    "#         return model\n",
    "    \n",
    "#     def lightgbm(self, X_train, y_train):\n",
    "#         from lightgbm import LGBMClassifier\n",
    "#         model = LGBMClassifier()\n",
    "#         self.train_model(model, X_train, y_train)\n",
    "#         return model\n",
    "    \n",
    "#     def catboost(self, X_train, y_train):\n",
    "#         from catboost import CatBoostClassifier\n",
    "#         model = CatBoostClassifier()\n",
    "#         self.train_model(model, X_train, y_train)\n",
    "#         return model\n",
    "    \n",
    "#     def logistic_regression(self, X_train, y_train):\n",
    "#         from sklearn.linear_model import LogisticRegression\n",
    "#         model = LogisticRegression()\n",
    "#         self.train_model(model, X_train, y_train)\n",
    "#         return model\n",
    "    \n",
    "#     def decision_tree(self, X_train, y_train):\n",
    "#         from sklearn.tree import DecisionTreeClassifier\n",
    "#         model = DecisionTreeClassifier()\n",
    "#         self.train_model(model, X_train, y_train)\n",
    "#         return model\n",
    "    \n",
    "#     def k_nearest_neighbors(self, X_train, y_train):\n",
    "#         from sklearn.neighbors import KNeighborsClassifier\n",
    "#         model = KNeighborsClassifier()\n",
    "#         self.train_model(model, X_train, y_train)\n",
    "#         return model\n",
    "    \n",
    "#     def support_vector_machines(self, X_train, y_train):\n",
    "#         from sklearn.svm import SVC\n",
    "#         model = SVC()\n",
    "#         self.train_model(model, X_train, y_train)\n",
    "#         return model\n",
    "    \n",
    "#     def naive_bayes(self, X_train, y_train):\n",
    "#         from sklearn.naive_bayes import GaussianNB\n",
    "#         model = GaussianNB()    \n",
    "#         self.train_model(model, X_train, y_train)\n",
    "#         return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import numpy as np\n",
    "# from typing import Dict, Any, Optional, Union\n",
    "# from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "# from scipy.stats import uniform, randint, loguniform\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# class EnhancedClassifierModel(ClassifierModel):\n",
    "#     \"\"\"\n",
    "#     Enhanced classifier model with hyperparameter optimization capabilities.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, json_file_path: str):\n",
    "#         \"\"\"\n",
    "#         Initialize the enhanced classifier model.\n",
    "        \n",
    "#         Args:\n",
    "#             json_file_path: Path to the JSON file containing hyperparameter configurations\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "#         self.json_file_path = json_file_path\n",
    "#         self.hyperparameters = self.load_from_json()\n",
    "        \n",
    "#     def load_from_json(self) -> Dict[str, Any]:\n",
    "#         \"\"\"\n",
    "#         Load hyperparameter configurations from JSON file.\n",
    "        \n",
    "#         Returns:\n",
    "#             Dictionary containing hyperparameter configurations\n",
    "#         \"\"\"\n",
    "#         with open(self.json_file_path, 'r') as file:\n",
    "#             data = json.load(file)\n",
    "#         return data\n",
    "    \n",
    "#     def _parse_distribution(self, param_value: str) -> Any:\n",
    "#         \"\"\"\n",
    "#         Parse distribution strings from JSON into scipy.stats distributions.\n",
    "        \n",
    "#         Args:\n",
    "#             param_value: String representation of the distribution\n",
    "            \n",
    "#         Returns:\n",
    "#             Scipy stats distribution or original value\n",
    "#         \"\"\"\n",
    "#         if isinstance(param_value, str):\n",
    "#             if param_value.startswith('randint'):\n",
    "#                 low, high = map(int, param_value.strip('randint()').split(','))\n",
    "#                 return randint(low, high)\n",
    "#             elif param_value.startswith('uniform'):\n",
    "#                 low, high = map(float, param_value.strip('uniform()').split(','))\n",
    "#                 return uniform(low, high)\n",
    "#             elif param_value.startswith('loguniform'):\n",
    "#                 low, high = map(float, param_value.strip('loguniform()').split(','))\n",
    "#                 return loguniform(low, high)\n",
    "#         return param_value\n",
    "\n",
    "#     def _prepare_param_grid(self, model_type: str, is_random: bool = False) -> Dict[str, Any]:\n",
    "#         \"\"\"\n",
    "#         Prepare parameter grid for hyperparameter search.\n",
    "        \n",
    "#         Args:\n",
    "#             model_type: Type of the model\n",
    "#             is_random: Whether to prepare for random search\n",
    "            \n",
    "#         Returns:\n",
    "#             Parameter grid dictionary\n",
    "#         \"\"\"\n",
    "#         if model_type not in self.hyperparameters:\n",
    "#             raise ValueError(f\"Model type {model_type} not found in hyperparameter configurations\")\n",
    "            \n",
    "#         param_grid = {}\n",
    "#         model_params = self.hyperparameters[model_type]\n",
    "        \n",
    "#         for param_name, param_value in model_params.items():\n",
    "#             if is_random:\n",
    "#                 param_grid[param_name] = self._parse_distribution(param_value)\n",
    "#             else:\n",
    "#                 if isinstance(param_value, list):\n",
    "#                     param_grid[param_name] = param_value\n",
    "#                 else:\n",
    "#                     # For grid search, create a reasonable set of values if distribution is specified\n",
    "#                     try:\n",
    "#                         dist = self._parse_distribution(param_value)\n",
    "#                         if isinstance(dist, randint):\n",
    "#                             param_grid[param_name] = list(range(dist.low, dist.high, (dist.high - dist.low) // 5))\n",
    "#                         elif isinstance(dist, (uniform, loguniform)):\n",
    "#                             if isinstance(dist, loguniform):\n",
    "#                                 values = np.logspace(np.log10(dist.a), np.log10(dist.b), 5)\n",
    "#                             else:\n",
    "#                                 values = np.linspace(dist.loc, dist.loc + dist.scale, 5)\n",
    "#                             param_grid[param_name] = list(values)\n",
    "#                     except:\n",
    "#                         param_grid[param_name] = param_value\n",
    "                        \n",
    "#         return param_grid\n",
    "\n",
    "#     def randomized_search_hyper_parameters(\n",
    "#         self,\n",
    "#         model_type: str,\n",
    "#         X_train: np.ndarray,\n",
    "#         y_train: np.ndarray,\n",
    "#         n_iter: int = 100,\n",
    "#         cv: int = 5,\n",
    "#         scoring: str = 'accuracy',\n",
    "#         n_jobs: int = -1\n",
    "#     ) -> RandomizedSearchCV:\n",
    "#         \"\"\"\n",
    "#         Perform randomized search for hyperparameter optimization.\n",
    "        \n",
    "#         Args:\n",
    "#             model_type: Type of the model to optimize\n",
    "#             X_train: Training features\n",
    "#             y_train: Training labels\n",
    "#             n_iter: Number of parameter settings sampled\n",
    "#             cv: Number of cross-validation folds\n",
    "#             scoring: Scoring metric to use\n",
    "#             n_jobs: Number of jobs to run in parallel\n",
    "            \n",
    "#         Returns:\n",
    "#             Fitted RandomizedSearchCV object\n",
    "#         \"\"\"\n",
    "#         # Get the base model\n",
    "#         base_model = self.select_classifier(model_type)(X_train, y_train).model\n",
    "        \n",
    "#         # Prepare parameter distributions for random search\n",
    "#         param_distributions = self._prepare_param_grid(model_type, is_random=True)\n",
    "        \n",
    "#         # Create and fit RandomizedSearchCV\n",
    "#         random_search = RandomizedSearchCV(\n",
    "#             estimator=base_model,\n",
    "#             param_distributions=param_distributions,\n",
    "#             n_iter=n_iter,\n",
    "#             cv=cv,\n",
    "#             scoring=scoring,\n",
    "#             n_jobs=n_jobs,\n",
    "#             random_state=42,\n",
    "#             verbose=1\n",
    "#         )\n",
    "        \n",
    "#         random_search.fit(X_train, y_train)\n",
    "        \n",
    "#         # Print results\n",
    "#         print(f\"\\nBest parameters found: {random_search.best_params_}\")\n",
    "#         print(f\"Best cross-validation score: {random_search.best_score_:.4f}\")\n",
    "        \n",
    "#         return random_search\n",
    "\n",
    "#     def grid_search_hyper_parameters(\n",
    "#         self,\n",
    "#         model_type: str,\n",
    "#         X_train: np.ndarray,\n",
    "#         y_train: np.ndarray,\n",
    "#         cv: int = 5,\n",
    "#         scoring: str = 'accuracy',\n",
    "#         n_jobs: int = -1\n",
    "#     ) -> GridSearchCV:\n",
    "#         \"\"\"\n",
    "#         Perform grid search for hyperparameter optimization.\n",
    "        \n",
    "#         Args:\n",
    "#             model_type: Type of the model to optimize\n",
    "#             X_train: Training features\n",
    "#             y_train: Training labels\n",
    "#             cv: Number of cross-validation folds\n",
    "#             scoring: Scoring metric to use\n",
    "#             n_jobs: Number of jobs to run in parallel\n",
    "            \n",
    "#         Returns:\n",
    "#             Fitted GridSearchCV object\n",
    "#         \"\"\"\n",
    "#         # Get the base model\n",
    "#         base_model = self.select_classifier(model_type)(X_train, y_train).model\n",
    "        \n",
    "#         # Prepare parameter grid\n",
    "#         param_grid = self._prepare_param_grid(model_type, is_random=False)\n",
    "        \n",
    "#         # Create and fit GridSearchCV\n",
    "#         grid_search = GridSearchCV(\n",
    "#             estimator=base_model,\n",
    "#             param_grid=param_grid,\n",
    "#             cv=cv,\n",
    "#             scoring=scoring,\n",
    "#             n_jobs=n_jobs,\n",
    "#             verbose=1\n",
    "#         )\n",
    "        \n",
    "#         grid_search.fit(X_train, y_train)\n",
    "        \n",
    "#         # Print results\n",
    "#         print(f\"\\nBest parameters found: {grid_search.best_params_}\")\n",
    "#         print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "        \n",
    "#         return grid_search\n",
    "\n",
    "\n",
    "# class ModelOptimizer:\n",
    "#     \"\"\"\n",
    "#     Class for optimizing model training process.\n",
    "#     \"\"\"\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         model: Any,\n",
    "#         optimizer: str = 'grid',\n",
    "#         loss_fn: str = 'accuracy',\n",
    "#         metrics: Optional[list] = None,\n",
    "#         epochs: int = 100,\n",
    "#         batch_size: int = 32\n",
    "#     ):\n",
    "#         \"\"\"\n",
    "#         Initialize the model optimizer.\n",
    "        \n",
    "#         Args:\n",
    "#             model: The model to optimize\n",
    "#             optimizer: Type of optimizer ('grid' or 'random')\n",
    "#             loss_fn: Loss function to use\n",
    "#             metrics: List of metrics to track\n",
    "#             epochs: Number of training epochs\n",
    "#             batch_size: Training batch size\n",
    "#         \"\"\"\n",
    "#         self.model = model\n",
    "#         self.optimizer = optimizer\n",
    "#         self.loss_fn = loss_fn\n",
    "#         self.metrics = metrics or ['accuracy']\n",
    "#         self.epochs = epochs\n",
    "#         self.batch_size = batch_size\n",
    "        \n",
    "#     def optimize(\n",
    "#         self,\n",
    "#         X_train: np.ndarray,\n",
    "#         y_train: np.ndarray,\n",
    "#         model_type: str,\n",
    "#         **kwargs\n",
    "#     ) -> Union[GridSearchCV, RandomizedSearchCV]:\n",
    "#         \"\"\"\n",
    "#         Optimize the model using specified strategy.\n",
    "        \n",
    "#         Args:\n",
    "#             X_train: Training features\n",
    "#             y_train: Training labels\n",
    "#             model_type: Type of the model\n",
    "#             **kwargs: Additional arguments for the search\n",
    "            \n",
    "#         Returns:\n",
    "#             Optimized model\n",
    "#         \"\"\"\n",
    "#         if self.optimizer == 'random':\n",
    "#             return self.model.randomized_search_hyper_parameters(\n",
    "#                 model_type=model_type,\n",
    "#                 X_train=X_train,\n",
    "#                 y_train=y_train,\n",
    "#                 n_iter=self.epochs,\n",
    "#                 scoring=self.loss_fn,\n",
    "#                 **kwargs\n",
    "#             )\n",
    "#         else:  # grid search\n",
    "#             return self.model.grid_search_hyper_parameters(\n",
    "#                 model_type=model_type,\n",
    "#                 X_train=X_train,\n",
    "#                 y_train=y_train,\n",
    "#                 scoring=self.loss_fn,\n",
    "#                 **kwargs\n",
    "#             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **_Actual Code_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, model_type):\n",
    "        self.model_type = model_type\n",
    "        self.model = None\n",
    "    \n",
    "    def train_model(self, model, X_train, y_train):\n",
    "        self.model = model\n",
    "        self.model.fit(X_train, y_train)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        \"\"\" Make predictions using the trained model and create a CSV file for large prediction datasets. \"\"\"\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        # Uncomment to save predictions to a CSV file\n",
    "        output_df = pd.DataFrame({'id': X_test.index, 'yield': y_pred})\n",
    "        output_df.to_csv('Predictions.csv', index=False)\n",
    "        return y_pred\n",
    "    \n",
    "    def evaluate(self, y_true, y_pred):\n",
    "        \"\"\" Evaluate the model performance using various metrics. \"\"\"\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred, average='weighted')\n",
    "        recall = recall_score(y_true, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        print(f\"Precision: {precision}\")\n",
    "        print(f\"Recall: {recall}\")\n",
    "        print(f\"F1: {f1}\")\n",
    "        print(f\"MSE: {mse}\")\n",
    "        print(f\"MAE: {mae}\")\n",
    "        print(f\"R-squared (R2): {r2}\")\n",
    "    \n",
    "    def save_model(self, model_path):\n",
    "        \"\"\" Save the model to a file. \"\"\"\n",
    "        joblib.dump(self.model, model_path)\n",
    "\n",
    "    def load_model(self, model_path):\n",
    "        \"\"\" Load a model from a file. \"\"\"\n",
    "        self.model = joblib.load(model_path)\n",
    "        return self.model\n",
    "\n",
    "\n",
    "class ClassifierModel(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__(model_type='classifier')\n",
    "        \n",
    "    def select_classifier(self, model_type):\n",
    "        \"\"\" Select a classifier based on the specified model type. \"\"\"\n",
    "        if model_type == \"GradientBoostingClassifier\":\n",
    "            return self.gradients_boosting\n",
    "        elif model_type == \"RandomForestClassifier\":\n",
    "            return self.random_forest\n",
    "        elif model_type == \"XGBClassifier\":\n",
    "            return self.xgboost\n",
    "        elif model_type == \"LogisticRegression\":\n",
    "            return self.logistic_regression\n",
    "        elif model_type == \"DecisionTreeClassifier\":\n",
    "            return self.decision_tree\n",
    "        elif model_type == \"LightGBM\":\n",
    "            return self.lightgbm\n",
    "        elif model_type == \"CatBoost\":\n",
    "            return self.catboost\n",
    "        elif model_type == \"KNN\":\n",
    "            return self.k_nearest_neighbors\n",
    "        elif model_type == \"SVC\":\n",
    "            return self.support_vector_machines\n",
    "        elif model_type == \"NaiveBayes\":\n",
    "            return self.naive_bayes\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid model type: {model_type}\")\n",
    "\n",
    "    def gradients_boosting(self, X_train, y_train):\n",
    "        from sklearn.ensemble import GradientBoostingClassifier\n",
    "        model = GradientBoostingClassifier()\n",
    "        self.train_model(model, X_train, y_train)\n",
    "        return model\n",
    "    \n",
    "    def random_forest(self, X_train, y_train):\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        model = RandomForestClassifier()\n",
    "        self.train_model(model, X_train, y_train)\n",
    "        return model\n",
    "    \n",
    "    def xgboost(self, X_train, y_train):\n",
    "        from xgboost import XGBClassifier\n",
    "        model = XGBClassifier()\n",
    "        self.train_model(model, X_train, y_train)\n",
    "        return model\n",
    "    \n",
    "    def lightgbm(self, X_train, y_train):\n",
    "        from lightgbm import LGBMClassifier\n",
    "        model = LGBMClassifier()\n",
    "        self.train_model(model, X_train, y_train)\n",
    "        return model\n",
    "    \n",
    "    def catboost(self, X_train, y_train):\n",
    "        from catboost import CatBoostClassifier\n",
    "        model = CatBoostClassifier()\n",
    "        self.train_model(model, X_train, y_train)\n",
    "        return model\n",
    "    \n",
    "    def logistic_regression(self, X_train, y_train):\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        model = LogisticRegression()\n",
    "        self.train_model(model , X_train, y_train)\n",
    "        return model\n",
    "    \n",
    "    def decision_tree(self, X_train, y_train):\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        model = DecisionTreeClassifier()\n",
    "        self.train_model(model, X_train, y_train)\n",
    "        return model\n",
    "    \n",
    "    def k_nearest_neighbors(self, X_train, y_train):\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        model = KNeighborsClassifier()\n",
    "        self.train_model(model, X_train, y_train)\n",
    "        return model\n",
    "    \n",
    "    def support_vector_machines(self, X_train, y_train):\n",
    "        from sklearn.svm import SVC\n",
    "        model = SVC()\n",
    "        self.train_model(model, X_train, y_train)\n",
    "        return model\n",
    "    \n",
    "    def naive_bayes(self, X_train, y_train):\n",
    "        from sklearn.naive_bayes import GaussianNB\n",
    "        model = GaussianNB()    \n",
    "        self.train_model(model, X_train, y_train)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, Any, Optional, Union\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from scipy.stats import uniform, loguniform, randint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class EnhancedClassifierModel(ClassifierModel):\n",
    "    \"\"\"Enhanced classifier model with hyperparameter optimization capabilities.\"\"\"\n",
    "    \n",
    "    def __init__(self, json_file_path: str):\n",
    "        \"\"\"\n",
    "        Initialize the enhanced classifier model.\n",
    "        \n",
    "        Args:\n",
    "            json_file_path: Path to the JSON file containing hyperparameter configurations\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.json_file_path = json_file_path\n",
    "        self.hyperparameters = self.load_from_json()\n",
    "        \n",
    "    def load_from_json(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Load hyperparameter configurations from JSON file.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary containing hyperparameter configurations\n",
    "        \"\"\"\n",
    "        with open(self.json_file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "    \n",
    "    def _create_distribution(self, param_value: Union[str, list]) -> Any:\n",
    "        \"\"\"\n",
    "        Create scipy distribution objects for randomized search.\n",
    "        \n",
    "        Args:\n",
    "            param_value: Parameter value from JSON\n",
    "            \n",
    "        Returns:\n",
    "            Appropriate distribution object or list\n",
    "        \"\"\"\n",
    "        if isinstance(param_value, list):\n",
    "            return param_value\n",
    "        \n",
    "        if isinstance(param_value, str):\n",
    "            if param_value.startswith('randint'):\n",
    "                start, end = map(int, param_value.strip('randint()').split(','))\n",
    "                return randint(start, end)\n",
    "            elif param_value.startswith('uniform'):\n",
    "                start, end = map(float, param_value.strip('uniform()').split(','))\n",
    "                return uniform(start, end)\n",
    "            elif param_value.startswith('loguniform'):\n",
    "                start, end = map(float, param_value.strip('loguniform()').split(','))\n",
    "                return loguniform(start, end)\n",
    "        return param_value\n",
    "\n",
    "    def _prepare_search_params(self, model_type: str, search_type: str = 'random') -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Prepare parameters for hyperparameter search.\n",
    "        \n",
    "        Args:\n",
    "            model_type: Type of the model\n",
    "            search_type: Type of search ('random' or 'grid')\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of search parameters\n",
    "        \"\"\"\n",
    "        if model_type not in self.hyperparameters:\n",
    "            raise ValueError(f\"Model type {model_type} not found in hyperparameter configurations\")\n",
    "            \n",
    "        params = self.hyperparameters[model_type]\n",
    "        \n",
    "        if search_type == 'random':\n",
    "            return {k: self._create_distribution(v) for k, v in params.items()}\n",
    "        else:\n",
    "            return params\n",
    "\n",
    "    def randomized_search_hyper_parameters(\n",
    "        self,\n",
    "        model_type: str,\n",
    "        X_train: np.ndarray,\n",
    "        y_train: np.ndarray,\n",
    "        n_iter: int = 100,\n",
    "        cv: int = 5,\n",
    "        scoring: str = 'accuracy',\n",
    "        n_jobs: int = -1\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Perform randomized search for hyperparameter optimization.\n",
    "        \n",
    "        Args:\n",
    "            model_type: Type of model to optimize\n",
    "            X_train: Training features\n",
    "            y_train: Training labels\n",
    "            n_iter: Number of parameter settings sampled\n",
    "            cv: Number of cross-validation folds\n",
    "            scoring: Scoring metric to use\n",
    "            n_jobs: Number of parallel jobs\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing best parameters and scores\n",
    "        \"\"\"\n",
    "        base_model = self.select_classifier(model_type)(X_train, y_train)\n",
    "        search_params = self._prepare_search_params(model_type, 'random')\n",
    "        \n",
    "        random_search = RandomizedSearchCV(\n",
    "            estimator=base_model,\n",
    "            param_distributions=search_params,\n",
    "            n_iter=n_iter,\n",
    "            cv=cv,\n",
    "            scoring=scoring,\n",
    "            n_jobs=n_jobs,\n",
    "            random_state=42,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        random_search.fit(X_train, y_train)\n",
    "        \n",
    "        results = {\n",
    "            'best_params': random_search.best_params_,\n",
    "            'best_score': random_search.best_score_,\n",
    "            'best_estimator': random_search.best_estimator_,\n",
    "            'cv_results': random_search.cv_results_\n",
    "        }\n",
    "        \n",
    "        self.model = random_search.best_estimator_\n",
    "        return results\n",
    "\n",
    "    def grid_search_hyper_parameters(\n",
    "        self,\n",
    "        model_type: str,\n",
    "        X_train: np.ndarray,\n",
    "        y_train: np.ndarray,\n",
    "        cv: int = 5,\n",
    "        scoring: str = 'accuracy',\n",
    "        n_jobs: int = -1\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Perform grid search for hyperparameter optimization.\n",
    "        \n",
    "        Args:\n",
    "            model_type: Type of model to optimize\n",
    "            X_train: Training features\n",
    "            y_train: Training labels\n",
    "            cv: Number of cross-validation folds\n",
    "            scoring: Scoring metric to use\n",
    "            n_jobs: Number of parallel jobs\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing best parameters and scores\n",
    "        \"\"\"\n",
    "        base_model = self.select_classifier(model_type)(X_train, y_train)\n",
    "        search_params = self._prepare_search_params(model_type, 'grid')\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=base_model,\n",
    "            param_grid=search_params,\n",
    "            cv=cv,\n",
    "            scoring=scoring,\n",
    "            n_jobs=n_jobs,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        results = {\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'best_score': grid_search.best_score_,\n",
    "            'best_estimator': grid_search.best_estimator_,\n",
    "            'cv_results': grid_search.cv_results_\n",
    "        }\n",
    "        \n",
    "        self.model = grid_search.best_estimator_\n",
    "        return results\n",
    "\n",
    "\n",
    "class ModelOptimizer:\n",
    "    \"\"\"Class for optimizing model training process.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Any,\n",
    "        optimizer: Any,\n",
    "        loss_fn: Any,\n",
    "        metrics: list,\n",
    "        epochs: int = 100,\n",
    "        batch_size: int = 32\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the model optimizer.\n",
    "        \n",
    "        Args:\n",
    "            model: Model to optimize\n",
    "            optimizer: Optimization algorithm\n",
    "            loss_fn: Loss function\n",
    "            metrics: List of metrics to track\n",
    "            epochs: Number of training epochs\n",
    "            batch_size: Batch size for training\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.metrics = metrics\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.history = []\n",
    "        \n",
    "    def optimize(self, X_train: np.ndarray, y_train: np.ndarray) -> Dict[str, list]:\n",
    "        \"\"\"\n",
    "        Optimize the model training process.\n",
    "        \n",
    "        Args:\n",
    "            X_train: Training features\n",
    "            y_train: Training labels\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing training history\n",
    "        \"\"\"\n",
    "        # Create data loader or batches\n",
    "        n_samples = len(X_train)\n",
    "        n_batches = n_samples // self.batch_size\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            epoch_metrics = {metric.__name__: 0.0 for metric in self.metrics}\n",
    "            epoch_loss = 0.0\n",
    "            \n",
    "            # Shuffle data\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X_train[indices]\n",
    "            y_shuffled = y_train[indices]\n",
    "            \n",
    "            for batch in range(n_batches):\n",
    "                start_idx = batch * self.batch_size\n",
    "                end_idx = start_idx + self.batch_size\n",
    "                \n",
    "                X_batch = X_shuffled[start_idx:end_idx]\n",
    "                y_batch = y_shuffled[start_idx:end_idx]\n",
    "                \n",
    "                # Forward pass\n",
    "                y_pred = self.model.predict(X_batch)\n",
    "                loss = self.loss_fn(y_batch, y_pred)\n",
    "                \n",
    "                # Backward pass and optimization\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                # Update metrics\n",
    "                epoch_loss += loss.item()\n",
    "                for metric in self.metrics:\n",
    "                    epoch_metrics[metric.__name__] += metric(y_batch, y_pred)\n",
    "            \n",
    "            # Average metrics\n",
    "            epoch_loss /= n_batches\n",
    "            for metric_name in epoch_metrics:\n",
    "                epoch_metrics[metric_name] /= n_batches\n",
    "                \n",
    "            # Store history\n",
    "            self.history.append({\n",
    "                'epoch': epoch + 1,\n",
    "                'loss': epoch_loss,\n",
    "                **epoch_metrics\n",
    "            })\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{self.epochs}\")\n",
    "                print(f\"Loss: {epoch_loss:.4f}\")\n",
    "                for metric_name, value in epoch_metrics.items():\n",
    "                    print(f\"{metric_name}: {value:.4f}\")\n",
    "                print()\n",
    "                \n",
    "        return self.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **_Testing Code Here_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"../Datasets/train.csv\"\n",
    "test_path = \"../Datasets/test.csv\"\n",
    "loader = DataLoaderPipeline(train_path=train_path, test_path=test_path)\n",
    "\n",
    "train_data, test_data = loader.load_data()\n",
    "print(train_data)\n",
    "loader.label_encode_data(train_data, test_data)\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data.drop(['Exited'], axis=1)\n",
    "y = train_data['Exited']\n",
    "Z = test_data\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier = OutlierPipeline(X)\n",
    "print(outlier.detect_outliers_zscore(\"CreditScore\"))\n",
    "print(outlier.detect_outliers_mad(\"CreditScore\"))\n",
    "print(outlier.detect_outliers_iqr(\"CreditScore\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_v = CrossValidator(n_splits=5, shuffle=False, random_state=42)\n",
    "if c_v is not None: \n",
    "    for X_train, X_test, y_train, y_test in c_v.k_fold(X, y):\n",
    "        break\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model('SVC')\n",
    "c_model = ClassifierModel()\n",
    "s_model = c_model.select_classifier('GradientBoostingClassifier')\n",
    "s_model(X_train=X_train, y_train=y_train)\n",
    "\n",
    "y_pred = c_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_model.evaluate(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "c_model.predict(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the enhanced classifier with hyperparameter configurations\n",
    "# classifier = EnhancedClassifierModel('hyperparameters.json')\n",
    "\n",
    "# # For randomized search\n",
    "# random_search_results = classifier.randomized_search_hyper_parameters(\n",
    "#     model_type='RandomForestClassifier',\n",
    "#     X_train=X_train,\n",
    "#     y_train=y_train,\n",
    "#     n_iter=100,\n",
    "#     cv=5,\n",
    "#     scoring='accuracy'\n",
    "# )\n",
    "\n",
    "classifier = EnhancedClassifierModel('../PyFiles/JSON/GridSearchHP.json')\n",
    "# For grid search\n",
    "grid_search_results = classifier.grid_search_hyper_parameters(\n",
    "    model_type='GradientBoostingClassifier',\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    cv=5,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "# Get the best model and make predictions\n",
    "best_model = classifier.model\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "metrics = classifier.evaluate(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **_Extra Comments File_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from typing import Dict, Any\n",
    "\n",
    "# class ModelOptimizer:\n",
    "#     \"\"\"\n",
    "#     A class for optimizing machine learning models using GridSearchCV.\n",
    "#     Includes predefined parameter grids for various classifiers.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __init__(self, cv=5, n_jobs=-1, verbose=1):\n",
    "#         self.cv = cv\n",
    "#         self.n_jobs = n_jobs\n",
    "#         self.verbose = verbose\n",
    "#         self.best_params = {}\n",
    "#         self.best_score = None\n",
    "#         self.best_model = None\n",
    "        \n",
    "#     def _get_param_grid(self, model_type: str) -> Dict[str, Any]:\n",
    "#         \"\"\"Returns the parameter grid for the specified model type.\"\"\"\n",
    "#         param_grids = {\n",
    "#             \"GradientBoostingClassifier\": {\n",
    "#                 'n_estimators': [100, 200, 300],\n",
    "#                 'learning_rate': [0.01, 0.1, 0.3],\n",
    "#                 'max_depth': [3, 4, 5],\n",
    "#                 'min_samples_split': [2, 5],\n",
    "#                 'min_samples_leaf': [1, 2],\n",
    "#                 'subsample': [0.8, 0.9, 1.0]\n",
    "#             },\n",
    "#             \"RandomForestClassifier\": {\n",
    "#                 'n_estimators': [100, 200, 300],\n",
    "#                 'max_depth': [None, 10, 20, 30],\n",
    "#                 'min_samples_split': [2, 5, 10],\n",
    "#                 'min_samples_leaf': [1, 2, 4],\n",
    "#                 'max_features': ['sqrt', 'log2']\n",
    "#             },\n",
    "#             \"XGBClassifier\": {\n",
    "#                 'n_estimators': [100, 200, 300],\n",
    "#                 'max_depth': [3, 4, 5],\n",
    "#                 'learning_rate': [0.01, 0.1, 0.3],\n",
    "#                 'subsample': [0.8, 0.9, 1.0],\n",
    "#                 'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "#                 'min_child_weight': [1, 3, 5]\n",
    "#             },\n",
    "#             \"LogisticRegression\": {\n",
    "#                 'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "#                 'penalty': ['l1', 'l2'],\n",
    "#                 'solver': ['liblinear', 'saga'],\n",
    "#                 'max_iter': [100, 200, 300]\n",
    "#             },\n",
    "#             \"DecisionTreeClassifier\": {\n",
    "#                 'max_depth': [None, 10, 20, 30],\n",
    "#                 'min_samples_split': [2, 5, 10],\n",
    "#                 'min_samples_leaf': [1, 2, 4],\n",
    "#                 'criterion': ['gini', 'entropy']\n",
    "#             },\n",
    "#             \"LGBMClassifier\": {\n",
    "#                 'n_estimators': [100, 200, 300],\n",
    "#                 'learning_rate': [0.01, 0.1, 0.3],\n",
    "#                 'max_depth': [-1, 5, 10],\n",
    "#                 'num_leaves': [31, 62, 127],\n",
    "#                 'min_child_samples': [20, 30, 50],\n",
    "#                 'subsample': [0.8, 0.9, 1.0]\n",
    "#             },\n",
    "#             \"CatBoostClassifier\": {\n",
    "#                 'iterations': [100, 200, 300],\n",
    "#                 'learning_rate': [0.01, 0.1, 0.3],\n",
    "#                 'depth': [4, 6, 8],\n",
    "#                 'l2_leaf_reg': [1, 3, 5, 7],\n",
    "#                 'border_count': [32, 64, 128]\n",
    "#             },\n",
    "#             \"KNeighborsClassifier\": {\n",
    "#                 'n_neighbors': [3, 5, 7, 9, 11],\n",
    "#                 'weights': ['uniform', 'distance'],\n",
    "#                 'algorithm': ['auto', 'ball_tree', 'kd_tree'],\n",
    "#                 'leaf_size': [20, 30, 40],\n",
    "#                 'p': [1, 2]\n",
    "#             },\n",
    "#             \"SVC\": {\n",
    "#                 'C': [0.1, 1, 10, 100],\n",
    "#                 'kernel': ['rbf', 'linear'],\n",
    "#                 'gamma': ['scale', 'auto', 0.1, 0.01],\n",
    "#                 'class_weight': [None, 'balanced']\n",
    "#             },\n",
    "#             \"NaiveBayes\": {\n",
    "#                 'var_smoothing': np.logspace(0,-9, num=5)\n",
    "#             }\n",
    "#         }\n",
    "#         return param_grids.get(model_type, {})\n",
    "    \n",
    "#     def optimize(self, model, X_train, y_train, model_type):\n",
    "#         \"\"\"\n",
    "#         Performs grid search optimization for the specified model.\n",
    "        \n",
    "#         Args:\n",
    "#             model: The model instance to optimize\n",
    "#             X_train: Training features\n",
    "#             y_train: Training labels\n",
    "#             model_type: String identifier for the model type\n",
    "            \n",
    "#         Returns:\n",
    "#             The best model found by grid search\n",
    "#         \"\"\"\n",
    "#         param_grid = self._get_param_grid(model_type)\n",
    "#         if not param_grid:\n",
    "#             raise ValueError(f\"No parameter grid defined for model type: {model_type}\")\n",
    "            \n",
    "#         grid_search = GridSearchCV(\n",
    "#             estimator=model,\n",
    "#             param_grid=param_grid,\n",
    "#             cv=self.cv,\n",
    "#             n_jobs=self.n_jobs,\n",
    "#             verbose=self.verbose,\n",
    "#             scoring='accuracy'\n",
    "#         )\n",
    "        \n",
    "#         grid_search.fit(X_train, y_train)\n",
    "        \n",
    "#         self.best_params = grid_search.best_params_\n",
    "#         self.best_score = grid_search.best_score_\n",
    "#         self.best_model = grid_search.best_estimator_\n",
    "        \n",
    "#         print(f\"Best parameters for {model_type}:\")\n",
    "#         print(self.best_params)\n",
    "#         print(f\"Best cross-validation score: {self.best_score:.4f}\")\n",
    "        \n",
    "#         return self.best_model\n",
    "\n",
    "# class EnhancedClassifierModel(ClassifierModel):\n",
    "#     \"\"\"\n",
    "#     Enhanced version of ClassifierModel that includes hyperparameter optimization.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.optimizer = ModelOptimizer()\n",
    "        \n",
    "#     def train_with_optimization(self, model_type, X_train, y_train):\n",
    "#         \"\"\"\n",
    "#         Trains the specified model type with hyperparameter optimization.\n",
    "        \n",
    "#         Args:\n",
    "#             model_type: String identifier for the model type\n",
    "#             X_train: Training features\n",
    "#             y_train: Training labels\n",
    "            \n",
    "#         Returns:\n",
    "#             Optimized model instance\n",
    "#         \"\"\"\n",
    "#         base_model = self.select_classifier(model_type)(X_train, y_train)\n",
    "#         optimized_model = self.optimizer.optimize(base_model, X_train, y_train, model_type)\n",
    "#         return optimized_model\n",
    "\n",
    "# # Example usage:\n",
    "# # \"\"\"\n",
    "# # Initialize the enhanced model\n",
    "# enhanced_model = EnhancedClassifierModel()\n",
    "\n",
    "# # Train and optimize a specific model\n",
    "# best_model = enhanced_model.train_with_optimization('SVC', X_train, y_train)\n",
    "\n",
    "# # Make predictions\n",
    "# y_pred = best_model.predict(X_test)\n",
    "\n",
    "# # Evaluate\n",
    "# enhanced_model.evaluate(y_test, y_pred)\n",
    "# # \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "\n",
    "# class EnhancedClassifierModel(ClassifierModel):\n",
    "#     def __init__(self, json_file_path):\n",
    "#         super().__init__()\n",
    "#         self.json_file_path = json_file_path\n",
    "#         self.optimizer = ModelOptimizer()\n",
    "\n",
    "#     def load_from_json(self):\n",
    "#         with open(self.json_file_path, 'r') as file:\n",
    "#             data = json.load(file)\n",
    "#         return data\n",
    "    \n",
    "#     def randomized_search_hyper_parameters(self):\n",
    "#         pass\n",
    "\n",
    "#     def grid_search_hyper_parameters(self):\n",
    "#         pass\n",
    "\n",
    "\n",
    "# class ModelOptimizer:\n",
    "#     def __init__(self, model, optimizer, loss_fn, metrics, epochs, batch_size):\n",
    "#         self.model = model\n",
    "#         self.optimizer = optimizer\n",
    "#         self.loss_fn = loss_fn\n",
    "#         self.metrics = metrics\n",
    "#         self.epochs = epochs\n",
    "#         self.batch_size = batch_size\n",
    "        \n",
    "#     def optimize(self, X_train, y_train):\n",
    "#         pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **_Combine_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **_Mark 2_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix,\n",
    "                             f1_score, r2_score, mean_squared_error, mean_absolute_error,\n",
    "                             roc_curve, auc)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Pipeline DataLoader\n",
    "class PipelineDataLoader:\n",
    "    def __init__(self, train_data_path, test_data_path):\n",
    "        self.train_data_path = train_data_path\n",
    "        self.test_data_path = test_data_path\n",
    "\n",
    "    def load_data(self):\n",
    "        train_data = pd.read_csv(self.train_data_path, index_col=0)\n",
    "        test_data = pd.read_csv(self.test_data_path, index_col=0)\n",
    "        X = train_data.drop(['Exited', 'CustomerId', 'NumOfProducts'], axis=1)\n",
    "        test_data = test_data.drop(['CustomerId', 'NumOfProducts'], axis=1)\n",
    "        y = train_data['Exited']\n",
    "        return train_data, test_data, X, y\n",
    "    \n",
    "    def encode_labels_data(self, data):\n",
    "        for col in data.columns:\n",
    "            if data[col].dtype == 'object':\n",
    "                data[col] = pd.Categorical(data[col]).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline Classifier Model\n",
    "class Model:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def scalling(self, data):\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "        return scaler.fit_transform(data)\n",
    "    \n",
    "    # def train_model(self, X_train, y_train, scalling=False):\n",
    "    #     if scalling:\n",
    "    #         X_train = self.scalling(X_train)\n",
    "        \n",
    "    #     self.model.fit(X_train, y_train)\n",
    "    def train_model(self, X_train, y_train, X_val=None, y_val=None, scalling=False):\n",
    "        if scalling:\n",
    "            from sklearn.preprocessing import StandardScaler\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            if X_val is not None:\n",
    "                X_val = scaler.transform(X_val)\n",
    "\n",
    "        if self.model_type in ['xgradient_boost', 'catboosting']:\n",
    "            evals = [(X_val, y_val)] if X_val is not None else None\n",
    "            self.model.fit(X_train, y_train, eval_set=evals, verbose=False)\n",
    "        else:\n",
    "            self.model.fit(X_train, y_train)\n",
    "        \n",
    "    def predict_model(self, X_test):\n",
    "        predictions = self.model.predict(X_test)\n",
    "        return predictions\n",
    "    \n",
    "    def predict_proba_model(self, X_test):\n",
    "        probabilities = self.model.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\n",
    "        return probabilities\n",
    "    \n",
    "    def evaluate_model(self, y_test, y_pred, y_proba=None):\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred)\n",
    "        matrix = confusion_matrix(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='macro')\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "        # Initialize roc_auc, fpr, tpr\n",
    "        roc_auc = None\n",
    "        fpr = None\n",
    "        tpr = None\n",
    "\n",
    "        # Calculate ROC AUC if probabilities are provided\n",
    "        if y_proba is not None:\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        return accuracy, report, matrix, f1, r2, mse, mae, roc_auc, fpr, tpr\n",
    "\n",
    "    def plot_roc_curve(self, fpr, tpr, roc_auc):\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "class PipelineClassifier(Model):\n",
    "    def __init__(self, model_type):\n",
    "        self.model_type = model_type\n",
    "        self.model = None\n",
    "\n",
    "    def select_model(self):\n",
    "        if self.model_type == 'logistic_regression':\n",
    "            self.model = LogisticRegression()\n",
    "        elif self.model_type == 'random_forest':\n",
    "            self.model = RandomForestClassifier()\n",
    "        elif self.model_type == 'gradient_boost':\n",
    "            self.model = GradientBoostingClassifier(n_estimators=200, max_depth=4)\n",
    "        elif self.model_type == 'xgradient_boost':\n",
    "            self.model = XGBClassifier(n_estimators=120, max_depth=2, early_stopping_rounds=10, eval_metric=\"logloss\")\n",
    "        elif self.model_type == 'catboosting':\n",
    "            self.model = CatBoostClassifier(n_estimators=150, max_depth=3, early_stopping_rounds=10, eval_metric=\"Logloss\", verbose=0)\n",
    "        elif self.model_type == 'lgbm':\n",
    "            self.model = LGBMClassifier(n_estimators=250, max_depth=4, reg_lambda=0.1, learning_rate=0.12)\n",
    "        elif self.model_type == 'decision_tree':\n",
    "            self.model = DecisionTreeClassifier(max_depth=4)\n",
    "        elif self.model_type == 'knn':\n",
    "            self.model = KNeighborsClassifier(algorithm='kd_tree', weights='distance')\n",
    "        elif self.model_type == 'naive_bayes':\n",
    "            self.model = GaussianNB(var_smoothing=1e-15)\n",
    "        elif self.model_type == 'svc':\n",
    "            self.model = SVC(kernel='poly', degree=1)\n",
    "        elif self.model_type == 'hist_gradient_boost':\n",
    "            self.model = HistGradientBoostingClassifier(max_iter=100, random_state=1)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model type\")\n",
    "    \n",
    "    def get_model(self):\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineCrossValidation:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def t_t_split(self, test_size=0.3, random_state=1):\n",
    "        \"\"\" Train Test Split Cross Validation \"\"\"\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def k_fold(self):\n",
    "        \"\"\" K-Fold Cross Validation \"\"\"\n",
    "        from sklearn.model_selection import KFold\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "        for train_index, test_index in kf.split(self.X):\n",
    "            X_train, X_test = self.X.iloc[train_index], self.X.iloc[test_index]\n",
    "            y_train, y_test = self.y.iloc[train_index], self.y.iloc[test_index]\n",
    "            yield X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "data_loader = PipelineDataLoader('../Datasets/train.csv', '../Datasets/test.csv')\n",
    "train_data, test_data, X, y = data_loader.load_data()\n",
    "data_loader.encode_labels_data(X)  # Ensure encoding is applied to features\n",
    "data_loader.encode_labels_data(test_data)  # Ensure encoding is applied to features\n",
    "\n",
    "\n",
    "\n",
    "# cross_validator = PipelineCrossValidation(X, y)\n",
    "# # X_train, X_test, y_train, y_test = cross_validator.t_t_split()\n",
    "\n",
    "# for Xtrain, Xtest, ytrain, ytest in cross_validator.k_fold():\n",
    "#     X_train, X_test, y_train, y_test = Xtrain, Xtest, ytrain, ytest\n",
    "#     break\n",
    "# # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# pipeline = PipelineClassifier('random_forest')\n",
    "# pipeline.select_model()\n",
    "# pipeline.train_model(X_train, y_train, scalling=False)\n",
    "\n",
    "# predictions = pipeline.predict_model(X_test)\n",
    "# probabilities = pipeline.predict_proba_model(X_test)  # Get predicted probabilities\n",
    "# accuracy, report, matrix, f1, r2, mse, mae, roc_auc, fpr, tpr = pipeline.evaluate_model(y_test, predictions, probabilities)\n",
    "\n",
    "# if roc_auc is not None:\n",
    "#     pipeline.plot_roc_curve(fpr, tpr, roc_auc)\n",
    "\n",
    "# print(\"Accuracy:\", accuracy)\n",
    "# print(\"R2 Score:\", r2)\n",
    "# print(\"F1 Score:\", f1)\n",
    "# print(\"Confusion Matrix:\\n\", matrix)\n",
    "# print(\"Classification Report:\\n\", report)\n",
    "# print(\"Auc Matrix:\", roc_auc)\n",
    "\n",
    "\n",
    "cross_validator = PipelineCrossValidation(X, y)\n",
    "\n",
    "for Xtrain, Xtest, ytrain, ytest in cross_validator.k_fold():\n",
    "    pipeline = PipelineClassifier('catboosting')  # or 'catboosting', 'hist_gradient_boost', etc.\n",
    "    pipeline.select_model()\n",
    "    pipeline.train_model(Xtrain, ytrain, X_val=Xtest, y_val=ytest, scalling=False)\n",
    "\n",
    "    predictions = pipeline.predict_model(Xtest)\n",
    "    probabilities = pipeline.predict_proba_model(Xtest)  # Get predicted probabilities\n",
    "    accuracy, report, matrix, f1, r2, mse, mae, roc_auc, fpr, tpr = pipeline.evaluate_model(ytest, predictions, probabilities)\n",
    "\n",
    "    if roc_auc is not None:\n",
    "        pipeline.plot_roc_curve(fpr, tpr, roc_auc)\n",
    "\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"R2 Score:\", r2)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"Confusion Matrix:\\n\", matrix)\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "    print(\"AUC Score:\", roc_auc)\n",
    "\n",
    "# Example usage for predictions on the test dataset\n",
    "predictions = pipeline.predict_model(test_data)\n",
    "dataframe = pd.DataFrame({\n",
    "    'id': test_data.index,\n",
    "    'prediction': predictions\n",
    "})\n",
    "\n",
    "dataframe.to_csv(\"Predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a preprocessing pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, PolynomialFeatures\n",
    "\n",
    "\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Create a pipeline with preprocessing and model\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('poly_features', PolynomialFeatures(degree=2, include_bias=False)),  # Adding polynomial features\n",
    "    ('model', RandomForestRegressor())\n",
    "])\n",
    "\n",
    "# Hyperparameter tuning\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 200],\n",
    "    'model__max_depth': [None, 10, 20],\n",
    "    'model__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='r2')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"Best RÂ² Score:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline.predict_model(test_data)\n",
    "predictions\n",
    "\n",
    "dataframe = pd.DataFrame({\n",
    "    'id': test_data.index,\n",
    "    'prediction': predictions\n",
    "})\n",
    "\n",
    "dataframe.to_csv(\"Predictions.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NIKAL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create pipeline for Feature Engineering and Feature Extraction and Feature Selection methods\n",
    "# class PipelineFeatureExtraction:\n",
    "#     def __init__(self, dataset):\n",
    "#         self.dataset = dataset\n",
    "\n",
    "#     def feature_engineering(self):\n",
    "#         # Perform feature engineering methods here\n",
    "#         # For example, let's create a new feature that is the square of the age\n",
    "#         self.dataset['age_squared'] = self.dataset['age'] ** 2\n",
    "#         self.dataset['"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# def plot_feature_correlations(data, figsize=(12, 8)):\n",
    "#     \"\"\"\n",
    "#     Create comprehensive correlation plots for all numerical features in the dataset.\n",
    "    \n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     data : pandas.DataFrame\n",
    "#         Input DataFrame containing the features to analyze\n",
    "#     figsize : tuple, optional (default=(12, 8))\n",
    "#         Size of the correlation heatmap figure\n",
    "        \n",
    "#     Returns:\n",
    "#     --------\n",
    "#     None (displays plots)\n",
    "#     \"\"\"\n",
    "#     # Select only numerical columns\n",
    "#     numerical_data = data.select_dtypes(include=[np.number])\n",
    "    \n",
    "#     # Calculate correlation matrix\n",
    "#     correlation_matrix = numerical_data.corr()\n",
    "    \n",
    "#     # Create subplots\n",
    "#     plt.figure(figsize=figsize)\n",
    "    \n",
    "#     # Create correlation heatmap\n",
    "#     plt.subplot(1, 1, 1)\n",
    "#     sns.heatmap(correlation_matrix, \n",
    "#                 annot=True,  # Show correlation values\n",
    "#                 cmap='coolwarm',  # Color scheme\n",
    "#                 center=0,  # Center the colormap at 0\n",
    "#                 square=True,  # Make the plot square-shaped\n",
    "#                 fmt='.2f',  # Format correlation values to 2 decimal places\n",
    "#                 linewidths=0.5,  # Add gridlines\n",
    "#                 cbar_kws={\"shrink\": .8})  # Adjust colorbar size\n",
    "    \n",
    "#     plt.title('Feature Correlation Heatmap')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "    \n",
    "#     # Create pairplot for detailed feature relationships\n",
    "#     sns.pairplot(data=numerical_data, \n",
    "#                  diag_kind='kde',  # Show kernel density estimation on diagonal\n",
    "#                  plot_kws={'alpha': 0.6})  # Add transparency to scatter plots\n",
    "#     plt.show()\n",
    "    \n",
    "#     # Print strong correlations (absolute value > 0.5)\n",
    "#     print(\"\\nStrong Feature Correlations (|correlation| > 0.5):\")\n",
    "#     strong_correlations = []\n",
    "#     for i in range(len(correlation_matrix.columns)):\n",
    "#         for j in range(i+1, len(correlation_matrix.columns)):\n",
    "#             if abs(correlation_matrix.iloc[i,j]) > 0.5:\n",
    "#                 strong_correlations.append({\n",
    "#                     'Feature 1': correlation_matrix.columns[i],\n",
    "#                     'Feature 2': correlation_matrix.columns[j],\n",
    "#                     'Correlation': correlation_matrix.iloc[i,j]\n",
    "#                 })\n",
    "    \n",
    "#     if strong_correlations:\n",
    "#         strong_corr_df = pd.DataFrame(strong_correlations)\n",
    "#         print(strong_corr_df.sort_values(by='Correlation', key=abs, ascending=False))\n",
    "#     else:\n",
    "#         print(\"No strong correlations found in the dataset.\")\n",
    "\n",
    "# plot_feature_correlations(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Pipeline for Outlier Analysis and Outlier Removal\n",
    "# # # =============================================================================\n",
    "# # class Outlier:\n",
    "# #     def __init__(self, data, threshold=1.5):\n",
    "# #         self.data = data\n",
    "# #         self.threshold = threshold\n",
    "\n",
    "# #     def outlier_detections(self, method='IQR'):\n",
    "# #         \"\"\" Here we are defining the outlier detection method for the data cleaning algorithm \"\"\"\n",
    "# #         if method == 'IQR':\n",
    "# #             return self.IQR_outliers()\n",
    "# #         elif method == 'Z-score':\n",
    "# #             return self.Z_score_outliers()\n",
    "# #         elif method == 'Modified Z-score':\n",
    "# #             return self.Modified_Z_score_outliers()\n",
    "# #         elif method == 'Percentile':\n",
    "# #             return self.Percentile_outliers()\n",
    "# #         else:\n",
    "# #             raise ValueError(\"Invalid outlier detection method. Please choose from 'IQR', 'Z-score', 'Modified Z-score', or 'Percentile'\")\n",
    "        \n",
    "# #     def outlier_removal(self, outlier, method=None):\n",
    "# #         \"\"\" Here we are defining the outlier removal method for the data cleaning algorithm \"\"\"\n",
    "# #         if method == 'mean':\n",
    "# #             return self.mean_removal(outlier)\n",
    "# #         elif method == 'median':\n",
    "# #             return self.median_removal(outlier)\n",
    "# #         elif method == 'mode':\n",
    "# #             return self.mode_removal(outlier)\n",
    "# #         elif method == 'drop':\n",
    "# #             return self.drop_removal(outlier)\n",
    "# #         else:\n",
    "# #             raise ValueError(\"Invalid outlier removal method. Please choose from 'mean', 'median', 'mode', or 'drop'\")\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from scipy import stats\n",
    "\n",
    "# # Define the Outlier class (as provided in your modified version)\n",
    "# class Outlier:\n",
    "#     def __init__(self, data, threshold=1.5):\n",
    "#         self.data = data\n",
    "#         self.threshold = threshold\n",
    "\n",
    "#     def outlier_detections(self, columns, method='IQR'):\n",
    "#         \"\"\" Detect outliers for specified columns using the specified method. \"\"\"\n",
    "#         outliers = {}\n",
    "#         for column in columns:\n",
    "#             if method == 'IQR':\n",
    "#                 # outliers[column] = self.IQR_outliers(column)\n",
    "#             elif method == 'Z-score':\n",
    "#                 outliers[column] = self.Z_score_outliers(column)\n",
    "#             elif method == 'Modified Z-score':\n",
    "#                 outliers[column] = self.Modified_Z_score_outliers(column)\n",
    "#             elif method == 'Percentile':\n",
    "#                 outliers[column] = self.Percentile_outliers(column)\n",
    "#             else:\n",
    "#                 raise ValueError(\"Invalid outlier detection method. Please choose from 'IQR', 'Z-score', 'Modified Z-score', or 'Percentile'\")\n",
    "#         return outliers\n",
    "        \n",
    "#     def IQR_outliers(self, column):\n",
    "#         \"\"\" Identify outliers using the Interquartile Range (IQR) method for a specific column \"\"\"\n",
    "#         Q1 = self.data[column].quantile(0.25)\n",
    "#         Q3 = self.data[column].quantile(0.75)\n",
    "#         IQR = Q3 - Q1\n",
    "#         lower_bound = Q1 - self.threshold * IQR\n",
    "#         upper_bound = Q3 + self.threshold * IQR\n",
    "#         return self.data[(self.data[column] < lower_bound) | (self.data[column] > upper_bound)].index.tolist()\n",
    "\n",
    "#     def Z_score_outliers(self, column):\n",
    "#         \"\"\" Identify outliers using the Z-score method for a specific column \"\"\"\n",
    "#         z_scores = np.abs(stats.zscore(self.data[column]))\n",
    "#         return self.data[z_scores > self.threshold].index.tolist()\n",
    "\n",
    "#     def Modified_Z_score_outliers(self, column):\n",
    "#         \"\"\" Identify outliers using the Modified Z-score method for a specific column \"\"\"\n",
    "#         median = self.data[column].median()\n",
    "#         mad = np.median(np.abs(self.data[column] - median))\n",
    "#         modified_z_scores = 0.6745 * (self.data[column] - median) / mad\n",
    "#         return self.data[np.abs(modified_z_scores) > self.threshold].index.tolist()\n",
    "\n",
    "#     def Percentile_outliers(self, column):\n",
    "#         \"\"\" Identify outliers using the Percentile method for a specific column \"\"\"\n",
    "#         lower_bound = self.data[column].quantile(self.threshold / 100)\n",
    "#         upper_bound = self.data[column].quantile(100 - self.threshold / 100)\n",
    "#         return self.data[(self.data[column] < lower_bound) | (self.data[column] > upper_bound)].index.tolist()\n",
    "\n",
    "#     def outlier_removal(self, outlier_indices, method='drop'):\n",
    "#         \"\"\" Remove outliers from the dataset based on the specified method. \"\"\"\n",
    "#         cleaned_data = self.data.copy()\n",
    "#         if method == 'mean':\n",
    "#             for column in cleaned_data.columns:\n",
    "#                 mean_value = cleaned_data[column].mean()\n",
    "#                 cleaned_data.loc[outlier_indices[column], column] = mean_value\n",
    "#         elif method == 'median':\n",
    "#             for column in cleaned_data.columns:\n",
    "#                 median_value = cleaned_data[column].median()\n",
    "#                 cleaned_data.loc[outlier_indices[column], column] = median_value\n",
    "#         elif method == 'mode':\n",
    "#             for column in cleaned_data.columns:\n",
    "#                 mode_value = stats.mode(cleaned_data[column])[0][0]\n",
    "#                 cleaned_data.loc[outlier_indices[column], column] = mode_value\n",
    "#         elif method == 'drop':\n",
    "#             cleaned_data = cleaned_data.drop(index=np.unique(np.concatenate(list(outlier_indices.values()))))\n",
    "#         else:\n",
    "#             raise ValueError(\"Invalid outlier removal method. Please choose from 'mean', 'median', 'mode', or 'drop'\")\n",
    "#         return cleaned_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
