{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 9554, number of negative: 9554\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002717 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3965\n",
      "[LightGBM] [Info] Number of data points in the train set: 19108, number of used features: 21\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "Model Performance:\n",
      "ROC AUC: 0.9167\n",
      "Accuracy: 0.8903\n",
      "R² Score: 0.3238\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.94      0.93      2389\n",
      "         1.0       0.74      0.70      0.72       611\n",
      "\n",
      "    accuracy                           0.89      3000\n",
      "   macro avg       0.83      0.82      0.83      3000\n",
      "weighted avg       0.89      0.89      0.89      3000\n",
      "\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, classification_report, r2_score\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "class ChurnPredictor:\n",
    "    def __init__(self, random_state=42):\n",
    "        self.random_state = random_state\n",
    "        self.model = None\n",
    "        self.preprocessor = None\n",
    "        \n",
    "    def create_features(self, df):\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Basic features\n",
    "        df['Balance_per_Product'] = df['Balance'] / (df['NumOfProducts'] + 1)\n",
    "        df['Balance_per_Age'] = df['Balance'] / (df['Age'] + 1)\n",
    "        df['Balance_per_Tenure'] = df['Balance'] / (df['Tenure'] + 1)\n",
    "        df['Products_per_Tenure'] = df['NumOfProducts'] / (df['Tenure'] + 1)\n",
    "        \n",
    "        # Interaction features\n",
    "        df['Credit_Age_Ratio'] = df['CreditScore'] / (df['Age'] + 1)\n",
    "        \n",
    "        # Advanced features\n",
    "        df['IsHighValue'] = ((df['Balance'] > df['Balance'].mean()) & \n",
    "                           (df['EstimatedSalary'] > df['EstimatedSalary'].mean())).astype(int)\n",
    "        df['IsLongTerm'] = (df['Tenure'] > df['Tenure'].median()).astype(int)\n",
    "        \n",
    "        # Risk score feature\n",
    "        df['RiskScore'] = (\n",
    "            (df['CreditScore'] < 600).astype(int) * 2 +\n",
    "            (df['Balance'] > 100000).astype(int) * 1.5 +\n",
    "            (df['Age'] < 30).astype(int) * 1.2 +\n",
    "            (df['IsActiveMember'] == 0).astype(int) * 1.8 +\n",
    "            (df['NumOfProducts'] > 2).astype(int) * 1.3\n",
    "        )\n",
    "        \n",
    "        # Additional behavioral features\n",
    "        df['HasZeroBalance'] = (df['Balance'] == 0).astype(int)\n",
    "        df['IsNewCustomer'] = (df['Tenure'] <= 1).astype(int)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def prepare_pipelines(self):\n",
    "        numeric_features = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', \n",
    "                          'EstimatedSalary', 'Balance_per_Product', 'Balance_per_Age',\n",
    "                          'Balance_per_Tenure', 'Products_per_Tenure', 'Credit_Age_Ratio',\n",
    "                          'RiskScore', 'HasCrCard', 'IsActiveMember', 'IsHighValue', 'IsLongTerm', \n",
    "                          'HasZeroBalance', 'IsNewCustomer']\n",
    "        \n",
    "        categorical_features = ['Geography', 'Gender']\n",
    "        \n",
    "        self.preprocessor = ColumnTransformer([\n",
    "            ('numeric', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='median')),\n",
    "                ('scaler', StandardScaler())\n",
    "            ]), numeric_features),\n",
    "            ('categorical', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "                ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore'))\n",
    "            ]), categorical_features)\n",
    "        ])\n",
    "        \n",
    "        return self.preprocessor\n",
    "    \n",
    "    def create_model(self):\n",
    "        model = LGBMClassifier(\n",
    "            objective='binary',\n",
    "            boosting_type='goss',\n",
    "            random_state=self.random_state,\n",
    "            n_jobs=-1  # Use all available cores\n",
    "        )\n",
    "        \n",
    "        param_distributions = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [ 5, 10, 15],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'num_leaves': [31, 63, 127],\n",
    "            'min_child_samples': [5, 10, 20],\n",
    "            'subsample': [0.6, 0.8, 1.0],\n",
    "            'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "            'reg_alpha': [0, 0.1, 0.5],\n",
    "            'reg_lambda': [0, 0.1, 0.5]\n",
    "        }\n",
    "        \n",
    "        self.model = RandomizedSearchCV(\n",
    "            model,\n",
    "            param_distributions=param_distributions,\n",
    "            n_iter=30,  # Reduced iterations for faster tuning\n",
    "            cv=3,  # Reduced cross-validation folds\n",
    "            scoring='roc_auc',\n",
    "            n_jobs=-1,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Create features\n",
    "        X_enhanced = self.create_features(X)\n",
    "        \n",
    "        # Prepare and fit preprocessing pipeline\n",
    "        preprocessor = self.prepare_pipelines()\n",
    "        X_processed = preprocessor.fit_transform(X_enhanced)\n",
    "        \n",
    "        # Handle class imbalance\n",
    "        smote = SMOTE(random_state=self.random_state)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X_processed, y)\n",
    "        \n",
    "        # Create and fit model\n",
    "        model = self.create_model()\n",
    "        model.fit(X_resampled, y_resampled)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_enhanced = self.create_features(X)\n",
    "        X_processed = self.preprocessor.transform(X_enhanced)\n",
    "        return self.model.predict(X_processed)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X_enhanced = self.create_features(X)\n",
    "        X_processed = self.preprocessor.transform(X_enhanced)\n",
    "        return self.model.predict_proba(X_processed)\n",
    "\n",
    "# Load and prepare data\n",
    "train_df = pd.read_csv('../Datasets/train.csv')\n",
    "test_df = pd.read_csv('../Datasets/test.csv')\n",
    "\n",
    "# Drop unnecessary columns and handle missing values\n",
    "drop_cols = ['id', 'CustomerId', 'Surname']\n",
    "X = train_df.drop(drop_cols + ['Exited'], axis=1)\n",
    "y = train_df['Exited']\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Create and train model\n",
    "churn_predictor = ChurnPredictor(random_state=42)\n",
    "churn_predictor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_val_pred = churn_predictor.predict(X_val)\n",
    "y_val_pred_proba = churn_predictor.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Evaluate model\n",
    "print(\"Model Performance:\")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_val, y_val_pred_proba):.4f}\")\n",
    "print(f\"Accuracy: {accuracy_score(y_val, y_val_pred):.4f}\")\n",
    "print(f\"R² Score: {r2_score(y_val, y_val_pred):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Generate predictions for test set\n",
    "test_predictions = churn_predictor.predict_proba(test_df.drop(drop_cols, axis=1))[:, 1]\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'Exited': test_predictions\n",
    "})\n",
    "submission_df.to_csv('optimized_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, classification_report, r2_score\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "class ChurnPredictor:\n",
    "    def __init__(self, random_state=42):\n",
    "        self.random_state = random_state\n",
    "        self.model = None\n",
    "        self.preprocessor = None\n",
    "        \n",
    "    def create_features(self, df):\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Basic features\n",
    "        df['Balance_per_Product'] = df['Balance'] / (df['NumOfProducts'] + 1)\n",
    "        df['Balance_per_Age'] = df['Balance'] / (df['Age'] + 1)\n",
    "        df['Balance_per_Tenure'] = df['Balance'] / (df['Tenure'] + 1)\n",
    "        \n",
    "        # Advanced features\n",
    "        df['IsHighValue'] = ((df['Balance'] > df['Balance'].mean()) & \n",
    "                           (df['EstimatedSalary'] > df['EstimatedSalary'].mean())).astype(int)\n",
    "        df['IsLongTerm'] = (df['Tenure'] > df['Tenure'].median()).astype(int)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def prepare_pipelines(self):\n",
    "        numeric_features = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', \n",
    "                          'EstimatedSalary', 'Balance_per_Product', 'Balance_per_Age',\n",
    "                          'Balance_per_Tenure']\n",
    "        \n",
    "        categorical_features = ['Geography', 'Gender']\n",
    "        \n",
    "        self.numeric_pipeline = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('pca', PCA(n_components=0.95))\n",
    "        ])\n",
    "        \n",
    "        self.categorical_pipeline = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore'))\n",
    "        ])\n",
    "        \n",
    "        self.preprocessor = ColumnTransformer([\n",
    "            ('numeric', self.numeric_pipeline, numeric_features),\n",
    "            ('categorical', self.categorical_pipeline, categorical_features)\n",
    "        ])\n",
    "        \n",
    "        return self.preprocessor\n",
    "    \n",
    "    def create_model(self):\n",
    "        model = LGBMClassifier(\n",
    "            objective='binary',\n",
    "            boosting_type='goss',\n",
    "            random_state=self.random_state,\n",
    "            class_weight='balanced',  # Handle class imbalance\n",
    "            n_jobs=-1  # Use all available cores\n",
    "        )\n",
    "        \n",
    "        param_distributions = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [5, 10, 15],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'num_leaves': [31, 50, 70],\n",
    "        }\n",
    "        \n",
    "        self.model = RandomizedSearchCV(\n",
    "            model,\n",
    "            param_distributions=param_distributions,\n",
    "            n_iter=20,  # Reduced iterations for faster execution\n",
    "            cv=3,  # Reduced cross-validation folds\n",
    "            scoring='roc_auc',\n",
    "            n_jobs=-1,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Create features\n",
    "        X_enhanced = self.create_features(X)\n",
    "        \n",
    "        # Prepare and fit preprocessing pipeline\n",
    "        preprocessor = self.prepare_pipelines()\n",
    "        X_processed = preprocessor.fit_transform(X_enhanced)\n",
    "        \n",
    "        # Handle class imbalance\n",
    "        smote = SMOTE(random_state=self.random_state)\n",
    "        X_resampled, y_resampled = smote.fit_resample (X_processed, y)\n",
    "        \n",
    "        # Create and fit model\n",
    "        model = self.create_model()\n",
    "        model.fit(X_resampled, y_resampled)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_enhanced = self.create_features(X)\n",
    "        X_processed = self.preprocessor.transform(X_enhanced)\n",
    "        return self.model.predict(X_processed)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X_enhanced = self.create_features(X)\n",
    "        X_processed = self.preprocessor.transform(X_enhanced)\n",
    "        return self.model.predict_proba(X_processed)\n",
    "\n",
    "# Load and prepare data\n",
    "train_df = pd.read_csv('../Datasets/train.csv')\n",
    "test_df = pd.read_csv('../Datasets/test.csv')\n",
    "\n",
    "# Drop unnecessary columns and handle missing values\n",
    "drop_cols = ['id', 'CustomerId', 'Surname']\n",
    "X = train_df.drop(drop_cols + ['Exited'], axis=1)\n",
    "y = train_df['Exited']\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Create and train model\n",
    "churn_predictor = ChurnPredictor(random_state=42)\n",
    "churn_predictor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_val_pred = churn_predictor.predict(X_val)\n",
    "y_val_pred_proba = churn_predictor.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Evaluate model\n",
    "print(\"Model Performance:\")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_val, y_val_pred_proba):.4f}\")\n",
    "print(f\"Accuracy: {accuracy_score(y_val, y_val_pred):.4f}\")\n",
    "print(f\"R² Score: {r2_score(y_val, y_val_pred):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Generate predictions for test set\n",
    "test_predictions = churn_predictor.predict_proba(test_df.drop(drop_cols, axis=1))[:, 1]\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'Exited': test_predictions\n",
    "})\n",
    "submission_df.to_csv('optimized_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training data...\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "# from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
    "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier, ExtraTreesClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, classification_report, r2_score\n",
    "# from xgboost import XGBClassifier\n",
    "# from catboost import CatBoostClassifier\n",
    "# from lightgbm import LGBMClassifier\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.feature_selection import RFECV, VarianceThreshold\n",
    "# from sklearn.cluster import KMeans\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from optuna import create_study, Trial\n",
    "# from optuna.samplers import TPESampler\n",
    "\n",
    "# # =============================================================================================================\n",
    "# # =============================================================================================================\n",
    "\n",
    "# # File paths\n",
    "# INPUT_PATH = '../Datasets'\n",
    "# train_path = f\"{INPUT_PATH}/train.csv\"\n",
    "# test_path = f\"{INPUT_PATH}/test.csv\"\n",
    "\n",
    "# # Load datasets\n",
    "# train_df = pd.read_csv(train_path)\n",
    "# test_df = pd.read_csv(test_path)\n",
    "\n",
    "# # =============================================================================================================\n",
    "# # =============================================================================================================\n",
    "\n",
    "# class SafeLabelEncoder:\n",
    "#     def __init__(self, unknown_value=-1):\n",
    "#         self.unknown_value = unknown_value\n",
    "#         self.label_encoder = LabelEncoder()\n",
    "#         self.classes_ = None\n",
    "        \n",
    "#     def fit(self, series):\n",
    "#         series = pd.Series(series)\n",
    "#         unique_values = series.unique().tolist()\n",
    "#         if 'UNKNOWN' not in unique_values:\n",
    "#             unique_values.append('UNKNOWN')\n",
    "#         self.label_encoder.fit(unique_values)\n",
    "#         self.classes_ = self.label_encoder.classes_\n",
    "#         return self\n",
    "    \n",
    "#     def transform(self, series):\n",
    "#         series = pd.Series(series)\n",
    "#         series = series.map(lambda x: 'UNKNOWN' if x not in self.classes_ else x)\n",
    "#         return self.label_encoder.transform(series)\n",
    "    \n",
    "#     def fit_transform(self, series):\n",
    "#         return self.fit(series).transform(series)\n",
    "\n",
    "# # =============================================================================================================\n",
    "# # =============================================================================================================\n",
    "\n",
    "# def prepare_data(df, is_training=True):\n",
    "#     df_processed = df.copy()\n",
    "#     categorical_columns = df_processed.select_dtypes(include=['object']).columns\n",
    "#     for column in categorical_columns:\n",
    "#         if is_training:\n",
    "#             if column not in label_encoders:\n",
    "#                 label_encoders[column] = SafeLabelEncoder()\n",
    "#                 df_processed[column] = label_encoders[column].fit_transform(df_processed[column])\n",
    "#         else:\n",
    "#             if column in label_encoders:\n",
    "#                 df_processed[column] = label_encoders[column].transform(df_processed[column])\n",
    "\n",
    "#     # Log transformations\n",
    "#     if 'Balance' in df_processed.columns:\n",
    "#         df_processed['Balance_log'] = np.log1p(df_processed['Balance'].clip(lower=0))\n",
    "#     if 'EstimatedSalary' in df_processed.columns:\n",
    "#         df_processed['EstimatedSalary_log'] = np.log1p(df_processed['EstimatedSalary'].clip(lower=0))\n",
    "    \n",
    "#     # Interaction terms\n",
    "#     if 'Balance' in df_processed.columns and 'Age' in df_processed.columns:\n",
    "#         df_processed['Balance_Age'] = df_processed['Balance'] * df_processed['Age']\n",
    "#     if 'CreditScore' in df_processed.columns and 'Age' in df_processed.columns:\n",
    "#         df_processed['CreditScore_Age'] = df_processed['CreditScore'] * df_processed['Age']\n",
    "    \n",
    "#     # Additional Interaction Terms\n",
    "#     if 'EstimatedSalary' in df_processed.columns:\n",
    "#         df_processed['Balance_EstimatedSalary'] = df_processed['Balance'] * df_processed['EstimatedSalary']\n",
    "#         df_processed['CreditScore_EstimatedSalary'] = df_processed['CreditScore'] * df_processed['EstimatedSalary']\n",
    "    \n",
    "#     # Polynomial terms\n",
    "#     df_processed['CreditScore_sq'] = df_processed['CreditScore'] ** 2\n",
    "#     df_processed['Balance_sq'] = df_processed['Balance'] ** 2\n",
    "#     df_processed['Age_sq'] = df_processed['Age'] ** 2\n",
    "#     df_processed['EstimatedSalary_sq'] = df_processed['EstimatedSalary'] ** 2\n",
    "    \n",
    "#     # Ratios\n",
    "#     if 'Balance' in df_processed.columns and 'EstimatedSalary' in df_processed.columns:\n",
    "#         df_processed['Balance_to_EstimatedSalary'] = df_processed['Balance'] / (df_processed['EstimatedSalary'] + 1e-5)  # Avoid division by zero\n",
    "    \n",
    "#     # Clustering features\n",
    "#     kmeans = KMeans(n_clusters=5, random_state= 2)\n",
    "#     df_processed['cluster'] = kmeans.fit_predict(df_processed[['CreditScore', 'Age', 'Balance', 'EstimatedSalary']])\n",
    "    \n",
    "#     return df_processed\n",
    "\n",
    "# # Initialize label encoders dictionary\n",
    "# label_encoders = {}\n",
    "\n",
    "# # Process training data\n",
    "# print(\"Processing training data...\")\n",
    "# train_df_processed = prepare_data(train_df, is_training=True)\n",
    "# test_df_processed = prepare_data(test_df, is_training=False)\n",
    "\n",
    "# # Additional Feature Engineering\n",
    "# def additional_features(df):\n",
    "#     df['Age_Balance_Ratio'] = df['Age'] / (df['Balance'] + 1e-5)\n",
    "#     df['Age_EstimatedSalary_Ratio'] = df['Age'] / (df['EstimatedSalary'] + 1e-5)\n",
    "#     df['Balance_EstimatedSalary_Difference'] = df['Balance'] - df['EstimatedSalary']\n",
    "#     df['CreditScore_Balance_Ratio'] = df['CreditScore'] / (df['Balance'] + 1e-5)\n",
    "#     return df\n",
    "\n",
    "# # Process training data with additional features\n",
    "# train_df_processed = additional_features(train_df_processed)\n",
    "# test_df_processed = additional_features(test_df_processed)\n",
    "\n",
    "# # Define numeric features\n",
    "# numeric_features = ['CreditScore', 'Age', 'Balance', 'EstimatedSalary', \n",
    "#                     'Balance_log', 'EstimatedSalary_log', 'Balance_Age', \n",
    "#                     'CreditScore_Age', 'Balance_EstimatedSalary', \n",
    "#                     'CreditScore_EstimatedSalary', 'CreditScore_sq', \n",
    "#                     'Balance_sq', 'Age_sq', 'EstimatedSalary_sq', \n",
    "#                     'Balance_to_EstimatedSalary', 'Age_Balance_Ratio', \n",
    "#                     'Age_EstimatedSalary_Ratio', 'Balance_EstimatedSalary_Difference', \n",
    "#                     'CreditScore_Balance_Ratio']\n",
    "\n",
    "# # Scaling numeric features with RobustScaler (handles outliers better)\n",
    "# scaler = RobustScaler()\n",
    "# train_df_processed[numeric_features] = scaler.fit_transform(train_df_processed[numeric_features])\n",
    "# test_df_processed[numeric_features] = scaler.transform(test_df_processed[numeric_features])\n",
    "\n",
    "# # PCA for dimensionality reduction on the expanded feature set\n",
    "# pca = PCA(n_components=19)  # Decrease the number of components to 19\n",
    "# pca_features_train = pca.fit_transform(train_df_processed[numeric_features])\n",
    "# pca_df_train = pd.DataFrame(pca_features_train, columns=[f'pca_{i+1}' for i in range(pca_features_train.shape[1])])\n",
    "# train_df_processed = pd.concat([train_df_processed, pca_df_train], axis=1)\n",
    "\n",
    "# pca_features_test = pca.transform(test_df_processed[numeric_features])\n",
    "# pca_df_test = pd.DataFrame(pca_features_test, columns=[f'pca_{i+1}' for i in range(pca_features_test.shape[1])])\n",
    "# test_df_processed = pd.concat([test_df_processed, pca_df_test], axis=1)\n",
    "\n",
    "# # Prepare features and target\n",
    "# X = train_df_processed.drop(['id', 'Exited', 'CustomerId'], axis=1)\n",
    "# y = train_df_processed['Exited']\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=2, stratify=y)\n",
    "\n",
    "# # Handle class imbalance using SMOTE\n",
    "# smote = SMOTE(random_state=2)\n",
    "# X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# # Feature selection using RFECV with a stronger base estimator (e.g., GradientBoostingClassifier)\n",
    "# rfecv = RFECV(estimator=GradientBoostingClassifier(), step=1, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "# X_train_selected = rfecv.fit_transform(X_train_res, y_train_res)\n",
    "# X_val_selected = rfecv.transform(X_val)\n",
    "\n",
    "# # Convert the numpy array back to DataFrame with correct column names\n",
    "# X_train_selected_df = pd.DataFrame(X_train_selected, columns=X_train.columns[rfecv.get_support()])\n",
    "# X_val_selected_df = pd.DataFrame(X_val_selected, columns=X_val.columns[rfecv.get_support()])\n",
    "# X_test_selected_df = pd.DataFrame(rfecv.transform(test_df_processed.drop(['CustomerId'], axis=1)), columns=test_df_processed.columns[rfecv.get_support()])\n",
    "\n",
    "# # Hyperparameter optimization with Optuna for multiple models\n",
    "# def objective(trial: Trial):\n",
    "#     model_type = trial.suggest_categorical('model_type', ['lgbm', 'xgb', 'rf'])\n",
    "    \n",
    "#     if model_type == 'lgbm':\n",
    "#         param = {\n",
    "#             'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "#             'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "#             'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3)\n",
    "#         }\n",
    "#         model = LGBMClassifier(**param, random_state=2)\n",
    "    \n",
    "#     elif model_type == 'xgb':\n",
    "#         param = {\n",
    "#             'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "#             'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "#             'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3)\n",
    "#         }\n",
    "#         model = XGBClassifier(**param, random_state=2, use_label_encoder=False, eval_metric='logloss')\n",
    "    \n",
    "#     else:  # RandomForestClassifier\n",
    "#         param = {\n",
    "#             'n_estimators': trial.suggest_int('n_estimators', 100, 700),\n",
    "#             'max_depth': trial.suggest_int('max_depth', 3, 15)\n",
    "#         }\n",
    "#         model = RandomForestClassifier(**param, random_state=2)\n",
    "    \n",
    "#     model.fit(X_train_selected_df, y_train_res)\n",
    "#     y_pred = model.predict(X_val_selected_df)\n",
    "#     return accuracy_score(y_val, y_pred)\n",
    "\n",
    "# # Create an Optuna study and optimize\n",
    "# study = create_study(direction='maximize', sampler=TPESampler())\n",
    "# study.optimize(objective, n_trials=20)\n",
    "\n",
    "# # Train the best model based on the best hyperparameters\n",
    "# best_model_type = study.best_params['model_type']\n",
    "# if best_model_type == 'lgbm':\n",
    "#     best_model = LGBMClassifier(n_estimators=study.best_params['n_estimators'],\n",
    "#                                 max_depth=study.best_params['max_depth'],\n",
    "#                                 learning_rate=study.best_params['learning_rate'], \n",
    "#                                 random_state=2)\n",
    "# elif best_model_type == 'xgb':\n",
    "#     best_model = XGBClassifier(n_estimators=study.best_params['n_estimators'],\n",
    "#                                max_depth=study.best_params['max_depth'],\n",
    "#                                learning_rate=study.best_params['learning_rate'],\n",
    "#                                random_state=2, use_label_encoder=False, eval_metric='logloss')\n",
    "# else:\n",
    "#     best_model = RandomForestClassifier(n_estimators=study.best_params['n_estimators'],\n",
    "#                                         max_depth=study.best_params['max_depth'],\n",
    "#                                         random_state=2)\n",
    "\n",
    "# best_model.fit(X_train_selected_df, y_train_res)\n",
    "\n",
    "# # Evaluate model\n",
    "# y_pred = best_model.predict(X_val_selected_df)\n",
    "# y_pred_prob = best_model.predict_proba(X_val_selected_df)[:, 1]\n",
    "\n",
    "# print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "# print(\"ROC AUC Score:\", roc_auc_score(y_val, y_pred_prob))\n",
    "# print(\"R² Score:\", r2_score(y_val, y_pred_prob))  # Add R² score for evaluation\n",
    "# print(\"Classification Report:\\n\", classification_report(y_val, y_pred))\n",
    "\n",
    "# # Plot feature importance of the best model\n",
    "# plt.barh(X_train_selected_df.columns, best_model.feature_importances_)\n",
    "# plt.title(f'Feature Importance for {best_model_type.upper()} Model')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split, cross_val_score\n",
    "# from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "# from sklearn.metrics import roc_auc_score, accuracy_score, classification_report, r2_score\n",
    "# from xgboost import XGBClassifier\n",
    "# from lightgbm import LGBMClassifier\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from category_encoders import TargetEncoder\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# from scipy.stats import randint, uniform\n",
    "\n",
    "# class ChurnPredictor:\n",
    "#     def __init__(self, random_state=42):\n",
    "#         self.random_state = random_state\n",
    "#         self.target_encoders = {}\n",
    "#         self.numeric_pipeline = None\n",
    "#         self.categorical_pipeline = None\n",
    "#         self.feature_names = None\n",
    "#         self.model = None\n",
    "        \n",
    "#     def create_features(self, df):\n",
    "#         df = df.copy()\n",
    "        \n",
    "#         # Basic features\n",
    "#         df['Balance_per_Product'] = df['Balance'] / (df['NumOfProducts'] + 1)\n",
    "#         df['Balance_per_Age'] = df['Balance'] / (df['Age'] + 1)\n",
    "#         df['Balance_per_Tenure'] = df['Balance'] / (df['Tenure'] + 1)\n",
    "#         df['Products_per_Tenure'] = df['NumOfProducts'] / (df['Tenure'] + 1)\n",
    "        \n",
    "#         # Interaction features\n",
    "#         df['Credit_Age_Ratio'] = df['CreditScore'] / (df['Age'] + 1)\n",
    "#         df['Tenure_Age_Ratio'] = df['Tenure'] / (df['Age'] + 1)\n",
    "#         df['Balance_Salary_Ratio'] = df['Balance'] / (df['EstimatedSalary'] + 1)\n",
    "        \n",
    "#         # Advanced features\n",
    "#         df['IsHighValue'] = (df['Balance'] > df['Balance'].mean()) & (df['EstimatedSalary'] > df['EstimatedSalary'].mean())\n",
    "#         df['IsLongTerm'] = df['Tenure'] > df['Tenure'].median()\n",
    "#         df['IsYoungActive'] = (df['Age'] < 40) & (df['IsActiveMember'] == 1)\n",
    "        \n",
    "#         # Polynomial features\n",
    "#         df['CreditScore_Squared'] = df['CreditScore'] ** 2\n",
    "#         df['Age_Squared'] = df['Age'] ** 2\n",
    "#         df['Balance_Squared'] = df['Balance'] ** 2\n",
    "#         df['Tenure_Squared'] = df['Tenure'] ** 2\n",
    "        \n",
    "#         # Risk score feature\n",
    "#         df['RiskScore'] = (\n",
    "#             (df['CreditScore'] < 600) * 2 +\n",
    "#             (df['Balance'] > 100000) * 1.5 +\n",
    "#             (df['Age'] < 30) * 1.2 +\n",
    "#             (df['IsActiveMember'] == 0) * 1.8 +\n",
    "#             (df['NumOfProducts'] > 2) * 1.3\n",
    "#         )\n",
    "        \n",
    "#         return df\n",
    "    \n",
    "#     def prepare_pipelines(self):\n",
    "#         numeric_features = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', \n",
    "#                           'EstimatedSalary', 'Balance_per_Product', 'Balance_per_Age',\n",
    "#                           'Balance_per_Tenure', 'Products_per_Tenure', 'Credit_Age_Ratio',\n",
    "#                           'Tenure_Age_Ratio', 'Balance_Salary_Ratio', 'CreditScore_Squared',\n",
    "#                           'Age_Squared', 'Balance_Squared', 'Tenure_Squared', 'RiskScore']\n",
    "        \n",
    "#         categorical_features = ['Geography', 'Gender']\n",
    "        \n",
    "#         self.numeric_pipeline = Pipeline([\n",
    "#             ('imputer', SimpleImputer(strategy='median')),\n",
    "#             ('scaler', StandardScaler()),\n",
    "#             ('pca', PCA(n_components=0.95))\n",
    "#         ])\n",
    "        \n",
    "#         self.categorical_pipeline = Pipeline([\n",
    "#             ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "#             ('target_encoder', TargetEncoder(random_state=self.random_state))\n",
    "#         ])\n",
    "        \n",
    "#         self.feature_names = numeric_features + categorical_features\n",
    "        \n",
    "#         return ColumnTransformer([\n",
    "#             ('numeric', self.numeric_pipeline, numeric_features),\n",
    "#             ('categorical', self.categorical_pipeline, categorical_features)\n",
    "#         ])\n",
    "    \n",
    "#     def create_model(self):\n",
    "#         base_models = [\n",
    "#             ('rf', RandomForestClassifier(random_state=self.random_state)),\n",
    "#             ('xgb', XGBClassifier(random_state=self.random_state)),\n",
    "#             ('lgbm', LGBMClassifier(random_state=self.random_state)),\n",
    "#             ('gb', GradientBoostingClassifier(random_state=self.random_state))\n",
    "#         ]\n",
    "        \n",
    "#         final_model = LGBMClassifier(random_state=self.random_state)\n",
    "        \n",
    "#         # Hyperparameter space\n",
    "#         param_distributions = {\n",
    "#             'n_estimators': randint(100, 1000),\n",
    "#             'max_depth': randint(3, 15),\n",
    "#             'learning_rate': uniform(0.01, 0.29),\n",
    "#             'num_leaves': randint(20, 100),\n",
    "#             'min_child_samples': randint(10, 50),\n",
    "#             'subsample': uniform(0.6, 0.4),\n",
    "#             'colsample_bytree': uniform(0.6, 0.4)\n",
    "#         }\n",
    "        \n",
    "#         self.model = RandomizedSearchCV(\n",
    "#             final_model,\n",
    "#             param_distributions=param_distributions,\n",
    "#             n_iter=50,\n",
    "#             cv=5,\n",
    "#             scoring='roc_auc',\n",
    "#             n_jobs=-1,\n",
    "#             random_state=self.random_state\n",
    "#         )\n",
    "        \n",
    "#         return self.model\n",
    "    \n",
    "#     def fit(self, X, y):\n",
    "#         # Create features\n",
    "#         X_enhanced = self.create_features(X)\n",
    "        \n",
    "#         # Prepare and fit preprocessing pipeline\n",
    "#         preprocessor = self.prepare_pipelines()\n",
    "#         X_processed = preprocessor.fit_transform(X_enhanced[self.feature_names], y)\n",
    "        \n",
    "#         # Create and fit model\n",
    "#         model = self.create_model()\n",
    "#         model.fit(X_processed, y)\n",
    "        \n",
    "#         return self\n",
    "    \n",
    "#     def predict(self, X):\n",
    "#         X_enhanced = self.create_features(X)\n",
    "#         X_processed = self.preprocessor.transform(X_enhanced[self.feature_names])\n",
    "#         return self.model.predict(X_processed)\n",
    "    \n",
    "#     def predict_proba(self, X):\n",
    "#         X_enhanced = self.create_features(X)\n",
    "#         X_processed = self.preprocessor.transform(X_enhanced[self.feature_names])\n",
    "#         return self.model.predict_proba(X_processed)\n",
    "\n",
    "# # Load and prepare data\n",
    "# train_df = pd.read_csv('../Datasets/train.csv')\n",
    "# test_df = pd.read_csv('../Datasets/test.csv')\n",
    "\n",
    "# # Drop unnecessary columns and handle missing values\n",
    "# drop_cols = ['id', 'CustomerId', 'Surname']\n",
    "# X = train_df.drop(drop_cols + ['Exited'], axis=1)\n",
    "# y = train_df['Exited']\n",
    "\n",
    "# # Split data\n",
    "# X_train, X_val, y_train, y_val = train_test_split(\n",
    "#     X, y, test_size=0.2, random_state=42, stratify=y\n",
    "# )\n",
    "\n",
    "# # Create and train model\n",
    "# churn_predictor = ChurnPredictor(random_state=42)\n",
    "# churn_predictor.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions\n",
    "# y_val_pred = churn_predictor.predict(X_val)\n",
    "# y_val_pred_proba = churn_predictor.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# # Evaluate model\n",
    "# print(\"Model Performance:\")\n",
    "# print(f\"ROC AUC: {roc_auc_score(y_val, y_val_pred_proba):.4f}\")\n",
    "# print(f\"Accuracy: {accuracy_score(y_val, y_val_pred):.4f}\")\n",
    "# print(f\"R² Score: {r2_score(y_val, y_val_pred):.4f}\")\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "# # Generate predictions for test set\n",
    "# test_predictions = churn_predictor.predict_proba(test_df.drop(drop_cols, axis=1))[:, 1]\n",
    "# submission_df = pd.DataFrame({\n",
    "#     'id': test_df['id'],\n",
    "#     'Exited': test_predictions\n",
    "# })\n",
    "# submission_df.to_csv('improved_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "# from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
    "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier, ExtraTreesClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, classification_report, r2_score\n",
    "# from xgboost import XGBClassifier\n",
    "# from catboost import CatBoostClassifier\n",
    "# from lightgbm import LGBMClassifier\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.feature_selection import RFECV\n",
    "# from sklearn.cluster import KMeans\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from optuna import create_study, Trial\n",
    "# from optuna.samplers import TPESampler\n",
    "\n",
    "# # File paths\n",
    "# INPUT_PATH = '../Datasets'\n",
    "# train_path = f\"{INPUT_PATH}/train.csv\"\n",
    "# test_path = f\"{INPUT_PATH}/test.csv\"\n",
    "\n",
    "# # Load datasets\n",
    "# train_df = pd.read_csv(train_path)\n",
    "# test_df = pd.read_csv(test_path)\n",
    "\n",
    "# class SafeLabelEncoder:\n",
    "#     def __init__(self, unknown_value=-1):\n",
    "#         self.unknown_value = unknown_value\n",
    "#         self.label_encoder = LabelEncoder()\n",
    "#         self.classes_ = None\n",
    "        \n",
    "#     def fit(self, series):\n",
    "#         series = pd.Series(series)\n",
    "#         unique_values = series.unique().tolist()\n",
    "#         if 'UNKNOWN' not in unique_values:\n",
    "#             unique_values.append('UNKNOWN')\n",
    "#         self.label_encoder.fit(unique_values)\n",
    "#         self.classes_ = self.label_encoder.classes_\n",
    "#         return self\n",
    "    \n",
    "#     def transform(self, series):\n",
    "#         series = pd.Series(series)\n",
    "#         series = series.map(lambda x: 'UNKNOWN' if x not in self.classes_ else x)\n",
    "#         return self.label_encoder.transform(series)\n",
    "    \n",
    "#     def fit_transform(self, series):\n",
    "#         return self.fit(series).transform(series)\n",
    "\n",
    "# def prepare_data(df, is_training=True):\n",
    "#     df_processed = df.copy()\n",
    "#     categorical_columns = df_processed.select_dtypes(include=['object']).columns\n",
    "#     for column in categorical_columns:\n",
    "#         if is_training:\n",
    "#             if column not in label_encoders:\n",
    "#                 label_encoders[column] = SafeLabelEncoder()\n",
    "#                 df_processed[column] = label_encoders[column].fit_transform(df_processed[column])\n",
    "#         else:\n",
    "#             if column in label_encoders:\n",
    "#                 df_processed[column] = label_encoders[column].transform(df_processed[column])\n",
    "\n",
    "#     # Log transformations\n",
    "#     if 'Balance' in df_processed.columns:\n",
    "#         df_processed['Balance_log'] = np.log1p(df_processed['Balance'].clip(lower=0))\n",
    "#     if 'EstimatedSalary' in df_processed.columns:\n",
    "#         df_processed['EstimatedSalary_log'] = np.log1p(df_processed['EstimatedSalary'].clip(lower=0))\n",
    "    \n",
    "#     # Interaction terms\n",
    "#     if 'Balance' in df_processed.columns and 'Age' in df_processed.columns:\n",
    "#         df_processed['Balance_Age'] = df_processed['Balance'] * df_processed['Age']\n",
    "#     if 'CreditScore' in df_processed.columns and 'Age' in df_processed.columns:\n",
    "#         df_processed['CreditScore_Age'] = df_processed['CreditScore'] * df_processed['Age']\n",
    "    \n",
    "#     # Additional Interaction Terms\n",
    "#     if 'EstimatedSalary' in df_processed.columns:\n",
    "#         df_processed['Balance_EstimatedSalary'] = df_processed['Balance'] * df_processed['EstimatedSalary']\n",
    "#         df_processed['CreditScore_EstimatedSalary'] = df_processed['CreditScore'] * df_processed['EstimatedSalary']\n",
    "    \n",
    "#     # Polynomial terms\n",
    "#     df_processed['CreditScore_sq'] = df_processed['CreditScore'] ** 2\n",
    "#     df_processed['Balance_sq'] = df_processed['Balance'] ** 2\n",
    "#     df_processed['Age_sq'] = df_processed['Age'] ** 2\n",
    "#     df_processed['EstimatedSalary_sq'] = df_processed['EstimatedSalary'] ** 2\n",
    "    \n",
    "#     # Ratios\n",
    "#     if 'Balance' in df_processed.columns and 'EstimatedSalary' in df_processed.columns:\n",
    "#         df_processed['Balance_to_EstimatedSalary'] = df_processed['Balance'] / (df_processed['EstimatedSalary'] + 1e-5)  # Avoid division by zero\n",
    "    \n",
    "#     # Clustering features\n",
    "#     kmeans = KMeans(n_clusters=5, random_state=2)\n",
    "#     df_processed['cluster'] = kmeans.fit_predict(df_processed[['CreditScore', 'Age', 'Balance', 'EstimatedSalary']])\n",
    "    \n",
    "#     return df_processed\n",
    "\n",
    "# # Initialize label encoders dictionary\n",
    "# label_encoders = {}\n",
    "\n",
    "# # Process training data\n",
    "# print(\"Processing training data...\")\n",
    "# train_df_processed = prepare_data(train_df, is_training=True)\n",
    "# test_df_processed = prepare_data(test_df, is_training=False)\n",
    "\n",
    "# # Define numeric features\n",
    "# numeric_features = ['CreditScore', 'Age', 'Balance', 'EstimatedSalary', \n",
    "#                     'Balance_log', 'EstimatedSalary_log', 'Balance_Age', \n",
    "#                     'CreditScore_Age', 'Balance_EstimatedSalary', \n",
    "#                     'CreditScore_EstimatedSalary', 'CreditScore_sq', \n",
    "#                     'Balance_sq', 'Age_sq', 'EstimatedSalary_sq', \n",
    "#                     'Balance_to_EstimatedSalary']\n",
    "\n",
    "# # Scaling numeric features with RobustScaler (handles outliers better)\n",
    "# scaler = RobustScaler()\n",
    "# train_df_processed[numeric_features] = scaler.fit_transform(train_df_processed[numeric_features])\n",
    "# test_df_processed[numeric_features] = scaler.transform(test_df_processed[numeric_features])\n",
    "\n",
    "# # PCA for dimensionality reduction\n",
    "# # PCA for dimensionality reduction\n",
    "# pca = PCA(n_components=15)  # Set n_components to 15 or less\n",
    "# pca_features_train = pca.fit_transform(train_df_processed[numeric_features])\n",
    "# pca_df_train = pd.DataFrame(pca_features_train, columns=[f'pca_{i+1}' for i in range(pca_features_train.shape[1])])\n",
    "# train_df_processed = pd.concat([train_df_processed, pca_df_train], axis=1)\n",
    "\n",
    "\n",
    "# pca_features_test = pca.transform(test_df_processed[numeric_features])\n",
    "# pca_df_test = pd.DataFrame(pca_features_test, columns=[f'pca_{i+1}' for i in range(pca_features_test.shape[1])])\n",
    "# test_df_processed = pd.concat([test_df_processed, pca_df_test], axis=1)\n",
    "\n",
    "# # Prepare features and target\n",
    "# X = train_df_processed.drop(['Exited', 'CustomerId'], axis=1)\n",
    "# y = train_df_processed['Exited']\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=2, stratify=y)\n",
    "\n",
    "# # Handle class imbalance using SMOTE\n",
    "# smote = SMOTE(random_state=2)\n",
    "# X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# # Feature selection using RFECV with a stronger base estimator (e.g., GradientBoostingClassifier)\n",
    "# rfecv = RFECV(estimator=GradientBoostingClassifier(), step=1, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "# X_train_selected = rfecv.fit_transform(X_train_res, y_train_res)\n",
    "# X_val_selected = rfecv.transform(X_val)\n",
    "\n",
    "# # Transform the test set using the same RFECV mask\n",
    "# X_test_selected = rfecv.transform(test_df_processed.drop(['CustomerId'], axis=1))\n",
    "\n",
    "# # Hyperparameter optimization with Optuna (Bayesian Optimization) for LightGBM\n",
    "# def objective(trial: Trial):\n",
    "#     param = {\n",
    "#         'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "#         'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3)\n",
    "#     }\n",
    "#     model = LGBMClassifier(**param, random_state=2)\n",
    "#     model.fit(X_train_selected, y_train_res)\n",
    "#     preds = model.predict_proba(X_val_selected)[:, 1]\n",
    "#     return roc_auc_score(y_val, preds)\n",
    "\n",
    "# study = create_study(direction='maximize', sampler=TPESampler())\n",
    "# study.optimize(objective, n_trials=50)\n",
    "\n",
    "# best_lgbm = LGBMClassifier(**study.best_params, random_state=2)\n",
    "# best_lgbm.fit(X_train_selected, y_train_res)\n",
    "\n",
    "# # Stacking model with GradientBoostingClassifier as final estimator for better predictive power\n",
    "# print(\"Training Stacking Model...\")\n",
    "# stacking_model = StackingClassifier(\n",
    "#     estimators=[\n",
    "#         ('rf', RandomForestClassifier(n_estimators=200, max_depth=10, random_state=2, class_weight='balanced')),\n",
    "#         ('gb', GradientBoostingClassifier(n_estimators=120, learning_rate=0.05, max_depth=4, random_state=2)),\n",
    "#         ('xgb', XGBClassifier(n_estimators=120, learning_rate=0.05, max_depth=4, random_state=2, use_label_encoder=False, eval_metric='logloss')),\n",
    "#         ('catboost', CatBoostClassifier(iterations=200, depth=6, learning_rate=0.05, silent=True)),\n",
    "#         ('lgbm', best_lgbm),\n",
    "#         ('extra', ExtraTreesClassifier(n_estimators=200, max_depth=10, random_state=2))\n",
    "#     ],\n",
    "#     final_estimator=GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3),\n",
    "#     cv=5\n",
    "# )\n",
    "# stacking_model.fit(X_train_selected, y_train_res)\n",
    "\n",
    "# # Model evaluation\n",
    "# print(\"Evaluating model...\")\n",
    "# y_val_pred_proba = stacking_model.predict_proba(X_val_selected)[:, 1]\n",
    "# roc_auc = roc_auc_score(y_val, y_val_pred_proba)\n",
    "# accuracy = accuracy_score(y_val, stacking_model.predict(X_val_selected))\n",
    "# r2 = r2_score(y_val, stacking_model.predict(X_val_selected))\n",
    "\n",
    "# print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "# print(f\"Accuracy: {accuracy:.4f}\")\n",
    "# print(f\"R² Score: {r2:.4f}\")\n",
    "# print(classification_report(y_val, stacking_model.predict(X_val_selected)))\n",
    "\n",
    "# # Make predictions on the test set\n",
    "# print(\"Making predictions on test set...\")\n",
    "# test_preds = stacking_model.predict_proba(X_test_selected)[:, 1]\n",
    "# submission_df = pd.DataFrame({'id': test_df['id'], 'Exited': test_preds})\n",
    "# submission_df.to_csv('customer_exit_predictions.csv', index=False)\n",
    "\n",
    "# print(\"Submission file created: customer_exit_predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "# from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
    "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier, ExtraTreesClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, classification_report, r2_score\n",
    "# from xgboost import XGBClassifier\n",
    "# from catboost import CatBoostClassifier\n",
    "# from lightgbm import LGBMClassifier\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.feature_selection import RFECV, VarianceThreshold\n",
    "# from sklearn.cluster import KMeans\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from optuna import create_study, Trial\n",
    "# from optuna.samplers import TPESampler\n",
    "\n",
    "# # File paths\n",
    "# INPUT_PATH = '../Datasets'\n",
    "# train_path = f\"{INPUT_PATH}/train.csv\"\n",
    "# test_path = f\"{INPUT_PATH}/test.csv\"\n",
    "\n",
    "# # Load datasets\n",
    "# train_df = pd.read_csv(train_path)\n",
    "# test_df = pd.read_csv(test_path)\n",
    "\n",
    "# class SafeLabelEncoder:\n",
    "#     def __init__(self, unknown_value=-1):\n",
    "#         self.unknown_value = unknown_value\n",
    "#         self.label_encoder = LabelEncoder()\n",
    "#         self.classes_ = None\n",
    "        \n",
    "#     def fit(self, series):\n",
    "#         series = pd.Series(series)\n",
    "#         unique_values = series.unique().tolist()\n",
    "#         if 'UNKNOWN' not in unique_values:\n",
    "#             unique_values.append('UNKNOWN')\n",
    "#         self.label_encoder.fit(unique_values)\n",
    "#         self.classes_ = self.label_encoder.classes_\n",
    "#         return self\n",
    "    \n",
    "#     def transform(self, series):\n",
    "#         series = pd.Series(series)\n",
    "#         series = series.map(lambda x: 'UNKNOWN' if x not in self.classes_ else x)\n",
    "#         return self.label_encoder.transform(series)\n",
    "    \n",
    "#     def fit_transform(self, series):\n",
    "#         return self.fit(series).transform(series)\n",
    "\n",
    "# def prepare_data(df, is_training=True):\n",
    "#     df_processed = df.copy()\n",
    "#     categorical_columns = df_processed.select_dtypes(include=['object']).columns\n",
    "#     for column in categorical_columns:\n",
    "#         if is_training:\n",
    "#             if column not in label_encoders:\n",
    "#                 label_encoders[column] = SafeLabelEncoder()\n",
    "#                 df_processed[column] = label_encoders[column].fit_transform(df_processed[column])\n",
    "#         else:\n",
    "#             if column in label_encoders:\n",
    "#                 df_processed[column] = label_encoders[column].transform(df_processed[column])\n",
    "\n",
    "#     # Log transformations\n",
    "#     if 'Balance' in df_processed.columns:\n",
    "#         df_processed['Balance_log'] = np.log1p(df_processed['Balance'].clip(lower=0))\n",
    "#     if 'EstimatedSalary' in df_processed.columns:\n",
    "#         df_processed['EstimatedSalary_log'] = np.log1p(df_processed['EstimatedSalary'].clip(lower=0))\n",
    "    \n",
    "#     # Interaction terms\n",
    "#     if 'Balance' in df_processed.columns and 'Age' in df_processed.columns:\n",
    "#         df_processed['Balance_Age'] = df_processed['Balance'] * df_processed['Age']\n",
    "#     if 'CreditScore' in df_processed.columns and 'Age' in df_processed.columns:\n",
    "#         df_processed['CreditScore_Age'] = df_processed['CreditScore'] * df_processed['Age']\n",
    "    \n",
    "#     # Additional Interaction Terms\n",
    "#     if 'EstimatedSalary' in df_processed.columns:\n",
    "#         df_processed['Balance_EstimatedSalary'] = df_processed['Balance'] * df_processed['EstimatedSalary']\n",
    "#         df_processed['CreditScore_EstimatedSalary'] = df_processed['CreditScore'] * df_processed['EstimatedSalary']\n",
    "    \n",
    "#     # Polynomial terms\n",
    "#     df_processed['CreditScore_sq'] = df_processed['CreditScore'] ** 2\n",
    "#     df_processed['Balance_sq'] = df_processed['Balance'] ** 2\n",
    "#     df_processed['Age_sq'] = df_processed['Age'] ** 2\n",
    "#     df_processed['EstimatedSalary_sq'] = df_processed['EstimatedSalary'] ** 2\n",
    "    \n",
    "#     # Ratios\n",
    "#     if 'Balance' in df_processed.columns and 'EstimatedSalary' in df_processed.columns:\n",
    "#         df_processed['Balance_to_EstimatedSalary'] = df_processed['Balance'] / (df_processed['EstimatedSalary'] + 1e-5)  # Avoid division by zero\n",
    "    \n",
    "#     # Clustering features\n",
    "#     kmeans = KMeans(n_clusters=5, random_state=2)\n",
    "#     df_processed['cluster'] = kmeans.fit_predict(df_processed[['CreditScore', 'Age', 'Balance', 'EstimatedSalary']])\n",
    "    \n",
    "#     return df_processed\n",
    "\n",
    "# # Initialize label encoders dictionary\n",
    "# label_encoders = {}\n",
    "\n",
    "# # Process training data\n",
    "# print(\"Processing training data...\")\n",
    "# train_df_processed = prepare_data(train_df, is_training=True)\n",
    "# test_df_processed = prepare_data(test_df, is_training=False)\n",
    "\n",
    "# # Define numeric features\n",
    "# numeric_features = ['CreditScore', 'Age', 'Balance', 'EstimatedSalary', \n",
    "#                     'Balance_log', 'EstimatedSalary_log', 'Balance_Age', \n",
    "#                     'CreditScore_Age', 'Balance_EstimatedSalary', \n",
    "#                     'CreditScore_EstimatedSalary', 'CreditScore_sq', \n",
    "#                     'Balance_sq', 'Age_sq', 'EstimatedSalary_sq', \n",
    "#                     'Balance_to_EstimatedSalary']\n",
    "\n",
    "# # Scaling numeric features with RobustScaler (handles outliers better)\n",
    "# scaler = RobustScaler()\n",
    "# train_df_processed[numeric_features] = scaler.fit_transform(train_df_processed[numeric_features])\n",
    "# test_df_processed[numeric_features] = scaler.transform(test_df_processed[numeric_features])\n",
    "\n",
    "# # PCA for dimensionality reduction\n",
    "# pca = PCA(n_components=15)  # Set n_components to 15 or less\n",
    "# pca_features_train = pca.fit_transform(train_df_processed[numeric_features])\n",
    "# pca_df_train = pd.DataFrame(pca_features_train, columns=[f'pca_{i+1}' for i in range(pca_features_train.shape[1])])\n",
    "# train_df_processed = pd.concat([train_df_processed, pca_df_train], axis=1)\n",
    "\n",
    "# pca_features_test = pca.transform(test_df_processed[numeric_features])\n",
    "# pca_df_test = pd.DataFrame(pca_features_test, columns=[f'pca_{i+1}' for i in range(pca_features_test.shape[1])])\n",
    "# test_df_processed = pd.concat([test_df_processed, pca_df_test], axis=1)\n",
    "\n",
    "# # Prepare features and target\n",
    "# X = train_df_processed.drop(['Exited', 'CustomerId'], axis=1)\n",
    "# y = train_df_processed['Exited']\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=2, stratify=y)\n",
    "\n",
    "# # Handle class imbalance using SMOTE\n",
    "# smote = SMOTE(random_state=2)\n",
    "# X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# # Feature selection using RFECV with a stronger base estimator (e.g., GradientBoostingClassifier)\n",
    "# rfecv = RFECV(estimator=GradientBoostingClassifier(), step=1, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "# X_train_selected = rfecv.fit_transform(X_train_res, y_train_res)\n",
    "# X_val_selected = rfecv.transform(X_val)\n",
    "\n",
    "# # Convert the numpy array back to DataFrame with correct column names\n",
    "# X_train_selected_df = pd.DataFrame(X_train_selected, columns=X_train.columns[rfecv.get_support()])\n",
    "# X_val_selected_df = pd.DataFrame(X_val_selected, columns=X_val.columns[rfecv.get_support()])\n",
    "# X_test_selected_df = pd.DataFrame(rfecv.transform(test_df_processed.drop(['CustomerId'], axis=1)), columns=test_df_processed.columns[rfecv.get_support()])\n",
    "\n",
    "# # Hyperparameter optimization with Optuna (Bayesian Optimization) for LightGBM\n",
    "# def objective(trial: Trial):\n",
    "#     param = {\n",
    "#         'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "#         'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3)\n",
    "#     }\n",
    "#     model = LGBMClassifier(**param, random_state=2)\n",
    "#     model.fit(X_train_selected_df, y_train_res)\n",
    "#     preds = model.predict_proba(X_val_selected_df)[:, 1]\n",
    "#     return roc_auc_score(y_val, preds)\n",
    "\n",
    "# study = create_study(direction='maximize', sampler=TPESampler())\n",
    "# study.optimize(objective, n_trials=50)\n",
    "\n",
    "# best_lgbm = LGBMClassifier(**study.best_params, random_state=2)\n",
    "# best_lgbm.fit(X_train_selected_df, y_train_res)\n",
    "\n",
    "# # Stacking model with GradientBoostingClassifier as final estimator for better predictive power\n",
    "# print(\"Training Stacking Model...\")\n",
    "# stacking_model = StackingClassifier(\n",
    "#     estimators=[ \n",
    "#         ('rf', RandomForestClassifier(n_estimators=200, max_depth=10, random_state=2, class_weight='balanced')),\n",
    "#         ('gb', GradientBoostingClassifier(n_estimators=120, learning_rate=0.05, max_depth=4, random_state=2)),\n",
    "#         ('xgb', XGBClassifier(n_estimators=120, learning_rate=0.05, max_depth=4, random_state=2, use_label_encoder=False, eval_metric='logloss')),\n",
    "#         ('catboost', CatBoostClassifier(iterations=200, depth=6, learning_rate=0.05, silent=True)),\n",
    "#         ('lgbm', best_lgbm),\n",
    "#         ('extra', ExtraTreesClassifier(n_estimators=200, max_depth=10, random_state=2))\n",
    "#     ],\n",
    "#     final_estimator=GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3),\n",
    "#     cv=5\n",
    "# )\n",
    "# stacking_model.fit(X_train_selected_df, y_train_res)\n",
    "\n",
    "# # Evaluate model\n",
    "# y_pred = stacking_model.predict(X_val_selected_df)\n",
    "# y_pred_prob = stacking_model.predict_proba(X_val_selected_df)[:, 1]\n",
    "\n",
    "# print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "# print(\"ROC AUC Score:\", roc_auc_score(y_val, y_pred_prob))\n",
    "# print(\"Classification Report:\\n\", classification_report(y_val, y_pred))\n",
    "\n",
    "# # Feature Importance Plot for LightGBM model\n",
    "# importance = stacking_model.named_estimators_['lgbm'].feature_importances_\n",
    "# plt.barh(X_train_selected_df.columns, importance)\n",
    "# plt.title('Feature Importance')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "# from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import roc_auc_score, r2_score\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# # File paths\n",
    "# INPUT_PATH = '../Datasets'\n",
    "# train_path = f\"{INPUT_PATH}/train.csv\"\n",
    "# test_path = f\"{INPUT_PATH}/test.csv\"\n",
    "\n",
    "# # Load datasets\n",
    "# train_df = pd.read_csv(train_path)\n",
    "# test_df = pd.read_csv(test_path)\n",
    "\n",
    "# class SafeLabelEncoder:\n",
    "#     def __init__(self, unknown_value=-1):\n",
    "#         self.unknown_value = unknown_value\n",
    "#         self.label_encoder = LabelEncoder()\n",
    "#         self.classes_ = None\n",
    "        \n",
    "#     def fit(self, series):\n",
    "#         series = pd.Series(series)\n",
    "#         unique_values = series.unique().tolist()\n",
    "#         if 'UNKNOWN' not in unique_values:\n",
    "#             unique_values.append('UNKNOWN')\n",
    "#         self.label_encoder.fit(unique_values)\n",
    "#         self.classes_ = self.label_encoder.classes_\n",
    "#         return self\n",
    "    \n",
    "#     def transform(self, series):\n",
    "#         series = pd.Series(series)\n",
    "#         series = series.map(lambda x: 'UNKNOWN' if x not in self.classes_ else x)\n",
    "#         return self.label_encoder.transform(series)\n",
    "    \n",
    "#     def fit_transform(self, series):\n",
    "#         return self.fit(series).transform(series)\n",
    "\n",
    "# def prepare_data(df, is_training=True):\n",
    "#     df_processed = df.copy()\n",
    "#     categorical_columns = df_processed.select_dtypes(include=['object']).columns\n",
    "#     for column in categorical_columns:\n",
    "#         if is_training:\n",
    "#             if column not in label_encoders:\n",
    "#                 label_encoders[column] = SafeLabelEncoder()\n",
    "#                 df_processed[column] = label_encoders[column].fit_transform(df_processed[column])\n",
    "#         else:\n",
    "#             if column in label_encoders:\n",
    "#                 df_processed[column] = label_encoders[column].transform(df_processed[column])\n",
    "\n",
    "#     # Log transformations\n",
    "#     if 'Balance' in df_processed.columns:\n",
    "#         df_processed['Balance_log'] = np.log1p(df_processed['Balance'].clip(lower=0))\n",
    "#     if 'EstimatedSalary' in df_processed.columns:\n",
    "#         df_processed['EstimatedSalary_log'] = np.log1p(df_processed['EstimatedSalary'].clip(lower=0))\n",
    "    \n",
    "#     # Interaction terms\n",
    "#     if 'Balance' in df_processed.columns and 'Age' in df_processed.columns:\n",
    "#         df_processed['Balance_Age'] = df_processed['Balance'] * df_processed['Age']\n",
    "#     if 'CreditScore' in df_processed.columns and 'Age' in df_processed.columns:\n",
    "#         df_processed['CreditScore_Age'] = df_processed['CreditScore'] * df_processed['Age']\n",
    "    \n",
    "#     # Polynomial terms\n",
    "#     df_processed['CreditScore_sq'] = df_processed['CreditScore'] ** 2\n",
    "#     df_processed['Balance_sq'] = df_processed['Balance'] ** 2\n",
    "#     df_processed['Age_sq'] = df_processed['Age'] ** 2\n",
    "    \n",
    "#     return df_processed\n",
    "\n",
    "# try:\n",
    "#     # Initialize label encoders dictionary\n",
    "#     label_encoders = {}\n",
    "\n",
    "#     # Process training data\n",
    "#     print(\"Processing training data...\")\n",
    "#     train_df_processed = prepare_data(train_df, is_training=True)\n",
    "\n",
    "#     # Define numeric features\n",
    "#     numeric_features = ['CreditScore', 'Age', 'Balance', 'EstimatedSalary', \n",
    "#                         'Balance_log', 'EstimatedSalary_log', 'Balance_Age', \n",
    "#                         'CreditScore_Age', 'CreditScore_sq', 'Balance_sq', 'Age_sq']\n",
    "\n",
    "#     # Scaling numeric features\n",
    "#     scaler = StandardScaler()\n",
    "#     train_df_processed[numeric_features] = scaler.fit_transform(train_df_processed[numeric_features])\n",
    "\n",
    "#     # PCA for dimensionality reduction\n",
    "#     pca = PCA(n_components=5)\n",
    "#     pca_features = pca.fit_transform(train_df_processed[numeric_features])\n",
    "#     pca_df = pd.DataFrame(pca_features, columns=[f'pca_{i+1}' for i in range(pca_features.shape[1])])\n",
    "#     train_df_processed = pd.concat([train_df_processed, pca_df], axis=1)\n",
    "\n",
    "#     # Prepare features and target\n",
    "#     X = train_df_processed.drop(['Exited', 'CustomerId'], axis=1)\n",
    "#     y = train_df_processed['Exited']\n",
    "#     X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#     # Hyperparameter tuning\n",
    "#     param_grid = {\n",
    "#         'n_estimators': [100, 200, 300],\n",
    "#         'max_depth': [10, 15, None],\n",
    "#         'min_samples_split': [2, 5, 10],\n",
    "#         'min_samples_leaf': [1, 2, 4],\n",
    "#     }\n",
    "\n",
    "#     print(\"Training Random Forest with RandomizedSearch...\")\n",
    "#     rf = RandomForestClassifier(random_state=42)\n",
    "#     rand_search_rf = RandomizedSearchCV(rf, param_grid, n_iter=30, cv=5, scoring='roc_auc', n_jobs=-1, verbose=1, random_state=42)\n",
    "#     rand_search_rf.fit(X_train, y_train)\n",
    "#     best_rf = rand_search_rf.best_estimator_\n",
    "#     print(f\"Best Random Forest parameters: {rand_search_rf.best_params_}\")\n",
    "\n",
    "#     # Stacking model with LogisticRegression as final estimator for predict_proba\n",
    "#     print(\"Training Stacking Model...\")\n",
    "#     stacking_model = StackingClassifier(\n",
    "#         estimators=[\n",
    "#             ('rf', best_rf),\n",
    "#             ('gb', GradientBoostingClassifier(n_estimators=100, learning_rate=0.05, max_depth=3, random_state=42)),\n",
    "#             ('xgb', XGBClassifier(n_estimators=100, learning_rate=0.05, max_depth=3, random_state=42, use_label_encoder=False, eval_metric='logloss'))\n",
    "#         ],\n",
    "#         final_estimator=LogisticRegression(),\n",
    "#         cv=5\n",
    "#     )\n",
    "#     stacking_model.fit(X_train, y_train)\n",
    "\n",
    "#     # Model evaluation\n",
    "#     print(\"Evaluating model...\")\n",
    "#     y_val_pred_proba = stacking_model.predict_proba(X_val)[:, 1]\n",
    "#     roc_auc = roc_auc_score(y_val, y_val_pred_proba)\n",
    "#     print(f\"Validation ROC AUC Score: {roc_auc}\")\n",
    "\n",
    "#     y_val_pred = stacking_model.predict(X_val)\n",
    "#     r2 = r2_score(y_val, y_val_pred)\n",
    "#     print(f\"R² Score on Validation Set: {r2}\")\n",
    "\n",
    "#     # Process test data\n",
    "#     print(\"Processing test data...\")\n",
    "#     test_df_processed = prepare_data(test_df, is_training=False)\n",
    "#     test_df_processed[numeric_features] = scaler.transform(test_df_processed[numeric_features])\n",
    "#     test_pca_features = pca.transform(test_df_processed[numeric_features])\n",
    "#     test_pca_df = pd.DataFrame(test_pca_features, columns=[f'pca_{i+1}' for i in range(test_pca_features.shape[1])])\n",
    "#     test_df_processed = pd.concat([test_df_processed, test_pca_df], axis=1)\n",
    "\n",
    "#     # Make predictions\n",
    "#     print(\"Making predictions...\")\n",
    "#     test_predictions = stacking_model.predict_proba(test_df_processed.drop(['CustomerId'], axis=1))[:, 1]\n",
    "\n",
    "#     # Create submission\n",
    "#     submission = pd.DataFrame({\n",
    "#         'id': test_df['id'],\n",
    "#         'Exited': test_predictions\n",
    "#     })\n",
    "#     submission.to_csv(\"submission.csv\", index=False)\n",
    "#     print(\"Submission saved successfully.\")\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"An error occurred: {str(e)}\")\n",
    "#     import traceback\n",
    "#     print(traceback.format_exc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.ensemble import StackingRegressor, GradientBoostingRegressor, RandomForestRegressor\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.metrics import r2_score\n",
    "# from skopt import BayesSearchCV\n",
    "# import xgboost as xgb\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# # File paths\n",
    "# INPUT_PATH = '../Datasets'\n",
    "# train_path = f\"{INPUT_PATH}/train.csv\"\n",
    "# test_path = f\"{INPUT_PATH}/test.csv\"\n",
    "\n",
    "# # Load datasets\n",
    "# train_df = pd.read_csv(train_path)\n",
    "# test_df = pd.read_csv(test_path)\n",
    "\n",
    "# class SafeLabelEncoder:\n",
    "#     def __init__(self, unknown_value=-1):\n",
    "#         self.unknown_value = unknown_value\n",
    "#         self.label_encoder = LabelEncoder()\n",
    "#         self.classes_ = None\n",
    "        \n",
    "#     def fit(self, series):\n",
    "#         series = pd.Series(series)\n",
    "#         unique_values = series.unique().tolist()\n",
    "#         if 'UNKNOWN' not in unique_values:\n",
    "#             unique_values.append('UNKNOWN')\n",
    "#         self.label_encoder.fit(unique_values)\n",
    "#         self.classes_ = self.label_encoder.classes_\n",
    "#         return self\n",
    "    \n",
    "#     def transform(self, series):\n",
    "#         series = pd.Series(series)\n",
    "#         series = series.map(lambda x: 'UNKNOWN' if x not in self.classes_ else x)\n",
    "#         return self.label_encoder.transform(series)\n",
    "    \n",
    "#     def fit_transform(self, series):\n",
    "#         return self.fit(series).transform(series)\n",
    "    \n",
    "# def prepare_data(df, is_training=True):\n",
    "#     df_processed = df.copy()\n",
    "#     categorical_columns = df_processed.select_dtypes(include=['object']).columns\n",
    "#     for column in categorical_columns:\n",
    "#         if is_training:\n",
    "#             if column not in label_encoders:\n",
    "#                 label_encoders[column] = SafeLabelEncoder()\n",
    "#                 df_processed[column] = label_encoders[column].fit_transform(df_processed[column])\n",
    "#         else:\n",
    "#             if column in label_encoders:\n",
    "#                 df_processed[column] = label_encoders[column].transform(df_processed[column])\n",
    "\n",
    "#     # Log transformations\n",
    "#     if 'Balance' in df_processed.columns:\n",
    "#         df_processed['Balance_log'] = np.log1p(df_processed['Balance'].clip(lower=0))\n",
    "#     if 'EstimatedSalary' in df_processed.columns:\n",
    "#         df_processed['EstimatedSalary_log'] = np.log1p(df_processed['EstimatedSalary'].clip(lower=0))\n",
    "    \n",
    "#     # Interaction terms\n",
    "#     if 'Balance' in df_processed.columns and 'Age' in df_processed.columns:\n",
    "#         df_processed['Balance_Age'] = df_processed['Balance'] * df_processed['Age']\n",
    "#     if 'CreditScore' in df_processed.columns and 'Age' in df_processed.columns:\n",
    "#         df_processed['CreditScore_Age'] = df_processed['CreditScore'] * df_processed['Age']\n",
    "    \n",
    "#     # Polynomial terms\n",
    "#     df_processed['CreditScore_sq'] = df_processed['CreditScore'] ** 2\n",
    "#     df_processed['Balance_sq'] = df_processed['Balance'] ** 2\n",
    "#     df_processed['Age_sq'] = df_processed['Age'] ** 2\n",
    "    \n",
    "#     return df_processed\n",
    "\n",
    "# # Create a sample dataset for demonstration\n",
    "# # (You should replace this with your own dataset)\n",
    "# # from sklearn.datasets import make_regression\n",
    "# # X_train, y_train = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
    "# # X_val, y_val = make_regression(n_samples=200, n_features=20, noise=0.1, random_state=42)\n",
    "\n",
    "# try:\n",
    "#     # Initialize label encoders dictionary\n",
    "#     label_encoders = {}\n",
    "\n",
    "#     # Process training data\n",
    "#     print(\"Processing training data...\")\n",
    "#     train_df_processed = prepare_data(train_df, is_training=True)\n",
    "#     test_df_processed = prepare_data(test_df, is_training=True)\n",
    "\n",
    "#     # Define numeric features\n",
    "#     numeric_features = ['CreditScore', 'Age', 'Balance', 'EstimatedSalary', \n",
    "#                         'Balance_log', 'EstimatedSalary_log', 'Balance_Age', \n",
    "#                         'CreditScore_Age', 'CreditScore_sq', 'Balance_sq', 'Age_sq']\n",
    "\n",
    "#     # Scaling numeric features\n",
    "#     scaler = StandardScaler()\n",
    "#     train_df_processed[numeric_features] = scaler.fit_transform(train_df_processed[numeric_features])\n",
    "\n",
    "#     # PCA for dimensionality reduction\n",
    "#     pca = PCA(n_components=5)\n",
    "#     pca_features = pca.fit_transform(train_df_processed[numeric_features])\n",
    "#     pca_df = pd.DataFrame(pca_features, columns=[f'pca_{i+1}' for i in range(pca_features.shape[1])])\n",
    "#     train_df_processed = pd.concat([train_df_processed, pca_df], axis=1)\n",
    "\n",
    "#     pca_features = pca.fit_transform(test_df_processed[numeric_features])\n",
    "#     pca_df = pd.DataFrame(pca_features, columns=[f'pca_{i+1}' for i in range(pca_features.shape[1])])\n",
    "#     test_df_processed = pd.concat([test_df_processed, pca_df], axis=1)\n",
    "\n",
    "#     # Prepare features and target\n",
    "#     X = train_df_processed.drop(['Exited', 'CustomerId'], axis=1)\n",
    "#     y = train_df_processed['Exited']\n",
    "#     X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#     # Define a KFold for cross-validation\n",
    "#     cv_strategy = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "#     # Bayesian Optimization for XGBoost\n",
    "#     param_search_xgb = {\n",
    "#         'n_estimators': [50, 100, 200, 500],\n",
    "#         'max_depth': [3, 4, 5, 6, 7, 10],\n",
    "#         'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "#         'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "#         'colsample_bytree': [0.6, 0.7, 0.8, 0.9],\n",
    "#         'reg_alpha': [0, 0.01, 0.1, 1],\n",
    "#         'reg_lambda': [0, 0.01, 0.1, 1]\n",
    "#     }\n",
    "\n",
    "#     # Using BayesSearchCV for XGBoost optimization\n",
    "#     xgb_bayes_search = BayesSearchCV(\n",
    "#         estimator=xgb.XGBRegressor(eval_metric='rmse', random_state=42),\n",
    "#         search_spaces=param_search_xgb,\n",
    "#         n_iter=30,\n",
    "#         cv=cv_strategy,\n",
    "#         scoring='r2',\n",
    "#         n_jobs=-1,\n",
    "#         verbose=1,\n",
    "#         random_state=42\n",
    "#     )\n",
    "\n",
    "#     # Fit the Bayesian search model\n",
    "#     xgb_bayes_search.fit(X_train, y_train)\n",
    "#     best_xgb = xgb_bayes_search.best_estimator_\n",
    "\n",
    "#     # Define the base models for stacking\n",
    "#     base_models = [\n",
    "#         ('rf', RandomForestRegressor(n_estimators=100, random_state=42)),\n",
    "#         ('gb', GradientBoostingRegressor(n_estimators=120, learning_rate=0.05, max_depth=4, random_state=2)),\n",
    "#         ('xgb', best_xgb)  # Use the best XGBoost model from optimization\n",
    "#     ]\n",
    "\n",
    "#     # Stacking with the optimized XGB and calibrated models\n",
    "#     stacking_model = StackingRegressor(\n",
    "#         estimators=base_models,\n",
    "#         final_estimator=LinearRegression(),\n",
    "#         cv=cv_strategy,\n",
    "#         n_jobs=-1\n",
    "#     )\n",
    "\n",
    "#     # Create a pipeline with scaling and imputation\n",
    "#     pipeline = Pipeline([\n",
    "#         ('imputer', SimpleImputer(strategy='mean')),  # Handle missing values\n",
    "#         ('scaler', StandardScaler()),  # Scale features\n",
    "#         ('stacking', stacking_model)\n",
    "#     ])\n",
    "\n",
    "#     # Fit the stacking model\n",
    "#     pipeline.fit(X_train, y_train)\n",
    "\n",
    "#     # Model evaluation\n",
    "#     y_val_pred = pipeline.predict(X_val)\n",
    "#     r2 = r2_score(y_val, y_val_pred)\n",
    "#     print(f\"R² Score on Validation Set: {r2}\")\n",
    "\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"An error occurred: {str(e)}\")\n",
    "#     import traceback\n",
    "#     print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create submission\n",
    "# submission = pd.DataFrame({\n",
    "#     'id': test_df['id'],\n",
    "#     'Exited': pipeline.predict(test_df_processed.drop(['id', 'CustomerId'], axis=1))\n",
    "# })\n",
    "# submission.to_csv(\"submission.csv\", index=False)\n",
    "# print(\"Submission saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "# from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier, ExtraTreesClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, classification_report\n",
    "# from xgboost import XGBClassifier\n",
    "# from catboost import CatBoostClassifier\n",
    "# from lightgbm import LGBMClassifier\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.feature_selection import RFECV\n",
    "# from sklearn.cluster import KMeans\n",
    "# from optuna import create_study, Trial\n",
    "# from optuna.samplers import TPESampler\n",
    "\n",
    "# # File paths\n",
    "# INPUT_PATH = '../Datasets'\n",
    "# train_path = f\"{INPUT_PATH}/train.csv\"\n",
    "# test_path = f\"{INPUT_PATH}/test.csv\"\n",
    "\n",
    "# # Load datasets\n",
    "# train_df = pd.read_csv(train_path)\n",
    "# test_df = pd.read_csv(test_path)\n",
    "\n",
    "# class SafeLabelEncoder:\n",
    "#     def __init__(self, unknown_value=-1):\n",
    "#         self.unknown_value = unknown_value\n",
    "#         self.label_encoder = LabelEncoder()\n",
    "#         self.classes_ = None\n",
    "        \n",
    "#     def fit(self, series):\n",
    "#         series = pd.Series(series)\n",
    "#         unique_values = series.unique().tolist()\n",
    "#         if 'UNKNOWN' not in unique_values:\n",
    "#             unique_values.append('UNKNOWN')\n",
    "#         self.label_encoder.fit(unique_values)\n",
    "#         self.classes_ = self.label_encoder.classes_\n",
    "#         return self\n",
    "    \n",
    "#     def transform(self, series):\n",
    "#         series = pd.Series(series)\n",
    "#         series = series.map(lambda x: 'UNKNOWN' if x not in self.classes_ else x)\n",
    "#         return self.label_encoder.transform(series)\n",
    "    \n",
    "#     def fit_transform(self, series):\n",
    "#         return self.fit(series).transform(series)\n",
    "\n",
    "# def prepare_data(df, is_training=True):\n",
    "#     df_processed = df.copy()\n",
    "#     categorical_columns = df_processed.select_dtypes(include=['object']).columns\n",
    "#     for column in categorical_columns:\n",
    "#         if is_training:\n",
    "#             if column not in label_encoders:\n",
    "#                 label_encoders[column] = SafeLabelEncoder()\n",
    "#                 df_processed[column] = label_encoders[column].fit_transform(df_processed[column])\n",
    "#         else:\n",
    "#             if column in label_encoders:\n",
    "#                 df_processed[column] = label_encoders[column].transform(df_processed[column])\n",
    "\n",
    "#     # Log transformations\n",
    "#     if 'Balance' in df_processed.columns:\n",
    "#         df_processed['Balance_log'] = np.log1p(df_processed['Balance'].clip(lower=0))\n",
    "#     if 'EstimatedSalary' in df_processed.columns:\n",
    "#         df_processed['EstimatedSalary_log'] = np.log1p(df_processed['EstimatedSalary'].clip(lower=0))\n",
    "    \n",
    "#     # Interaction terms\n",
    "#     if 'Balance' in df_processed.columns and 'Age' in df_processed.columns:\n",
    "#         df_processed['Balance_Age'] = df_processed['Balance'] * df_processed['Age']\n",
    "#     if 'CreditScore' in df_processed.columns and 'Age' in df_processed.columns:\n",
    "#         df_processed['CreditScore_Age'] = df_processed['CreditScore'] * df_processed['Age']\n",
    "    \n",
    "#     # Polynomial terms\n",
    "#     df_processed['CreditScore_sq'] = df_processed['CreditScore'] ** 2\n",
    "#     df_processed['Balance_sq'] = df_processed['Balance'] ** 2\n",
    "#     df_processed['Age_sq'] = df_processed['Age'] ** 2\n",
    "    \n",
    "#     return df_processed\n",
    "\n",
    "# try:\n",
    "#     # Initialize label encoders dictionary\n",
    "#     label_encoders = {}\n",
    "\n",
    "#     # Process training data\n",
    "#     print(\"Processing training data...\")\n",
    "#     train_df_processed = prepare_data(train_df, is_training=True)\n",
    "\n",
    "#     # Define numeric features\n",
    "#     numeric_features = ['CreditScore', 'Age', 'Balance', 'EstimatedSalary', \n",
    "#                         'Balance_log', 'EstimatedSalary_log', 'Balance_Age', \n",
    "#                         'CreditScore_Age', 'CreditScore_sq', 'Balance_sq', 'Age_sq']\n",
    "\n",
    "#     # Scaling numeric features with RobustScaler (handles outliers better)\n",
    "#     scaler = RobustScaler()\n",
    "#     train_df_processed[numeric_features] = scaler.fit_transform(train_df_processed[numeric_features])\n",
    "\n",
    "#     # PCA for dimensionality reduction\n",
    "#     pca = PCA(n_components=10)  # Increase number of components\n",
    "#     pca_features = pca.fit_transform(train_df_processed[numeric_features])\n",
    "#     pca_df = pd.DataFrame(pca_features, columns=[f'pca_{i+1}' for i in range(pca_features.shape[1])])\n",
    "#     train_df_processed = pd.concat([train_df_processed, pca_df], axis=1)\n",
    "\n",
    "#     # Clustering features\n",
    "#     kmeans = KMeans(n_clusters=6, random_state=2)  # Increase number of clusters\n",
    "#     train_df_processed['Cluster'] = kmeans.fit_predict(train_df_processed[numeric_features])\n",
    "\n",
    "#     # Prepare features and target\n",
    "#     X = train_df_processed.drop(['Exited', 'CustomerId'], axis=1)\n",
    "#     y = train_df_processed['Exited']\n",
    "#     X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=2, stratify=y)\n",
    "\n",
    "#     # Feature selection using RFECV with a stronger base estimator (e.g., XGBClassifier)\n",
    "#     rfecv = RFECV(estimator=XGBClassifier(), step=1, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "#     X_train_selected = rfecv.fit_transform(X_train, y_train)\n",
    "#     X_val_selected = rfecv.transform(X_val)\n",
    "\n",
    "#     # Hyperparameter optimization with Optuna (Bayesian Optimization)\n",
    "#     def objective(trial: Trial):\n",
    "#         param = {\n",
    "#             'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "#             'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "#             'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3)\n",
    "#         }\n",
    "#         model = LGBMClassifier(**param, random_state=2)\n",
    "#         model.fit(X_train_selected, y_train)\n",
    "#         preds = model.predict_proba(X_val_selected)[:, 1]\n",
    "#         return roc_auc_score(y_val, preds)\n",
    "\n",
    "#     study = create_study(direction='maximize', sampler=TPESampler())\n",
    "#     study.optimize(objective, n_trials=50)\n",
    "\n",
    "#     best_lgbm = LGBMClassifier(**study.best_params, random_state=2)\n",
    "#     best_lgbm.fit(X_train_selected, y_train)\n",
    "\n",
    "#     # Stacking model with LogisticRegression as final estimator for predict_proba\n",
    "#     print(\"Training Stacking Model...\")\n",
    "#     stacking_model = StackingClassifier(\n",
    "#         estimators=[\n",
    "#             ('rf', RandomForestClassifier(n_estimators=200, max_depth=10, random_state=2)),\n",
    "#             ('gb', GradientBoostingClassifier(n_estimators=120, learning_rate=0.05, max_depth=4, random_state=2)),\n",
    "#             ('xgb', XGBClassifier(n_estimators=120, learning_rate=0.05, max_depth=4, random_state=2, use_label_encoder=False, eval_metric='logloss')),\n",
    "#             ('catboost', CatBoostClassifier(iterations=200, depth=6, learning_rate=0.05, silent=True)),\n",
    "#             ('lgbm', best_lgbm),\n",
    "#             ('extra', ExtraTreesClassifier(n_estimators=200, max_depth=10, random_state=2))\n",
    "#         ],\n",
    "#         final_estimator=LogisticRegression(),\n",
    "#         cv=5\n",
    "#     )\n",
    "#     stacking_model.fit(X_train_selected, y_train)\n",
    "\n",
    "#     # Model evaluation\n",
    "#     print(\"Evaluating model...\")\n",
    "#     y_val_pred_proba = stacking_model.predict_proba(X_val_selected)[:, 1]\n",
    "#     roc_auc = roc_auc_score(y_val, y_val_pred_proba)\n",
    "#     print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "#     print(classification_report(y_val, stacking_model.predict(X_val_selected)))\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "# from sklearn.feature_selection import RFECV\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# # File paths\n",
    "# INPUT_PATH = '../Datasets'\n",
    "# train_path = f\"{INPUT_PATH}/train.csv\"\n",
    "# test_path = f\"{INPUT_PATH}/test.csv\"\n",
    "\n",
    "# # Load datasets\n",
    "# train_df = pd.read_csv(train_path)\n",
    "# test_df = pd.read_csv(test_path)\n",
    "\n",
    "# X = train_df.drop(['id', 'Exited', 'CustomerId'], axis=1)\n",
    "# y = train_df['Exited']\n",
    "\n",
    "# # Splitting the data into training and validation sets\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# # Preprocessing: Standardizing the data\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# # PCA for dimensionality reduction (Optional, based on your need)\n",
    "# pca = PCA(n_components=10)  # Adjust the number of components based on your dataset\n",
    "# X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "# X_val_pca = pca.transform(X_val_scaled)\n",
    "\n",
    "# # Feature selection using RFECV with a reduced number of cross-validation folds and step size\n",
    "# rfecv = RFECV(estimator=XGBClassifier(), step=5, cv=3, scoring='roc_auc', n_jobs=-1, verbose=1)\n",
    "\n",
    "# # Perform train-test split before feature selection to reduce size if needed\n",
    "# X_train_reduced, _, y_train_reduced, _ = train_test_split(X_train_pca, y_train, test_size=0.8, random_state=42)\n",
    "\n",
    "# # Apply feature selection on a reduced dataset to speed up the process\n",
    "# X_train_selected = rfecv.fit_transform(X_train_reduced, y_train_reduced)\n",
    "# X_val_selected = rfecv.transform(X_val_pca)\n",
    "\n",
    "# # Define a function to display evaluation metrics\n",
    "# def display_metrics(y_true, y_pred):\n",
    "#     print(\"Confusion Matrix:\")\n",
    "#     print(confusion_matrix(y_true, y_pred))\n",
    "#     print(\"\\nClassification Report:\")\n",
    "#     print(classification_report(y_true, y_pred))\n",
    "#     print(\"\\nROC AUC Score:\")\n",
    "#     print(roc_auc_score(y_true, y_pred))\n",
    "\n",
    "# # Hyperparameter optimization with RandomizedSearchCV for Random Forest\n",
    "# param_grid_rf = {\n",
    "#     'n_estimators': [100, 200, 500],\n",
    "#     'max_depth': [5, 10, 15],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4],\n",
    "#     'bootstrap': [True, False]\n",
    "# }\n",
    "\n",
    "# rf = RandomForestClassifier(random_state=42)\n",
    "# random_search_rf = RandomizedSearchCV(estimator=rf, param_distributions=param_grid_rf, n_iter=10, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "# # Fitting the random search model for Random Forest\n",
    "# random_search_rf.fit(X_train_selected, y_train_reduced)\n",
    "\n",
    "# # Predicting and evaluating the Random Forest model\n",
    "# y_pred_rf = random_search_rf.predict(X_val_selected)\n",
    "# display_metrics(y_val, y_pred_rf)\n",
    "\n",
    "# # Hyperparameter optimization with RandomizedSearchCV for Gradient Boosting\n",
    "# param_grid_gb = {\n",
    "#     'n_estimators': [100, 200, 500],\n",
    "#     'learning_rate': [0.01, 0.05, 0.1],\n",
    "#     'max_depth': [3, 5, 8],\n",
    "#     'subsample': [0.7, 0.8, 1.0]\n",
    "# }\n",
    "\n",
    "# gb = GradientBoostingClassifier(random_state=42)\n",
    "# random_search_gb = RandomizedSearchCV(estimator=gb, param_distributions=param_grid_gb, n_iter=10, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "# # Fitting the random search model for Gradient Boosting\n",
    "# random_search_gb.fit(X_train_selected, y_train_reduced)\n",
    "\n",
    "# # Predicting and evaluating the Gradient Boosting model\n",
    "# y_pred_gb = random_search_gb.predict(X_val_selected)\n",
    "# display_metrics(y_val, y_pred_gb)\n",
    "\n",
    "# # Stacking Classifier\n",
    "# estimators = [\n",
    "#     ('rf', random_search_rf.best_estimator_),\n",
    "#     ('gb', random_search_gb.best_estimator_),\n",
    "#     ('xgb', XGBClassifier(random_state=42))\n",
    "# ]\n",
    "\n",
    "# stacking_clf = StackingClassifier(estimators=estimators, final_estimator=GradientBoostingClassifier(random_state=42))\n",
    "\n",
    "# # Fitting the stacking classifier\n",
    "# stacking_clf.fit(X_train_selected, y_train_reduced)\n",
    "\n",
    "# # Predicting and evaluating the Stacking Classifier\n",
    "# y_pred_stack = stacking_clf.predict(X_val_selected)\n",
    "# display_metrics(y_val, y_pred_stack)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "# from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
    "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier, ExtraTreesClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, classification_report, r2_score\n",
    "# from xgboost import XGBClassifier\n",
    "# from catboost import CatBoostClassifier\n",
    "# from lightgbm import LGBMClassifier\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.feature_selection import RFECV\n",
    "# from sklearn.cluster import KMeans\n",
    "# from optuna import create_study, Trial\n",
    "# from optuna.samplers import TPESampler\n",
    "\n",
    "# # File paths\n",
    "# INPUT_PATH = '../Datasets'\n",
    "# train_path = f\"{INPUT_PATH}/train.csv\"\n",
    "# test_path = f\"{INPUT_PATH}/test.csv\"\n",
    "\n",
    "# # Load datasets\n",
    "# train_df = pd.read_csv(train_path)\n",
    "# test_df = pd.read_csv(test_path)\n",
    "\n",
    "# class SafeLabelEncoder:\n",
    "#     def __init__(self, unknown_value=-1):\n",
    "#         self.unknown_value = unknown_value\n",
    "#         self.label_encoder = LabelEncoder()\n",
    "#         self.classes_ = None\n",
    "        \n",
    "#     def fit(self, series):\n",
    "#         series = pd.Series(series)\n",
    "#         unique_values = series.unique().tolist()\n",
    "#         if 'UNKNOWN' not in unique_values:\n",
    "#             unique_values.append('UNKNOWN')\n",
    "#         self.label_encoder.fit(unique_values)\n",
    "#         self.classes_ = self.label_encoder.classes_\n",
    "#         return self\n",
    "    \n",
    "#     def transform(self, series):\n",
    "#         series = pd.Series(series)\n",
    "#         series = series.map(lambda x: 'UNKNOWN' if x not in self.classes_ else x)\n",
    "#         return self.label_encoder.transform(series)\n",
    "    \n",
    "#     def fit_transform(self, series):\n",
    "#         return self.fit(series).transform(series)\n",
    "\n",
    "# def prepare_data(df, is_training=True):\n",
    "#     df_processed = df.copy()\n",
    "#     categorical_columns = df_processed.select_dtypes(include=['object']).columns\n",
    "#     for column in categorical_columns:\n",
    "#         if is_training:\n",
    "#             if column not in label_encoders:\n",
    "#                 label_encoders[column] = SafeLabelEncoder()\n",
    "#                 df_processed[column] = label_encoders[column].fit_transform(df_processed[column])\n",
    "#         else:\n",
    "#             if column in label_encoders:\n",
    "#                 df_processed[column] = label_encoders[column].transform(df_processed[column])\n",
    "\n",
    "#     # Log transformations\n",
    "#     if 'Balance' in df_processed.columns:\n",
    "#         df_processed['Balance_log'] = np.log1p(df_processed['Balance'].clip(lower=0))\n",
    "#     if 'EstimatedSalary' in df_processed.columns:\n",
    "#         df_processed['EstimatedSalary_log'] = np.log1p(df_processed['EstimatedSalary'].clip(lower=0))\n",
    "    \n",
    "#     # Interaction terms\n",
    "#     if 'Balance' in df_processed.columns and 'Age' in df_processed.columns:\n",
    "#         df_processed['Balance_Age'] = df_processed['Balance'] * df_processed['Age']\n",
    "#     if 'CreditScore' in df_processed.columns and 'Age' in df_processed.columns:\n",
    "#         df_processed['CreditScore_Age'] = df_processed['CreditScore'] * df_processed['Age']\n",
    "    \n",
    "#     # Polynomial terms\n",
    "#     df_processed['CreditScore_sq'] = df_processed['CreditScore'] ** 2\n",
    "#     df_processed['Balance_sq'] = df_processed['Balance'] ** 2\n",
    "#     df_processed['Age_sq'] = df_processed['Age'] ** 2\n",
    "    \n",
    "#     return df_processed\n",
    "\n",
    "# # Initialize label encoders dictionary\n",
    "# label_encoders = {}\n",
    "\n",
    "# # Process training data\n",
    "# print(\"Processing training data...\")\n",
    "# train_df_processed = prepare_data(train_df, is_training=True)\n",
    "# test_df_processed = prepare_data(test_df, is_training=False)\n",
    "\n",
    "# # Define numeric features\n",
    "# numeric_features = ['CreditScore', 'Age', 'Balance', 'EstimatedSalary', \n",
    "#                     'Balance_log', 'EstimatedSalary_log', 'Balance_Age', \n",
    "#                     'CreditScore_Age', 'CreditScore_sq', 'Balance_sq', 'Age_sq']\n",
    "\n",
    "# # Scaling numeric features with RobustScaler (handles outliers better)\n",
    "# scaler = RobustScaler()\n",
    "# train_df_processed[numeric_features] = scaler.fit_transform(train_df_processed[numeric_features])\n",
    "# test_df_processed[numeric_features] = scaler.transform(test_df_processed[numeric_features])\n",
    "\n",
    "# # PCA for dimensionality reduction\n",
    "# pca = PCA(n_components=10)  # Increase number of components\n",
    "# pca_features_train = pca.fit_transform(train_df_processed[numeric_features])\n",
    "# pca_df_train = pd.DataFrame(pca_features_train, columns=[f'pca_{i+1}' for i in range(pca_features_train.shape[1])])\n",
    "# train_df_processed = pd.concat([train_df_processed, pca_df_train], axis=1)\n",
    "\n",
    "# pca_features_test = pca.transform(test_df_processed[numeric_features])\n",
    "# pca_df_test = pd.DataFrame(pca_features_test, columns=[f'pca_{i+1}' for i in range(pca_features_test.shape[1])])\n",
    "# test_df_processed = pd.concat([test_df_processed, pca_df_test], axis=1)\n",
    "\n",
    "# # Clustering features\n",
    "# kmeans = KMeans(n_clusters=6, random_state=2)  # Increase number of clusters\n",
    "# train_df_processed['Cluster'] = kmeans.fit_predict(train_df_processed[numeric_features])\n",
    "# test_df_processed['Cluster'] = kmeans.predict(test_df_processed[numeric_features])\n",
    "\n",
    "# # Prepare features and target\n",
    "# X = train_df_processed.drop(['Exited', 'CustomerId'], axis=1)\n",
    "# y = train_df_processed['Exited']\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=2, stratify=y)\n",
    "\n",
    "# # Feature selection using RFECV with a stronger base estimator (e.g., XGBClassifier)\n",
    "# rfecv = RFECV(estimator=XGBClassifier(), step=1, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "# X_train_selected = rfecv.fit_transform(X_train, y_train)\n",
    "# X_val_selected = rfecv.transform(X_val)\n",
    "\n",
    "# # Hyperparameter optimization with Optuna (Bayesian Optimization)\n",
    "# def objective(trial: Trial):\n",
    "#     param = {\n",
    "#         'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "#         'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3)\n",
    "#     }\n",
    "#     model = LGBMClassifier(**param, random_state=2)\n",
    "#     model.fit(X_train_selected, y_train)\n",
    "#     preds = model.predict_proba(X_val_selected)[:, 1]\n",
    "#     return roc_auc_score(y_val, preds)\n",
    "\n",
    "# study = create_study(direction='maximize', sampler=TPESampler())\n",
    "# study.optimize(objective, n_trials=50)\n",
    "\n",
    "# best_lgbm = LGBMClassifier(**study.best_params, random_state=2)\n",
    "# best_lgbm.fit(X_train_selected, y_train)\n",
    "\n",
    "# # Stacking model with LogisticRegression as final estimator for predict_proba\n",
    "# print(\"Training Stacking Model...\")\n",
    "# stacking_model = StackingClassifier(\n",
    "#     estimators=[\n",
    "#         ('rf', RandomForestClassifier(n_estimators=200, max_depth=10, random_state=2)),\n",
    "#         ('gb', GradientBoostingClassifier(n_estimators=120, learning_rate=0.05, max_depth=4, random_state=2)),\n",
    "#         ('xgb', XGBClassifier(n_estimators=120, learning_rate=0.05, max_depth=4, random_state=2, use_label_encoder=False, eval_metric='logloss')),\n",
    "#         ('catboost', CatBoostClassifier(iterations=200, depth=6, learning_rate=0.05, silent=True)),\n",
    "#         ('lgbm', best_lgbm),\n",
    "#         ('extra', ExtraTreesClassifier(n_estimators=200, max_depth=10, random_state=2))\n",
    "#     ],\n",
    "#     final_estimator=LogisticRegression(),\n",
    "#     cv=5\n",
    "# )\n",
    "# stacking_model.fit(X_train_selected, y_train)\n",
    "\n",
    "# # Model evaluation\n",
    "# print(\"Evaluating model...\")\n",
    "# y_val_pred_proba = stacking_model.predict_proba(X_val_selected)[:, 1]\n",
    "# roc_auc = roc_auc_score(y_val, y_val_pred_proba)\n",
    "# accuracy = accuracy_score(y_val, stacking_model.predict(X_val_selected))\n",
    "# r2 = r2_score(y_val, y_val_pred_proba)\n",
    "\n",
    "# print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "# print(f\"Accuracy: {accuracy:.4f}\")\n",
    "# print(f\"R² Score: {r2:.4f}\")\n",
    "# print(classification_report(y_val, stacking_model.predict(X_val_selected)))\n",
    "\n",
    "# pd.DataFrame({'id': test_df['CustomerId'], 'Exited': stacking_model.predict_proba(test_df_processed.drop(['CustomerId'], axis=1))[:, 1]}).to_csv('Predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "# from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
    "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier, ExtraTreesClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, classification_report, r2_score\n",
    "# from xgboost import XGBClassifier\n",
    "# from catboost import CatBoostClassifier\n",
    "# from lightgbm import LGBMClassifier\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.feature_selection import RFECV\n",
    "# from sklearn.cluster import KMeans\n",
    "# from optuna import create_study, Trial\n",
    "# from optuna.samplers import TPESampler\n",
    "\n",
    "# # File paths\n",
    "# INPUT_PATH = '../Datasets'\n",
    "# train_path = f\"{INPUT_PATH}/train.csv\"\n",
    "# test_path = f\"{INPUT_PATH}/test.csv\"\n",
    "\n",
    "# # Load datasets\n",
    "# train_df = pd.read_csv(train_path)\n",
    "# test_df = pd.read_csv(test_path)\n",
    "\n",
    "# class SafeLabelEncoder:\n",
    "#     def __init__(self, unknown_value=-1):\n",
    "#         self.unknown_value = unknown_value\n",
    "#         self.label_encoder = LabelEncoder()\n",
    "#         self.classes_ = None\n",
    "        \n",
    "#     def fit(self, series):\n",
    "#         series = pd.Series(series)\n",
    "#         unique_values = series.unique().tolist()\n",
    "#         if 'UNKNOWN' not in unique_values:\n",
    "#             unique_values.append('UNKNOWN')\n",
    "#         self.label_encoder.fit(unique_values)\n",
    "#         self.classes_ = self.label_encoder.classes_\n",
    "#         return self\n",
    "    \n",
    "#     def transform(self, series):\n",
    "#         series = pd.Series(series)\n",
    "#         series = series.map(lambda x: 'UNKNOWN' if x not in self.classes_ else x)\n",
    "#         return self.label_encoder.transform(series)\n",
    "    \n",
    "#     def fit_transform(self, series):\n",
    "#         return self.fit(series).transform(series)\n",
    "\n",
    "# def prepare_data(df, is_training=True):\n",
    "#     df_processed = df.copy()\n",
    "#     categorical_columns = df_processed.select_dtypes(include=['object']).columns\n",
    "#     for column in categorical_columns:\n",
    "#         if is_training:\n",
    "#             if column not in label_encoders:\n",
    "#                 label_encoders[column] = SafeLabelEncoder()\n",
    "#                 df_processed[column] = label_encoders[column].fit_transform(df_processed[column])\n",
    "#         else:\n",
    "#             if column in label_encoders:\n",
    "#                 df_processed[column] = label_encoders[column].transform(df_processed[column])\n",
    "\n",
    "#     # Log transformations\n",
    "#     if 'Balance' in df_processed.columns:\n",
    "#         df_processed['Balance_log'] = np.log1p(df_processed['Balance'].clip(lower=0))\n",
    "#     if 'EstimatedSalary' in df_processed.columns:\n",
    "#         df_processed['EstimatedSalary_log'] = np.log1p(df_processed['EstimatedSalary'].clip(lower=0))\n",
    "    \n",
    "#     # Interaction terms\n",
    "#     if 'Balance' in df_processed.columns and 'Age' in df_processed.columns:\n",
    "#         df_processed['Balance_Age'] = df_processed['Balance'] * df_processed['Age']\n",
    "#     if 'CreditScore' in df_processed.columns and 'Age' in df_processed.columns:\n",
    "#         df_processed['CreditScore_Age'] = df_processed['CreditScore'] * df_processed['Age']\n",
    "    \n",
    "#     # Polynomial terms\n",
    "#     df_processed['CreditScore_sq'] = df_processed['CreditScore'] ** 2\n",
    "#     df_processed['Balance_sq'] = df_processed['Balance'] ** 2\n",
    "#     df_processed['Age_sq'] = df_processed['Age'] ** 2\n",
    "    \n",
    "#     return df_processed\n",
    "\n",
    "# # Initialize label encoders dictionary\n",
    "# label_encoders = {}\n",
    "\n",
    "# # Process training data\n",
    "# print(\"Processing training data...\")\n",
    "# train_df_processed = prepare_data(train_df, is_training=True)\n",
    "# test_df_processed = prepare_data(test_df, is_training=False)\n",
    "\n",
    "# # Define numeric features\n",
    "# numeric_features = ['CreditScore', 'Age', 'Balance', 'EstimatedSalary', \n",
    "#                     'Balance_log', 'EstimatedSalary_log', 'Balance_Age', \n",
    "#                     'CreditScore_Age', 'CreditScore_sq', 'Balance_sq', 'Age_sq']\n",
    "\n",
    "# # Scaling numeric features with RobustScaler (handles outliers better)\n",
    "# scaler = RobustScaler()\n",
    "# train_df_processed[numeric_features] = scaler.fit_transform(train_df_processed[numeric_features])\n",
    "# test_df_processed[numeric_features] = scaler.transform(test_df_processed[numeric_features])\n",
    "\n",
    "# # PCA for dimensionality reduction\n",
    "# pca = PCA(n_components=10)  # Increase number of components\n",
    "# pca_features_train = pca.fit_transform(train_df_processed[numeric_features])\n",
    "# pca_df_train = pd.DataFrame(pca_features_train, columns=[f'pca_{i+1}' for i in range(pca_features_train.shape[1])])\n",
    "# train_df_processed = pd.concat([train_df_processed, pca_df_train], axis=1)\n",
    "\n",
    "# pca_features_test = pca.transform(test_df_processed[numeric_features])\n",
    "# pca_df_test = pd.DataFrame(pca_features_test, columns=[f'pca_{i+1}' for i in range(pca_features_test.shape[1])])\n",
    "# test_df_processed = pd.concat([test_df_processed, pca_df_test], axis=1)\n",
    "\n",
    "# # Prepare features and target\n",
    "# X = train_df_processed.drop(['Exited', 'CustomerId'], axis=1)\n",
    "# y = train_df_processed['Exited']\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=2, stratify=y)\n",
    "\n",
    "# # Feature selection using RFECV with a stronger base estimator (e.g., XGBClassifier)\n",
    "# rfecv = RFECV(estimator=XGBClassifier(), step=1, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "# X_train_selected = rfecv.fit_transform(X_train, y_train)\n",
    "# X_val_selected = rfecv.transform(X_val)\n",
    "\n",
    "# # Transform the test set using the same RFECV mask\n",
    "# X_test_selected = rfecv.transform(test_df_processed.drop(['CustomerId'], axis=1))\n",
    "\n",
    "# # Hyperparameter optimization with Optuna (Bayesian Optimization)\n",
    "# def objective(trial: Trial):\n",
    "#     param = {\n",
    "#         'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "#         'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3)\n",
    "#     }\n",
    "#     model = LGBMClassifier(**param, random_state=2)\n",
    "#     model.fit(X_train_selected, y_train)\n",
    "#     preds = model.predict_proba(X_val_selected)[:, 1]\n",
    "#     return roc_auc_score(y_val, preds)\n",
    "\n",
    "# study = create_study(direction='maximize', sampler=TPESampler())\n",
    "# study.optimize(objective, n_trials=50)\n",
    "\n",
    "# best_lgbm = LGBMClassifier(**study.best_params, random_state=2)\n",
    "# best_lgbm.fit(X_train_selected, y_train)\n",
    "\n",
    "# # Stacking model with LogisticRegression as final estimator for predict_proba\n",
    "# print(\"Training Stacking Model...\")\n",
    "# stacking_model = StackingClassifier(\n",
    "#     estimators=[\n",
    "#         ('rf', RandomForestClassifier(n_estimators=200, max_depth=10, random_state=2)),\n",
    "#         ('gb', GradientBoostingClassifier(n_estimators=120, learning_rate=0.05, max_depth=4, random_state=2)),\n",
    "#         ('xgb', XGBClassifier(n_estimators=120, learning_rate=0.05, max_depth=4, random_state=2, use_label_encoder=False, eval_metric='logloss')),\n",
    "#         ('catboost', CatBoostClassifier(iterations=200, depth=6, learning_rate=0.05, silent=True)),\n",
    "#         ('lgbm', best_lgbm),\n",
    "#         ('extra', ExtraTreesClassifier(n_estimators=200, max_depth=10, random_state=2))\n",
    "#     ],\n",
    "#     final_estimator=LogisticRegression(),\n",
    "#     cv=5\n",
    "# )\n",
    "# stacking_model.fit(X_train_selected, y_train)\n",
    "\n",
    "# # Model evaluation\n",
    "# print(\"Evaluating model...\")\n",
    "# y_val_pred_proba = stacking_model.predict_proba(X_val_selected)[:, 1]\n",
    "# roc_auc = roc_auc_score(y_val, y_val_pred_proba)\n",
    "# accuracy = accuracy_score(y_val, stacking_model.predict(X_val_selected))\n",
    "# r2 = r2_score(y_val, y_val_pred_proba)\n",
    "\n",
    "# print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "# print(f\"Accuracy: {accuracy:.4f}\")\n",
    "# print(f\"R² Score: {r2:.4f}\")\n",
    "# print(classification_report(y_val, stacking_model.predict(X_val_selected)))\n",
    "\n",
    "# # Make predictions on the test set\n",
    "# test_predictions = stacking_model.predict_proba(X_test_selected)[:, 1]\n",
    "# pd.DataFrame({'id': test_df['id'], 'Exited': test_predictions}).to_csv('Predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "# from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
    "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier, ExtraTreesClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, classification_report, r2_score\n",
    "# from xgboost import XGBClassifier\n",
    "# from catboost import CatBoostClassifier\n",
    "# from lightgbm import LGBMClassifier\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.feature_selection import RFECV\n",
    "# from sklearn.cluster import KMeans\n",
    "# from optuna import create_study, Trial\n",
    "# from optuna.samplers import TPESampler\n",
    "\n",
    "# # File paths\n",
    "# INPUT_PATH = '../Datasets'\n",
    "# train_path = f\"{INPUT_PATH}/train.csv\"\n",
    "# test_path = f\"{INPUT_PATH}/test.csv\"\n",
    "\n",
    "# # Load datasets\n",
    "# train_df = pd.read_csv(train_path)\n",
    "# test_df = pd.read_csv(test_path)\n",
    "\n",
    "# class SafeLabelEncoder:\n",
    "#     def __init__(self, unknown_value=-1):\n",
    "#         self.unknown_value = unknown_value\n",
    "#         self.label_encoder = LabelEncoder()\n",
    "#         self.classes_ = None\n",
    "        \n",
    "#     def fit(self, series):\n",
    "#         series = pd.Series(series)\n",
    "#         unique_values = series.unique().tolist()\n",
    "#         if 'UNKNOWN' not in unique_values:\n",
    "#             unique_values.append('UNKNOWN')\n",
    "#         self.label_encoder.fit(unique_values)\n",
    "#         self.classes_ = self.label_encoder.classes_\n",
    "#         return self\n",
    "    \n",
    "#     def transform(self, series):\n",
    "#         series = pd.Series(series)\n",
    "#         series = series.map(lambda x: 'UNKNOWN' if x not in self.classes_ else x)\n",
    "#         return self.label_encoder.transform(series)\n",
    "    \n",
    "#     def fit_transform(self, series):\n",
    "#         return self.fit(series).transform(series)\n",
    "\n",
    "# def prepare_data(df, is_training=True):\n",
    "#     df_processed = df.copy()\n",
    "#     categorical_columns = df_processed.select_dtypes(include=['object']).columns\n",
    "#     for column in categorical_columns:\n",
    "#         if is_training:\n",
    "#             if column not in label_encoders:\n",
    "#                 label_encoders[column] = SafeLabelEncoder()\n",
    "#                 df_processed[column] = label_encoders[column].fit_transform(df_processed[column])\n",
    "#         else:\n",
    "#             if column in label_encoders:\n",
    "#                 df_processed[column] = label_encoders[column].transform(df_processed[column])\n",
    "\n",
    "#     # Log transformations\n",
    "#     if 'Balance' in df_processed.columns:\n",
    "#         df_processed['Balance_log'] = np.log1p(df_processed['Balance'].clip(lower=0))\n",
    "#     if 'EstimatedSalary' in df_processed.columns:\n",
    "#         df_processed['EstimatedSalary_log'] = np.log1p(df_processed['EstimatedSalary'].clip(lower=0))\n",
    "    \n",
    "#     # Interaction terms\n",
    "#     if 'Balance' in df_processed.columns and 'Age' in df_processed.columns:\n",
    "#         df_processed['Balance_Age'] = df_processed['Balance'] * df_processed['Age']\n",
    "#     if 'CreditScore' in df_processed.columns and 'Age' in df_processed.columns:\n",
    "#         df_processed['CreditScore_Age'] = df_processed['CreditScore'] * df_processed['Age']\n",
    "    \n",
    "#     # Additional Interaction Terms\n",
    "#     if 'EstimatedSalary' in df_processed.columns:\n",
    "#         df_processed['Balance_EstimatedSalary'] = df_processed['Balance'] * df_processed['EstimatedSalary']\n",
    "#         df_processed['CreditScore_EstimatedSalary'] = df_processed['CreditScore'] * df_processed['EstimatedSalary']\n",
    "    \n",
    "#     # Polynomial terms\n",
    "#     df_processed['CreditScore_sq'] = df_processed['CreditScore'] ** 2\n",
    "#     df_processed['Balance_sq'] = df_processed['Balance'] ** 2\n",
    "#     df_processed['Age_sq'] = df_processed['Age'] ** 2\n",
    "#     df_processed['EstimatedSalary_sq'] = df_processed['EstimatedSalary'] ** 2\n",
    "    \n",
    "#     # Ratios\n",
    "#     if 'Balance' in df_processed.columns and 'EstimatedSalary' in df_processed.columns:\n",
    "#         df_processed['Balance_to_EstimatedSalary'] = df_processed['Balance'] / (df_processed['EstimatedSalary'] + 1e-5)  # Avoid division by zero\n",
    "\n",
    "#     return df_processed\n",
    "\n",
    "# # Initialize label encoders dictionary\n",
    "# label_encoders = {}\n",
    "\n",
    "# # Process training data\n",
    "# print(\"Processing training data...\")\n",
    "# train_df_processed = prepare_data(train_df, is_training=True)\n",
    "# test_df_processed = prepare_data(test_df, is_training =False)\n",
    "\n",
    "# # Define numeric features\n",
    "# numeric_features = ['CreditScore', 'Age', 'Balance', 'EstimatedSalary', \n",
    "#                     'Balance_log', 'EstimatedSalary_log', 'Balance_Age', \n",
    "#                     'CreditScore_Age', 'Balance_EstimatedSalary', \n",
    "#                     'CreditScore_EstimatedSalary', 'CreditScore_sq', \n",
    "#                     'Balance_sq', 'Age_sq', 'EstimatedSalary_sq', \n",
    "#                     'Balance_to_EstimatedSalary']\n",
    "\n",
    "# # Scaling numeric features with RobustScaler (handles outliers better)\n",
    "# scaler = RobustScaler()\n",
    "# train_df_processed[numeric_features] = scaler.fit_transform(train_df_processed[numeric_features])\n",
    "# test_df_processed[numeric_features] = scaler.transform(test_df_processed[numeric_features])\n",
    "\n",
    "# # PCA for dimensionality reduction\n",
    "# pca = PCA(n_components=15)  # Increase number of components\n",
    "# pca_features_train = pca.fit_transform(train_df_processed[numeric_features])\n",
    "# pca_df_train = pd.DataFrame(pca_features_train, columns=[f'pca_{i+1}' for i in range(pca_features_train.shape[1])])\n",
    "# train_df_processed = pd.concat([train_df_processed, pca_df_train], axis=1)\n",
    "\n",
    "# pca_features_test = pca.transform(test_df_processed[numeric_features])\n",
    "# pca_df_test = pd.DataFrame(pca_features_test, columns=[f'pca_{i+1}' for i in range(pca_features_test.shape[1])])\n",
    "# test_df_processed = pd.concat([test_df_processed, pca_df_test], axis=1)\n",
    "\n",
    "# # Prepare features and target\n",
    "# X = train_df_processed.drop(['Exited', 'CustomerId'], axis=1)\n",
    "# y = train_df_processed['Exited']\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=2, stratify=y)\n",
    "\n",
    "# # Feature selection using RFECV with a stronger base estimator (e.g., XGBClassifier)\n",
    "# rfecv = RFECV(estimator=XGBClassifier(), step=1, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "# X_train_selected = rfecv.fit_transform(X_train, y_train)\n",
    "# X_val_selected = rfecv.transform(X_val)\n",
    "\n",
    "# # Transform the test set using the same RFECV mask\n",
    "# X_test_selected = rfecv.transform(test_df_processed.drop(['CustomerId'], axis=1))\n",
    "\n",
    "# # Hyperparameter optimization with Optuna (Bayesian Optimization)\n",
    "# def objective(trial: Trial):\n",
    "#     param = {\n",
    "#         'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "#         'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3)\n",
    "#     }\n",
    "#     model = LGBMClassifier(**param, random_state=2)\n",
    "#     model.fit(X_train_selected, y_train)\n",
    "#     preds = model.predict_proba(X_val_selected)[:, 1]\n",
    "#     return roc_auc_score(y_val, preds)\n",
    "\n",
    "# study = create_study(direction='maximize', sampler=TPESampler())\n",
    "# study.optimize(objective, n_trials=50)\n",
    "\n",
    "# best_lgbm = LGBMClassifier(**study.best_params, random_state=2)\n",
    "# best_lgbm.fit(X_train_selected, y_train)\n",
    "\n",
    "# # Stacking model with LogisticRegression as final estimator for predict_proba\n",
    "# print(\"Training Stacking Model...\")\n",
    "# stacking_model = StackingClassifier(\n",
    "#     estimators=[\n",
    "#         ('rf', RandomForestClassifier(n_estimators=200, max_depth=10, random_state=2)),\n",
    "#         ('gb', GradientBoostingClassifier(n_estimators=120, learning_rate=0.05, max_depth=4, random_state=2)),\n",
    "#         ('xgb', XGBClassifier(n_estimators=120, learning_rate=0.05, max_depth=4, random_state=2, use_label_encoder=False, eval_metric='logloss')),\n",
    "#         ('catboost', CatBoostClassifier(iterations=200, depth=6, learning_rate=0.05, silent=True)),\n",
    "#         ('lgbm', best_lgbm),\n",
    "#         ('extra', ExtraTreesClassifier(n_estimators=200, max_depth=10, random_state=2))\n",
    "#     ],\n",
    "#     final_estimator=LogisticRegression(),\n",
    "#     cv=5\n",
    "# )\n",
    "# stacking_model.fit(X_train_selected, y_train)\n",
    "\n",
    "# # Model evaluation\n",
    "# print(\"Evaluating model...\")\n",
    "# y_val_pred_proba = stacking_model.predict_proba(X_val_selected)[:, 1]\n",
    "# roc_auc = roc_auc_score(y_val, y_val_pred_proba)\n",
    "# accuracy = accuracy_score(y_val, stacking_model.predict(X_val_selected))\n",
    "# r2 = r2_score(y_val, y_val_pred_proba)\n",
    "\n",
    "# print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "# print(f\"Accuracy: {accuracy:.4f}\")\n",
    "# print(f\"R² Score: {r2:.4f}\")\n",
    "# print(classification_report(y_val, stacking_model.predict(X_val_selected)))\n",
    "\n",
    "# # Make predictions on the test set\n",
    "# test_predictions = stacking_model.predict_proba(X_test_selected)[:, 1]\n",
    "# pd.DataFrame({'id': test_df['id'], 'Exited': test_predictions}).to_csv('Predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at Hidden Shadow\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../Datasets/train.csv'\n",
    "test_path = '../Datasets/test.csv'\n",
    "train_data = pd.read_csv(train_path)\n",
    "test_data = pd.read_csv(test_path)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.isnull().sum()\n",
    "# OUTPUT:\n",
    "#     id                 0\n",
    "#     CustomerId         0\n",
    "#     Surname            0\n",
    "#     CreditScore        0\n",
    "#     Geography          0\n",
    "#     Gender             0\n",
    "#     Age                0\n",
    "#     Tenure             0\n",
    "#     Balance            0\n",
    "#     NumOfProducts      0\n",
    "#     HasCrCard          0\n",
    "#     IsActiveMember     0\n",
    "#     EstimatedSalary    0\n",
    "#     Exited             0\n",
    "#     dtype: int64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.svm import SVC\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.metrics import accuracy_score, f1_score\n",
    "    \n",
    "# # Load your data\n",
    "# # Assuming the DataFrame is already loaded as 'train_data'\n",
    "\n",
    "# # Preprocessing\n",
    "# X = train_data.drop(columns=['Exited', 'CustomerId', 'Surname'])\n",
    "# y = train_data['Exited']\n",
    "\n",
    "# # Preprocess categorical columns and scale numerical columns\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', StandardScaler(), ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']),\n",
    "#         ('cat', OneHotEncoder(), ['Geography', 'Gender'])\n",
    "#     ])\n",
    "\n",
    "# # Split data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # List of models to train\n",
    "# models = {\n",
    "#     'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "#     'Random Forest': RandomForestClassifier(),\n",
    "#     'XGBoost': XGBClassifier(),\n",
    "#     'Support Vector Machine': SVC()\n",
    "# }\n",
    "\n",
    "# # Train and evaluate models\n",
    "# for name, model in models.items():\n",
    "#     # Create pipeline with preprocessor and classifier\n",
    "#     clf = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', model)])\n",
    "    \n",
    "#     # Fit model\n",
    "#     clf.fit(X_train, y_train)\n",
    "    \n",
    "#     # Predict on test set\n",
    "#     y_pred = clf.predict(X_test)\n",
    "    \n",
    "#     # Evaluate model\n",
    "#     accuracy = accuracy_score(y_test, y_pred)\n",
    "#     f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "#     print(f\"{name} - Accuracy: {accuracy:.4f}, F1-Score: {f1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "# from lightgbm import LGBMClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "\n",
    "# # Data Preprocessing (as before)\n",
    "# X = train_data.drop(columns=['CustomerId', 'Surname', 'Exited'])\n",
    "# y = train_data['Exited']\n",
    "\n",
    "# # One-hot encode categorical variables\n",
    "# X = pd.get_dummies(X, columns=['Geography', 'Gender'], drop_first=True)\n",
    "\n",
    "# # Train-test split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Scale the data\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# # Define models\n",
    "# models = {\n",
    "#     'Logistic Regression': LogisticRegression(),\n",
    "#     'Random Forest': RandomForestClassifier(),\n",
    "#     'Gradient Boosting': GradientBoostingClassifier(),\n",
    "#     'XGBoost': XGBClassifier(),\n",
    "#     'LightGBM': LGBMClassifier(),\n",
    "#     'SVM': SVC(probability=True),  # for AUC calculation\n",
    "#     'KNN': KNeighborsClassifier()\n",
    "# }\n",
    "\n",
    "# # Train and evaluate models\n",
    "# for name, model in models.items():\n",
    "#     model.fit(X_train_scaled, y_train)\n",
    "#     y_pred = model.predict(X_test_scaled)\n",
    "#     print(f\"Model: {name}\")\n",
    "#     print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "#     print(f\"ROC AUC: {roc_auc_score(y_test, model.predict_proba(X_test_scaled)[:, 1])}\")\n",
    "#     print(classification_report(y_test, y_pred))\n",
    "#     print(\"=\"*50)\n",
    "\n",
    "# # Ensemble Voting Classifier (with best models)\n",
    "# voting_clf = VotingClassifier(estimators=[\n",
    "#     ('lr', LogisticRegression()),\n",
    "#     ('rf', RandomForestClassifier()),\n",
    "#     ('xgb', XGBClassifier()),\n",
    "#     ('lgbm', LGBMClassifier())], voting='soft')\n",
    "\n",
    "# voting_clf.fit(X_train_scaled, y_train)\n",
    "# y_pred_voting = voting_clf.predict(X_test_scaled)\n",
    "\n",
    "# print(f\"Ensemble Voting Classifier Accuracy: {accuracy_score(y_test, y_pred_voting)}\")\n",
    "# print(f\"Ensemble Voting Classifier ROC AUC: {roc_auc_score(y_test, voting_clf.predict_proba(X_test_scaled)[:, 1])}\")\n",
    "# print(classification_report(y_test, y_pred_voting))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z = test_data.drop(['CustomerId', 'Surname'], axis=1)\n",
    "# z = pd.get_dummies(z, columns=['Geography', 'Gender'], drop_first=True)\n",
    "# pd.DataFrame({\n",
    "#     'id': test_data['id'], 'Exited': voting_clf.predict(z)\n",
    "# }).to_csv('Predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "# from lightgbm import LGBMClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, r2_score\n",
    "\n",
    "# # Data Preprocessing (as before)\n",
    "# X = train_data.drop(columns=['CustomerId', 'Surname', 'Exited'])\n",
    "# y = train_data['Exited']\n",
    "\n",
    "# # One-hot encode categorical variables\n",
    "# X = pd.get_dummies(X, columns=['Geography', 'Gender'], drop_first=True)\n",
    "\n",
    "# # Train-test split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Scale the data\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# # Regularized Logistic Regression (L2 Regularization)\n",
    "# log_reg = LogisticRegression(C=0.1, penalty='l2')  # C controls regularization strength\n",
    "# log_reg.fit(X_train_scaled, y_train)\n",
    "# y_pred_lr = log_reg.predict(X_test_scaled)\n",
    "# print(f\"Logistic Regression R²: {r2_score(y_test, y_pred_lr)}\")\n",
    "\n",
    "# # Random Forest with Hyperparameter Tuning\n",
    "# rf = RandomForestClassifier(max_depth=10, min_samples_split=10, min_samples_leaf=4, n_estimators=200)\n",
    "# rf.fit(X_train_scaled, y_train)\n",
    "# y_pred_rf = rf.predict(X_test_scaled)\n",
    "# print(f\"Random Forest R²: {r2_score(y_test, y_pred_rf)}\")\n",
    "\n",
    "# # XGBoost with regularization and hyperparameter tuning\n",
    "# xgb = XGBClassifier(max_depth=6, learning_rate=0.1, n_estimators=100, reg_alpha=0.01, reg_lambda=1)\n",
    "# xgb.fit(X_train_scaled, y_train)\n",
    "# y_pred_xgb = xgb.predict(X_test_scaled)\n",
    "# print(f\"XGBoost R²: {r2_score(y_test, y_pred_xgb)}\")\n",
    "\n",
    "# # LightGBM with hyperparameter tuning\n",
    "# lgbm = LGBMClassifier(max_depth=7, learning_rate=0.05, n_estimators=300, num_leaves=30, reg_alpha=0.1, reg_lambda=1)\n",
    "# lgbm.fit(X_train_scaled, y_train)\n",
    "# y_pred_lgbm = lgbm.predict(X_test_scaled)\n",
    "# print(f\"LightGBM R²: {r2_score(y_test, y_pred_lgbm)}\")\n",
    "\n",
    "# # Evaluation of Models\n",
    "# models = {'Logistic Regression': y_pred_lr, 'Random Forest': y_pred_rf, 'XGBoost': y_pred_xgb, 'LightGBM': y_pred_lgbm}\n",
    "\n",
    "# for name, preds in models.items():\n",
    "#     print(f\"\\nModel: {name}\")\n",
    "#     print(f\"R²: {r2_score(y_test, preds)}\")\n",
    "#     print(classification_report(y_test, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Example for Random Forest\n",
    "# param_grid_rf = {\n",
    "#     'n_estimators': [100, 200],\n",
    "#     'max_depth': [10, 20],\n",
    "#     'min_samples_split': [2, 5],\n",
    "#     'min_samples_leaf': [1, 2],\n",
    "# }\n",
    "\n",
    "# rf = RandomForestClassifier(random_state=42)\n",
    "# grid_search_rf = GridSearchCV(rf, param_grid_rf, cv=5, scoring='r2')\n",
    "# grid_search_rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# # Best model from the grid search\n",
    "# best_rf = grid_search_rf.best_estimator_\n",
    "# y_pred_best_rf = best_rf.predict(X_test_scaled)\n",
    "\n",
    "# print(f\"Tuned Random Forest R²: {r2_score(y_test, y_pred_best_rf)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Define the parameter grid\n",
    "# param_grid = {\n",
    "#     'n_estimators': [100, 200, 500],\n",
    "#     'max_depth': [None, 10, 20, 30],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4],\n",
    "#     'max_features': ['auto', 'sqrt', 'log2']\n",
    "# }\n",
    "\n",
    "# # Initialize RandomForestRegressor\n",
    "# rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# # Initialize GridSearchCV\n",
    "# grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, \n",
    "#                            cv=5, scoring='r2', n_jobs=-1, verbose=2)\n",
    "\n",
    "# # Fit the model\n",
    "# grid_search.fit(X_train_pca, y_train)\n",
    "\n",
    "# # Get the best parameters\n",
    "# print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# # Evaluate on the test set\n",
    "# best_rf = grid_search.best_estimator_\n",
    "# y_pred = best_rf.predict(X_test_pca)\n",
    "# print(f\"Test R² Score: {r2_score(y_test, y_pred)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
