{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5898b3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, r2_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# File paths\n",
    "# INPUT_PATH = '/kaggle/input/customer-churn-classification'\n",
    "INPUT_PATH = '../Datasets'\n",
    "train_path = f\"{INPUT_PATH}/train.csv\"\n",
    "test_path = f\"{INPUT_PATH}/test.csv\"\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "class SafeLabelEncoder:\n",
    "    def __init__(self, unknown_value=-1):\n",
    "        self.unknown_value = unknown_value\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.classes_ = None\n",
    "        \n",
    "    def fit(self, series):\n",
    "        series = pd.Series(series)\n",
    "        unique_values = series.unique().tolist()\n",
    "        if 'UNKNOWN' not in unique_values:\n",
    "            unique_values.append('UNKNOWN')\n",
    "        self.label_encoder.fit(unique_values)\n",
    "        self.classes_ = self.label_encoder.classes_\n",
    "        return self\n",
    "    \n",
    "    def transform(self, series):\n",
    "        series = pd.Series(series)\n",
    "        series = series.map(lambda x: 'UNKNOWN' if x not in self.classes_ else x)\n",
    "        return self.label_encoder.transform(series)\n",
    "    \n",
    "    def fit_transform(self, series):\n",
    "        return self.fit(series).transform(series)\n",
    "\n",
    "def prepare_data(df, is_training=True):\n",
    "    df_processed = df.copy()\n",
    "    categorical_columns = df_processed.select_dtypes(include=['object']).columns\n",
    "    for column in categorical_columns:\n",
    "        if is_training:\n",
    "            if column not in label_encoders:\n",
    "                label_encoders[column] = SafeLabelEncoder()\n",
    "                df_processed[column] = label_encoders[column].fit_transform(df_processed[column])\n",
    "        else:\n",
    "            if column in label_encoders:\n",
    "                df_processed[column] = label_encoders[column].transform(df_processed[column])\n",
    "\n",
    "    # Log transformations\n",
    "    if 'Balance' in df_processed.columns:\n",
    "        df_processed['Balance_log'] = np.log1p(df_processed['Balance'].clip(lower=0))\n",
    "    if 'EstimatedSalary' in df_processed.columns:\n",
    "        df_processed['EstimatedSalary_log'] = np.log1p(df_processed['EstimatedSalary'].clip(lower=0))\n",
    "    \n",
    "    # Interaction terms\n",
    "    if 'Balance' in df_processed.columns and 'Age' in df_processed.columns:\n",
    "        df_processed['Balance_Age'] = df_processed['Balance'] * df_processed['Age']\n",
    "    if 'CreditScore' in df_processed.columns and 'Age' in df_processed.columns:\n",
    "        df_processed['CreditScore_Age'] = df_processed['CreditScore'] * df_processed['Age']\n",
    "    \n",
    "    # Polynomial terms\n",
    "    df_processed['CreditScore_sq'] = df_processed['CreditScore'] ** 2\n",
    "    df_processed['Balance_sq'] = df_processed['Balance'] ** 2\n",
    "    df_processed['Age_sq'] = df_processed['Age'] ** 2\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "try:\n",
    "    # Initialize label encoders dictionary\n",
    "    label_encoders = {}\n",
    "\n",
    "    # Process training data\n",
    "    print(\"Processing training data...\")\n",
    "    train_df_processed = prepare_data(train_df, is_training=True)\n",
    "\n",
    "    # Define numeric features\n",
    "    numeric_features = ['CreditScore', 'Age', 'Balance', 'EstimatedSalary', \n",
    "                        'Balance_log', 'EstimatedSalary_log', 'Balance_Age', \n",
    "                        'CreditScore_Age', 'CreditScore_sq', 'Balance_sq', 'Age_sq']\n",
    "\n",
    "    # Scaling numeric features\n",
    "    scaler = StandardScaler()\n",
    "    train_df_processed[numeric_features] = scaler.fit_transform(train_df_processed[numeric_features])\n",
    "\n",
    "    # PCA for dimensionality reduction\n",
    "    pca = PCA(n_components=10)\n",
    "    pca_features = pca.fit_transform(train_df_processed[numeric_features])\n",
    "    pca_df = pd.DataFrame(pca_features, columns=[f'pca_{i+1}' for i in range(pca_features.shape[1])])\n",
    "    train_df_processed = pd.concat([train_df_processed, pca_df], axis=1)\n",
    "\n",
    "    # Prepare features and target\n",
    "    X = train_df_processed.drop(['id', 'Exited', 'CustomerId'], axis=1)\n",
    "    y = train_df_processed['Exited']\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 120, 150, 200, 300],\n",
    "        'max_depth': [3, 4, 5, 10, 15, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "    }\n",
    "\n",
    "    print(\"Training Random Forest with RandomizedSearch...\")\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    rand_search_rf = RandomizedSearchCV(rf, param_grid, n_iter=30, cv=5, scoring='roc_auc', n_jobs=-1, verbose=1, random_state=42)\n",
    "    rand_search_rf.fit(X_train, y_train)\n",
    "    best_rf = rand_search_rf.best_estimator_\n",
    "    print(f\"Best Random Forest parameters: {rand_search_rf.best_params_}\")\n",
    "\n",
    "    # Stacking model with LogisticRegression as final estimator for predict_proba\n",
    "    print(\"Training Stacking Model...\")\n",
    "    stacking_model = StackingClassifier(\n",
    "        estimators=[\n",
    "            ('rf', best_rf),\n",
    "            ('gb', GradientBoostingClassifier(n_estimators=100, learning_rate=0.05, max_depth=3, random_state=42)),\n",
    "            ('xgb', XGBClassifier(n_estimators=100, learning_rate=0.05, max_depth=3, random_state=42, use_label_encoder=False, eval_metric='logloss'))\n",
    "        ],\n",
    "        final_estimator=LogisticRegression(),\n",
    "        cv=5\n",
    "    )\n",
    "    stacking_model.fit(X_train, y_train)\n",
    "\n",
    "    # Model evaluation\n",
    "    print(\"Evaluating model...\")\n",
    "    y_val_pred_proba = stacking_model.predict_proba(X_val)[:, 1]\n",
    "    roc_auc = roc_auc_score(y_val, y_val_pred_proba)\n",
    "    print(f\"Validation ROC AUC Score: {roc_auc}\")\n",
    "\n",
    "    y_val_pred = stacking_model.predict(X_val)\n",
    "    r2 = r2_score(y_val, y_val_pred)\n",
    "    print(f\"R² Score on Validation Set: {r2}\")\n",
    "\n",
    "    # Process test data\n",
    "    print(\"Processing test data...\")\n",
    "    test_df_processed = prepare_data(test_df, is_training=False)\n",
    "    test_df_processed[numeric_features] = scaler.transform(test_df_processed[numeric_features])\n",
    "    test_pca_features = pca.transform(test_df_processed[numeric_features])\n",
    "    test_pca_df = pd.DataFrame(test_pca_features, columns=[f'pca_{i+1}' for i in range(test_pca_features.shape[1])])\n",
    "    test_df_processed = pd.concat([test_df_processed, test_pca_df], axis=1)\n",
    "\n",
    "    # Make predictions\n",
    "    print(\"Making predictions...\")\n",
    "    test_predictions = stacking_model.predict_proba(test_df_processed.drop(['id', 'CustomerId'], axis=1))[:, 1]\n",
    "\n",
    "    # Create submission\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_df['id'],\n",
    "        'Exited': test_predictions\n",
    "    })\n",
    "    submission.to_csv(\"Predictions.csv\", index=False)\n",
    "    print(\"Predictions saved successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "    import traceback\n",
    "    print(traceback.format_exc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f95049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, r2_score, accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# File paths\n",
    "INPUT_PATH = '../Datasets'\n",
    "train_path = f\"{INPUT_PATH}/train.csv\"\n",
    "test_path = f\"{INPUT_PATH}/test.csv\"\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "class SafeLabelEncoder:\n",
    "    def __init__(self):\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.classes_ = None\n",
    "        \n",
    "    def fit(self, series):\n",
    "        self.label_encoder.fit(series)\n",
    "        self.classes_ = self.label_encoder.classes_\n",
    "        return self\n",
    "    \n",
    "    def transform(self, series):\n",
    "        # Handle unknown categories by mapping them to the most frequent class\n",
    "        series = pd.Series(series).map(lambda x: self.classes_[0] if x not in self.classes_ else x)\n",
    "        return self.label_encoder.transform(series)\n",
    "    \n",
    "    def fit_transform(self, series):\n",
    "        return self.fit(series).transform(series)\n",
    "def create_advanced_features(df):\n",
    "    \"\"\"Create advanced feature combinations\"\"\"\n",
    "    # Financial features with better handling of zeros and outliers\n",
    "    df['HasBalance'] = (df['Balance'] > 0).astype(int)\n",
    "    df['BalanceToSalary'] = np.where(df['EstimatedSalary'] > 0, \n",
    "                                    df['Balance'] / df['EstimatedSalary'], 0)\n",
    "    df['CreditScoreToAge'] = df['CreditScore'] / df['Age'].clip(lower=18)\n",
    "    df['BalancePerProduct'] = np.where(df['NumOfProducts'] > 0,\n",
    "                                      df['Balance'] / df['NumOfProducts'], 0)\n",
    "    \n",
    "    # Advanced financial metrics\n",
    "    df['WealthScore'] = (df['Balance'] * df['CreditScore']) / (df['Age'] + 1)\n",
    "    df['ActivityScore'] = df['NumOfProducts'] * df['IsActiveMember']\n",
    "    df['RiskScore'] = (df['Balance'] * (1 - df['IsActiveMember'])) / (df['CreditScore'] + 1)\n",
    "    \n",
    "    # Polynomial features for key metrics (with scaling to prevent overflow)\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(df[['CreditScore', 'Balance', 'Age']])\n",
    "    df['CreditScore_sq'] = scaled_features[:, 0] ** 2\n",
    "    df['Balance_sq'] = scaled_features[:, 1] ** 2\n",
    "    df['Age_sq'] = scaled_features[:, 2] ** 2\n",
    "    \n",
    "    # Interaction features with scaling\n",
    "    df['AgeBalance'] = scaled_features[:, 2] * scaled_features[:, 1]\n",
    "    df['AgeSalary'] = scaled_features[:, 2] * scaler.fit_transform(df[['EstimatedSalary']])[:, 0]\n",
    "    df['CreditScoreBalance'] = scaled_features[:, 0] * scaled_features[:, 1]\n",
    "    \n",
    "    # Customer segments\n",
    "    df['IsYoungLowBalance'] = ((df['Age'] < 30) & (df['Balance'] < df['Balance'].median())).astype(int)\n",
    "    df['IsSeniorHighValue'] = ((df['Age'] > 60) & (df['Balance'] > df['Balance'].quantile(0.75))).astype(int)\n",
    "    df['IsRisky'] = ((df['CreditScore'] < df['CreditScore'].quantile(0.25)) & \n",
    "                     (df['Balance'] > df['Balance'].quantile(0.75))).astype(int)\n",
    "    \n",
    "    # Percentile ranks instead of raw bins\n",
    "    df['BalanceRank'] = df['Balance'].rank(pct=True)\n",
    "    df['SalaryRank'] = df['EstimatedSalary'].rank(pct=True)\n",
    "    df['CreditScoreRank'] = df['CreditScore'].rank(pct=True)\n",
    "    \n",
    "    # Complex interaction features\n",
    "    df['ProductBalanceInteraction'] = np.log1p(df['Balance']) * df['NumOfProducts']\n",
    "    df['SalaryScoreRatio'] = np.where(df['CreditScore'] > 0,\n",
    "                                     df['EstimatedSalary'] / df['CreditScore'], 0)\n",
    "    \n",
    "    # Custom risk indicators\n",
    "    df['HighRisk'] = ((df['Balance'] > df['EstimatedSalary']) & \n",
    "                      (df['CreditScore'] < df['CreditScore'].median()) &\n",
    "                      (df['IsActiveMember'] == 0)).astype(int)\n",
    "    df['LowRisk'] = ((df['Balance'] < df['Balance'].median()) &\n",
    "                     (df['CreditScore'] > df['CreditScore'].median()) &\n",
    "                     (df['IsActiveMember'] == 1)).astype(int)\n",
    "    \n",
    "    # Create quantile bins for Balance with duplicates='drop'\n",
    "    df['BalanceBin'] = pd.qcut(df['Balance'], q=5, labels=False, duplicates='drop')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_data(df, label_encoders=None, is_training=True):\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Handle categorical variables\n",
    "    categorical_columns = df_processed.select_dtypes(include=['object']).columns\n",
    "    if is_training:\n",
    "        label_encoders = {}\n",
    "        for column in categorical_columns:\n",
    "            label_encoders[column] = SafeLabelEncoder()\n",
    "            df_processed[column] = label_encoders[column].fit_transform(df_processed[column])\n",
    "    else:\n",
    "        for column in categorical_columns:\n",
    "            if column in label_encoders:\n",
    "                df_processed[column] = label_encoders[column].transform(df_processed[column])\n",
    "    \n",
    "    # Create advanced features\n",
    "    df_processed = create_advanced_features(df_processed)\n",
    "    \n",
    "    return df_processed, label_encoders\n",
    "\n",
    "try:\n",
    "    # Load and process data\n",
    "    print(\"Processing training data...\")\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    \n",
    "    # Prepare training data\n",
    "    train_df_processed, label_encoders = prepare_data(train_df, is_training=True)\n",
    "    \n",
    "    # Define features and target\n",
    "    target = 'Exited'\n",
    "    features = [col for col in train_df_processed.columns if col not in [target, 'CustomerId']]\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    train_df_processed[features] = scaler.fit_transform(train_df_processed[features])\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=0.95)  # Keep 95% of variance\n",
    "    pca_features = pca.fit_transform(train_df_processed[features])\n",
    "    pca_cols = [f'pca_{i+1}' for i in range(pca_features.shape[1])]\n",
    "    train_df_processed = pd.concat([\n",
    "        train_df_processed,\n",
    "        pd.DataFrame(pca_features, columns=pca_cols)\n",
    "    ], axis=1)\n",
    "    \n",
    "    # Split data\n",
    "    X = train_df_processed.drop([target, 'CustomerId'], axis=1)\n",
    "    y = train_df_processed[target]\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Define optimized models\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=500,\n",
    "        max_depth=15,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        max_features='sqrt',\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=500,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.01,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        scale_pos_weight=2,\n",
    "        enable_categorical=True,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    lgb = LGBMClassifier(\n",
    "        n_estimators=500,\n",
    "        max_depth=8,\n",
    "        learning_rate=0.01,\n",
    "        num_leaves=31,\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    gb = GradientBoostingClassifier(\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Create and train stacking model\n",
    "    print(\"Training Stacking Model...\")\n",
    "    stacking_model = StackingClassifier(\n",
    "        estimators=[\n",
    "            ('rf', rf),\n",
    "            ('xgb', xgb),\n",
    "            ('lgb', lgb),\n",
    "            ('gb', gb)\n",
    "        ],\n",
    "        final_estimator=LogisticRegression(max_iter=1000, class_weight='balanced'),\n",
    "        cv=5,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Fit stacking model\n",
    "    stacking_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate model\n",
    "    y_val_pred_proba = stacking_model.predict_proba(X_val)[:, 1]\n",
    "    y_val_pred = stacking_model.predict(X_val)\n",
    "    \n",
    "    roc_auc = roc_auc_score(y_val, y_val_pred_proba)\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    r2 = r2_score(y_val, y_val_pred)\n",
    "    \n",
    "    print(f\"Validation ROC AUC Score: {roc_auc:.4f}\")\n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"R² Score on Validation Set: {r2:.4f}\")\n",
    "    \n",
    "    # Process test data\n",
    "    print(\"Processing test data...\")\n",
    "    test_df_processed, _ = prepare_data(test_df, label_encoders=label_encoders, is_training=False)\n",
    "    test_df_processed[features] = scaler.transform(test_df_processed[features])\n",
    "    \n",
    "    # Apply PCA to test data\n",
    "    test_pca_features = pca.transform(test_df_processed[features])\n",
    "    test_df_processed = pd.concat([\n",
    "        test_df_processed,\n",
    "        pd.DataFrame(test_pca_features, columns=pca_cols)\n",
    "    ], axis=1)\n",
    "    \n",
    "    # Generate predictions\n",
    "    test_predictions = stacking_model.predict_proba(test_df_processed.drop(['CustomerId'], axis=1))[:, 1]\n",
    "    \n",
    "    # Create submission\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_df['id'],\n",
    "        'Exited': test_predictions\n",
    "    })\n",
    "    submission.to_csv(\"Predictions.csv\", index=False)\n",
    "    print(\"Predictions saved successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "    import traceback\n",
    "    print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eeccf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, r2_score\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Feature selection with RFECV\n",
    "def select_features(X, y, model):\n",
    "    selector = RFECV(estimator=model, step=1, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "    selector.fit(X, y)\n",
    "    selected_features = X.columns[selector.support_]\n",
    "    return selected_features\n",
    "\n",
    "# Optuna-based hyperparameter tuning\n",
    "def optimize_model(trial, model_type):\n",
    "    if model_type == 'xgb':\n",
    "        return XGBClassifier(\n",
    "            n_estimators=trial.suggest_int('n_estimators', 100, 1000),\n",
    "            max_depth=trial.suggest_int('max_depth', 3, 10),\n",
    "            learning_rate=trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "            subsample=trial.suggest_float('subsample', 0.6, 1.0),\n",
    "            colsample_bytree=trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "            scale_pos_weight=trial.suggest_float('scale_pos_weight', 1, 5),\n",
    "            random_state=42,\n",
    "        )\n",
    "    elif model_type == 'lgb':\n",
    "        return LGBMClassifier(\n",
    "            n_estimators=trial.suggest_int('n_estimators', 100, 1000),\n",
    "            max_depth=trial.suggest_int('max_depth', 3, 10),\n",
    "            learning_rate=trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "            num_leaves=trial.suggest_int('num_leaves', 10, 50),\n",
    "            random_state=42,\n",
    "            class_weight='balanced',\n",
    "        )\n",
    "    elif model_type == 'catboost':\n",
    "        return CatBoostClassifier(\n",
    "            iterations=trial.suggest_int('iterations', 100, 1000),\n",
    "            depth=trial.suggest_int('depth', 3, 10),\n",
    "            learning_rate=trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "            l2_leaf_reg=trial.suggest_float('l2_leaf_reg', 1, 10),\n",
    "            random_strength=trial.suggest_float('random_strength', 0, 1),\n",
    "            class_weights=[1, 2],  # Adjust for imbalance\n",
    "            verbose=0,\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "def optimize_pipeline(X_train, y_train, model_type):\n",
    "    def objective(trial):\n",
    "        model = optimize_model(trial, model_type)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict_proba(X_train)[:, 1]\n",
    "        return roc_auc_score(y_train, y_pred)\n",
    "\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=50)\n",
    "    return study.best_params\n",
    "\n",
    "# Training pipeline\n",
    "def train_stacking_model(X_train, y_train, X_val, y_val):\n",
    "    # Optimize each base model\n",
    "    print(\"Optimizing XGBoost...\")\n",
    "    xgb_params = optimize_pipeline(X_train, y_train, 'xgb')\n",
    "    print(\"Optimizing LightGBM...\")\n",
    "    lgb_params = optimize_pipeline(X_train, y_train, 'lgb')\n",
    "    print(\"Optimizing CatBoost...\")\n",
    "    cat_params = optimize_pipeline(X_train, y_train, 'catboost')\n",
    "\n",
    "    # Define models with best parameters\n",
    "    xgb = XGBClassifier(**xgb_params, random_state=42)\n",
    "    lgb = LGBMClassifier(**lgb_params, random_state=42)\n",
    "    cat = CatBoostClassifier(**cat_params, random_state=42, verbose=0)\n",
    "\n",
    "    # Stacking model\n",
    "    stacking_model = StackingClassifier(\n",
    "        estimators=[\n",
    "            ('xgb', xgb),\n",
    "            ('lgb', lgb),\n",
    "            ('cat', cat),\n",
    "        ],\n",
    "        final_estimator=LogisticRegression(max_iter=1000, class_weight='balanced'),\n",
    "        cv=5,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Train stacking model\n",
    "    print(\"Training Stacking Model...\")\n",
    "    stacking_model.fit(X_train, y_train)\n",
    "\n",
    "    # Validation performance\n",
    "    y_val_pred_proba = stacking_model.predict_proba(X_val)[:, 1]\n",
    "    y_val_pred = stacking_model.predict(X_val)\n",
    "\n",
    "    roc_auc = roc_auc_score(y_val, y_val_pred_proba)\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "    print(f\"Validation ROC AUC Score: {roc_auc:.4f}\")\n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"R² Score on Validation Set: {r2:.4f}\")\n",
    "\n",
    "    return stacking_model\n",
    "\n",
    "# Load dataset and preprocess (replace this with your data loading logic)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Select important features\n",
    "selected_features = select_features(X_train, y_train, RandomForestClassifier(random_state=42))\n",
    "X_train = X_train[selected_features]\n",
    "X_val = X_val[selected_features]\n",
    "\n",
    "# Train and evaluate stacking model\n",
    "stacking_model = train_stacking_model(X_train, y_train, X_val, y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98e9df1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
